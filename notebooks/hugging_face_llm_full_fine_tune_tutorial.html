<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Learn how to fully fine-tune a Small Language Model on a custom dataset with Hugging Face Transformers.">

<title>Fully Fine-tune a Small Language Model with Hugging Face Tutorial ‚Äì Learn Hugging Face ü§ó</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/learn-hf-favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-68c8bffd90dad8f2b55c52d7b6410dc0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-48b82b0f71f6c0c71417b6e285bbb595.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Fully Fine-tune a Small Language Model with Hugging Face Tutorial ‚Äì Learn Hugging Face ü§ó">
<meta property="og:description" content="Learn how to fully fine-tune a Small Language Model on a custom dataset with Hugging Face Transformers.">
<meta property="og:image" content="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-text-classification/00-project-food-not-food-overview.png">
<meta property="og:site_name" content="Learn Hugging Face ü§ó">
<meta name="twitter:title" content="Fully Fine-tune a Small Language Model with Hugging Face Tutorial ‚Äì Learn Hugging Face ü§ó">
<meta name="twitter:description" content="Learn how to fully fine-tune a Small Language Model on a custom dataset with Hugging Face Transformers.">
<meta name="twitter:image" content="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-text-classification/00-project-food-not-food-overview.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Learn Hugging Face ü§ó</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../extras/setup.html"> 
<span class="menu-text">Setup</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../extras/glossary.html"> 
<span class="menu-text">Glossary</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com/mrdbourke/learn-huggingface" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notebooks/hugging_face_llm_full_fine_tune_tutorial.html">Natural Language Processing (NLP)</a></li><li class="breadcrumb-item"><a href="../notebooks/hugging_face_llm_full_fine_tune_tutorial.html">Fully fine-tune an LLM to do structrued data extraction</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Natural Language Processing (NLP)</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/hugging_face_llm_full_fine_tune_tutorial.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Fully fine-tune an LLM to do structrued data extraction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/hugging_face_text_classification_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Build a custom text classification model and demo</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Computer Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/hugging_face_object_detection_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Build a custom object detection model and demo</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">1</span> Overview</a></li>
  <li><a href="#how-to-fine-tune-an-llm-model" id="toc-how-to-fine-tune-an-llm-model" class="nav-link" data-scroll-target="#how-to-fine-tune-an-llm-model"><span class="header-section-number">2</span> How to fine-tune an LLM model</a></li>
  <li><a href="#what-were-cooking" id="toc-what-were-cooking" class="nav-link" data-scroll-target="#what-were-cooking"><span class="header-section-number">3</span> What we‚Äôre cooking</a></li>
  <li><a href="#ingredients" id="toc-ingredients" class="nav-link" data-scroll-target="#ingredients"><span class="header-section-number">4</span> Ingredients</a></li>
  <li><a href="#method" id="toc-method" class="nav-link" data-scroll-target="#method"><span class="header-section-number">5</span> Method</a></li>
  <li><a href="#fine-tuning-llm-vs-rag" id="toc-fine-tuning-llm-vs-rag" class="nav-link" data-scroll-target="#fine-tuning-llm-vs-rag"><span class="header-section-number">6</span> Fine-tuning LLM vs RAG</a></li>
  <li><a href="#why-fine-tune-your-own-model" id="toc-why-fine-tune-your-own-model" class="nav-link" data-scroll-target="#why-fine-tune-your-own-model"><span class="header-section-number">7</span> Why fine-tune your own model?</a></li>
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions"><span class="header-section-number">8</span> Definitions</a></li>
  <li><a href="#import-dependencies" id="toc-import-dependencies" class="nav-link" data-scroll-target="#import-dependencies"><span class="header-section-number">9</span> Import dependencies</a></li>
  <li><a href="#setup-base-model" id="toc-setup-base-model" class="nav-link" data-scroll-target="#setup-base-model"><span class="header-section-number">10</span> Setup Base Model</a></li>
  <li><a href="#get-dataset" id="toc-get-dataset" class="nav-link" data-scroll-target="#get-dataset"><span class="header-section-number">11</span> Get dataset</a>
  <ul class="collapse">
  <li><a href="#format-the-dataset-into-llm-style-inputsoutputs" id="toc-format-the-dataset-into-llm-style-inputsoutputs" class="nav-link" data-scroll-target="#format-the-dataset-into-llm-style-inputsoutputs"><span class="header-section-number">11.1</span> Format the dataset into LLM-style inputs/outputs</a></li>
  <li><a href="#try-the-model-with-a-pipeline" id="toc-try-the-model-with-a-pipeline" class="nav-link" data-scroll-target="#try-the-model-with-a-pipeline"><span class="header-section-number">11.2</span> Try the model with a pipeline</a></li>
  <li><a href="#try-the-model-on-one-of-our-sequences" id="toc-try-the-model-on-one-of-our-sequences" class="nav-link" data-scroll-target="#try-the-model-on-one-of-our-sequences"><span class="header-section-number">11.3</span> Try the model on one of our sequences</a></li>
  <li><a href="#lets-try-to-prompt-the-model" id="toc-lets-try-to-prompt-the-model" class="nav-link" data-scroll-target="#lets-try-to-prompt-the-model"><span class="header-section-number">11.4</span> Let‚Äôs try to prompt the model</a></li>
  </ul></li>
  <li><a href="#fine-tuning-our-model" id="toc-fine-tuning-our-model" class="nav-link" data-scroll-target="#fine-tuning-our-model"><span class="header-section-number">12</span> Fine-tuning our model</a></li>
  <li><a href="#load-the-trained-model-back-in-and-see-how-it-performs" id="toc-load-the-trained-model-back-in-and-see-how-it-performs" class="nav-link" data-scroll-target="#load-the-trained-model-back-in-and-see-how-it-performs"><span class="header-section-number">13</span> Load the trained model back in and see how it performs</a></li>
  <li><a href="#counting-the-number-of-parameters-in-our-model" id="toc-counting-the-number-of-parameters-in-our-model" class="nav-link" data-scroll-target="#counting-the-number-of-parameters-in-our-model"><span class="header-section-number">14</span> Counting the number of parameters in our model</a></li>
  <li><a href="#uploading-our-fine-tuned-model-to-the-hugging-face-hub" id="toc-uploading-our-fine-tuned-model-to-the-hugging-face-hub" class="nav-link" data-scroll-target="#uploading-our-fine-tuned-model-to-the-hugging-face-hub"><span class="header-section-number">15</span> Uploading our fine-tuned model to the Hugging Face Hub</a></li>
  <li><a href="#turn-our-model-into-a-demo" id="toc-turn-our-model-into-a-demo" class="nav-link" data-scroll-target="#turn-our-model-into-a-demo"><span class="header-section-number">16</span> Turn our model into a demo</a>
  <ul class="collapse">
  <li><a href="#creating-the-app.py-file" id="toc-creating-the-app.py-file" class="nav-link" data-scroll-target="#creating-the-app.py-file"><span class="header-section-number">16.1</span> Creating the <code>app.py</code> file</a></li>
  <li><a href="#create-the-readme.md-file" id="toc-create-the-readme.md-file" class="nav-link" data-scroll-target="#create-the-readme.md-file"><span class="header-section-number">16.2</span> Create the <code>README.md</code> file</a></li>
  <li><a href="#create-the-requirements.txt-file" id="toc-create-the-requirements.txt-file" class="nav-link" data-scroll-target="#create-the-requirements.txt-file"><span class="header-section-number">16.3</span> Create the <code>requirements.txt</code> file</a></li>
  <li><a href="#upload-our-demo-to-the-hugging-face-hub" id="toc-upload-our-demo-to-the-hugging-face-hub" class="nav-link" data-scroll-target="#upload-our-demo-to-the-hugging-face-hub"><span class="header-section-number">16.4</span> Upload our demo to the Hugging Face Hub</a></li>
  </ul></li>
  <li><a href="#bonus-speeding-up-our-model-with-batched-inference" id="toc-bonus-speeding-up-our-model-with-batched-inference" class="nav-link" data-scroll-target="#bonus-speeding-up-our-model-with-batched-inference"><span class="header-section-number">17</span> Bonus: Speeding up our model with batched inference</a></li>
  <li><a href="#bonus-performing-evaluations-on-our-model" id="toc-bonus-performing-evaluations-on-our-model" class="nav-link" data-scroll-target="#bonus-performing-evaluations-on-our-model"><span class="header-section-number">18</span> Bonus: Performing evaluations on our model</a></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps"><span class="header-section-number">19</span> Next steps</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mrdbourke/learn-huggingface/issues" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notebooks/hugging_face_llm_full_fine_tune_tutorial.html">Natural Language Processing (NLP)</a></li><li class="breadcrumb-item"><a href="../notebooks/hugging_face_llm_full_fine_tune_tutorial.html">Fully fine-tune an LLM to do structrued data extraction</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Fully Fine-tune a Small Language Model with Hugging Face Tutorial</h1>
</div>

<div>
  <div class="description">
    Learn how to fully fine-tune a Small Language Model on a custom dataset with Hugging Face Transformers.
  </div>
</div>


<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<p><a target="_blank" href="https://colab.research.google.com/github/mrdbourke/learn-huggingface/blob/main/notebooks/hugging_face_llm_full_fine_tune_tutorial.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<blockquote class="blockquote">
<p><strong>Note:</strong> If you‚Äôre running in Google Colab, make sure to enable GPU usage by going to Runtime -&gt; Change runtime type -&gt; select GPU.</p>
</blockquote>
<p><a href="https://github.com/mrdbourke/learn-huggingface/blob/main/notebooks/hugging_face_llm_full_fine_tune_tutorial.ipynb">Source Code</a></p>
<section id="overview" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="overview"><span class="header-section-number">1</span> Overview</h2>
<p>Why fine-tune a Small Language Model (SLM)?</p>
<p>Because we want small model for a specific task (e.g.&nbsp;don‚Äôt to pay API credits or leak our data online).</p>
<p>If we have our own model‚Ä¶ we can run it anywhere and everywhere we like.</p>
</section>
<section id="how-to-fine-tune-an-llm-model" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="how-to-fine-tune-an-llm-model"><span class="header-section-number">2</span> How to fine-tune an LLM model</h2>
<p>There are several ways to fine-tune an LLM including reinforcement learning (RL) and Supervised Fine-tuning (SFT).</p>
<p>We are going to do SFT because it‚Äôs the most straightforward.</p>
<p>Basically SFT = give samples of inputs and outputs and the model learns to map a given input to a given output.</p>
<p>For example if our goal was to extract names:</p>
<ul>
<li>Input: Hello my name is Daniel</li>
<li>Output: Daniel</li>
</ul>
<p>Note:</p>
<ul>
<li>Our inputs can be <em>any</em> kind of input string.</li>
<li>Our outputs will be fine-tuned to conform to a structured data pattern.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>LLM Fine-tuning Mindset
</div>
</div>
<div class="callout-body-container callout-body">
<p>In LLM world, data inputs are tokens and data outputs are tokens.</p>
<p>A token is a numerical representation of some kind of data.</p>
<p>Computers like numbers (not images, text, videos, etc).</p>
<p>Everything must be turned into numbers.</p>
<p>And data = a very broad term.</p>
<p>It could be text, images, video (series of images), audio, DNA sequences, Excel spreadsheets, you name it.</p>
<p>The goal of the LLM is to be given an input sequence of tokens and then predict the following tokens.</p>
<p>So with this mindset, you can think of any problem as <strong>tokens in, tokens out</strong>.</p>
<p>Ask yourself: <em>What tokens do I want to put in and what tokens do I want my model to return?</em></p>
<p>In our case, we want to put in almost any string input. And we want to get back structured information specifically related to food and drinks.</p>
<p>This a very specific use case, however, the beauty of LLMs being so general is that you can apply this <strong>tokens in, tokens out</strong> mindset to almost anything.</p>
<p>If you‚Äôve got an existing dataset (no problem if you don‚Äôt, you can create one, let me know if you‚Äôd like a guide on this), chances are, you can fine-tune an LLM to do pretty well on it.</p>
</div>
</div>
</section>
<section id="what-were-cooking" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="what-were-cooking"><span class="header-section-number">3</span> What we‚Äôre cooking</h2>
<p>We‚Äôre going to build a SLM (Small Language Model) to extract food and drink items from text.</p>
<p>Why?</p>
<p>If we needed to go over a large dataset of image captions and filter them for food items (we could then use these filtered captions for a food app).</p>
<p>TK image - example of what we‚Äôre doing.</p>
</section>
<section id="ingredients" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="ingredients"><span class="header-section-number">4</span> Ingredients</h2>
<ol type="1">
<li>Model (<a href="https://huggingface.co/google/gemma-3-270m-it">Gemma-3-270M</a>)</li>
<li><a href="https://huggingface.co/datasets/mrdbourke/FoodExtract-1k">Dataset</a> (a pre-baked dataset to extract foods and drinks from text)</li>
<li>Training code (Hugging Face Transformers + TRL)</li>
<li>Eval code</li>
<li>Demo</li>
</ol>
</section>
<section id="method" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="method"><span class="header-section-number">5</span> Method</h2>
<ol type="1">
<li>Download model - <a href="https://huggingface.co/docs/transformers/index">Hugging Face <code>transformers</code></a></li>
<li>Download dataset - <a href="https://huggingface.co/docs/datasets/en/index">Hugging Face <code>datasets</code></a></li>
<li>Inspect dataset - Hugging Face <code>datasets</code></li>
<li>Train model on dataset - <a href="https://huggingface.co/docs/trl/en/index">Hugging Face <code>trl</code></a> (TRL = Transformers Reinforcement Learning)</li>
<li>Eval model - basically just look at a bunch of samples</li>
<li>Create an interactive demo - Hugging Face <code>gradio</code></li>
<li>Bonus: Make the demo public so other people can use it - Hugging Face Spaces</li>
</ol>
</section>
<section id="fine-tuning-llm-vs-rag" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="fine-tuning-llm-vs-rag"><span class="header-section-number">6</span> Fine-tuning LLM vs RAG</h2>
<ul>
<li>Fine-tuning = To do a very specific task, e.g.&nbsp;structured data extraction.
<ul>
<li>An example would be you‚Äôre an insurance company who gets 10,000 emails a day and you want to extract structured data directly from these emails to JSON.</li>
</ul></li>
<li>RAG = You want to inject custom knowledge into an LLM.
<ul>
<li>An example would be you‚Äôre an insurance company wanting to send <em>automatic</em> responses to people but you want the responses to include information from your own docs.</li>
</ul></li>
</ul>
</section>
<section id="why-fine-tune-your-own-model" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="why-fine-tune-your-own-model"><span class="header-section-number">7</span> Why fine-tune your own model?</h2>
<ol type="1">
<li>Own the model, can run on own hardware</li>
<li>Our task is simple enough to just use a small language model</li>
<li>No API calls needed</li>
<li>Can run in batch mode to get much faster inference than API calls</li>
<li>Model by default wasn‚Äôt very good at our task but now since fine-tuning, is <em>very good</em></li>
</ol>
</section>
<section id="definitions" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="definitions"><span class="header-section-number">8</span> Definitions</h2>
<p>Some quick definitions of what we‚Äôre doing.</p>
<ul>
<li><strong>Full fine-tuning</strong> - All weights on the model are updated. Often takes longer and requires larger hardware capacity, however, if your model is small enough (e.g.&nbsp;270M parameter or less), you can often do full fine-tuning.</li>
<li><strong>LORA fine-tuning</strong> (also known as partial fine-tuning) - <a href="https://arxiv.org/abs/2106.09685">Low Rank Adaptation</a> or training a small adapter to attach to your original model. Requires significantly less resources but <a href="https://thinkingmachines.ai/blog/lora/">can perform on par with full fine-tuning</a>.</li>
<li><strong>SLM (Small Language Model)</strong> - A subjective definition but to me a Small Language Model is a model with under 1B parameters, with added bonus for being under 500M parameters. Less parameters generally means less performance. However, when you have a specific task, SLMs often shine because they can be tailored for that specific task. If your task is ‚ÄúI want to create a chatbot capable of <em>anything</em>‚Äù, you‚Äôll generally want the biggest model you can reasonably serve. If your task is ‚ÄúI want to extract some structured data from raw text inputs‚Äù, you‚Äôll probably be surprised how well a SLM can perform.</li>
</ul>
</section>
<section id="import-dependencies" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="import-dependencies"><span class="header-section-number">9</span> Import dependencies</h2>
<blockquote class="blockquote">
<p><strong>Note:</strong> If you‚Äôre in Google Colab, you may have to install <code>trl</code>, <code>accelerate</code> and <code>gradio</code>.</p>
</blockquote>
<p>For Google Colab:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install trl accelerate gradio</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div id="421530e1" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> trl <span class="co"># trl = Transformers Reinforcement Learning -&gt; https://github.com/huggingface/trl </span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datasets </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> accelerate</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gradio <span class="im">as</span> gr</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="629dcca3" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the amount of GPU memory available (we need at least ~16GB)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.cuda.current_device()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    gpu_name <span class="op">=</span> torch.cuda.get_device_name(device)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    total_memory <span class="op">=</span> torch.cuda.get_device_properties(device).total_memory</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    allocated_memory <span class="op">=</span> torch.cuda.memory_allocated(device)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    reserved_memory <span class="op">=</span> torch.cuda.memory_reserved(device)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    free_memory <span class="op">=</span> total_memory <span class="op">-</span> reserved_memory</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"GPU: </span><span class="sc">{</span>gpu_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Total Memory:     </span><span class="sc">{</span>total_memory <span class="op">/</span> <span class="fl">1e6</span><span class="sc">:.2f}</span><span class="ss"> MB | </span><span class="sc">{</span>total_memory <span class="op">/</span> <span class="fl">1e9</span><span class="sc">:.2f}</span><span class="ss"> GB"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Allocated Memory: </span><span class="sc">{</span>allocated_memory <span class="op">/</span> <span class="fl">1e6</span><span class="sc">:.2f}</span><span class="ss"> MB | </span><span class="sc">{</span>allocated_memory <span class="op">/</span> <span class="fl">1e9</span><span class="sc">:.2f}</span><span class="ss"> GB"</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Reserved Memory:  </span><span class="sc">{</span>reserved_memory <span class="op">/</span> <span class="fl">1e6</span><span class="sc">:.2f}</span><span class="ss"> MB | </span><span class="sc">{</span>reserved_memory <span class="op">/</span> <span class="fl">1e9</span><span class="sc">:.2f}</span><span class="ss"> GB"</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Free Memory:      </span><span class="sc">{</span>free_memory <span class="op">/</span> <span class="fl">1e6</span><span class="sc">:.2f}</span><span class="ss"> MB | </span><span class="sc">{</span>free_memory <span class="op">/</span> <span class="fl">1e9</span><span class="sc">:.2f}</span><span class="ss"> GB"</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"No CUDA GPU available"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>GPU: NVIDIA GB10
Total Memory:     128524.03 MB | 128.52 GB
Allocated Memory: 0.00 MB | 0.00 GB
Reserved Memory:  0.00 MB | 0.00 GB
Free Memory:      128524.03 MB | 128.52 GB</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/mrdbourke/miniforge3/envs/ai/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(</code></pre>
</div>
</div>
</section>
<section id="setup-base-model" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="setup-base-model"><span class="header-section-number">10</span> Setup Base Model</h2>
<p>The base model we‚Äôll be using is <a href="https://huggingface.co/google/gemma-3-270m-it/tree/main">Gemma 3 270M</a> from Google.</p>
<p>It‚Äôs the same architecture style as larger LLMs such as Gemini but at a <em>much</em> smaller scale.</p>
<p>This is why we refer to it as a ‚ÄúSmall Language Model‚Äù or SLM.</p>
<p>We can load our model using <code>transformers</code>.</p>
<div id="4de92467" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"google/gemma-3-270m-it"</span> <span class="co"># note: "it" stands for "instruction tuned" which means the model has been tuned for following instructions</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    MODEL_NAME,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    dtype<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>, <span class="co"># put the model on the GPU</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    attn_implementation<span class="op">=</span><span class="st">"eager"</span> <span class="co"># could use flash_attention_2 but ran into issues... so stick with Eager for now</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="8ae5a6b9" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(MODEL_NAME)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Model on device: </span><span class="sc">{</span>model<span class="sc">.</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Model using dtype: </span><span class="sc">{</span>model<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Model on device: cuda:0
[INFO] Model using dtype: torch.bfloat16</code></pre>
</div>
</div>
<p>Our model requires numbers (tokens) as input.</p>
<p>We can turn strings into tokens via a tokenizer!</p>
<div id="e8d187d0" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>tokenizer(<span class="st">"Hello my name is Daniel"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>{'input_ids': [2, 9259, 1041, 1463, 563, 13108], 'attention_mask': [1, 1, 1, 1, 1, 1]}</code></pre>
</div>
</div>
<div id="5cf6296a" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(torch.tensor(tokenizer(<span class="st">"Hello my name is Daniel"</span>)[<span class="st">"input_ids"</span>]).unsqueeze(<span class="dv">0</span>).to(<span class="st">"cuda"</span>))</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>outputs.keys()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>odict_keys(['logits', 'past_key_values'])</code></pre>
</div>
</div>
</section>
<section id="get-dataset" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="get-dataset"><span class="header-section-number">11</span> Get dataset</h2>
<p>Our dataset is located here: <a href="https://huggingface.co/datasets/mrdbourke/FoodExtract-1k">https://huggingface.co/datasets/mrdbourke/FoodExtract-1k</a>.</p>
<p>It was created from image captions + random strings and then using <code>gpt-oss-120b</code> (a powerful open-source LLM) to do synthetic labelling.</p>
<p>For more on the dataset you can read the <a href="https://huggingface.co/datasets/mrdbourke/FoodExtract-1k">README.md</a> file explaining it.</p>
<p>The main thing we are concerned about is that we want the input to our model to be the <code>"sequence"</code> column and the output to be the <code>"gpt-oss-120b-label-condensed"</code> column.</p>
<p>We‚Äôll explore these below.</p>
<div id="5721a3b1" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"mrdbourke/FoodExtract-1k"</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Number of samples in the dataset: </span><span class="sc">{</span><span class="bu">len</span>(dataset[<span class="st">'train'</span>])<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Number of samples in the dataset: 1420</code></pre>
</div>
</div>
<div id="3fa6954c" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_random_idx(dataset):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns a random integer index based on the number of samples in the dataset."""</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    random_idx <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(dataset)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> random_idx</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>random_idx <span class="op">=</span> get_random_idx(dataset[<span class="st">"train"</span>])</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>random_sample <span class="op">=</span> dataset[<span class="st">"train"</span>][random_idx]</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>example_input <span class="op">=</span> random_sample[<span class="st">"sequence"</span>]</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>example_output <span class="op">=</span> random_sample[<span class="st">"gpt-oss-120b-label"</span>]</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>example_output_condensed <span class="op">=</span> random_sample[<span class="st">"gpt-oss-120b-label-condensed"</span>]</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Input:</span><span class="ch">\n</span><span class="sc">{</span>example_input<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Example structured output (what we want our model to learn to predict):"</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">eval</span>(example_output))</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Example output condensed (we'll train our model to predict the condensed output since it uses less tokens than JSON):"</span>)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(example_output_condensed)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Input:
The image depicts a classic British red double-decker bus adorned with a "Wedding Special" sign at the back, alongside a contact number, 01474-353-896, and a license plate reading JJ04020. The bus, highlighted by a gold trim separating its two levels, is parked within the vicinity of a Shell gas station, with the station's pumps and yellow Shell emblem visible on the left. The bus rests closely to the curb, making it appear as though maneuvering in this space might be challenging. Another vehicle is seen fueling up nearby. A prominently placed sign in the bottom right corner reads, "Shell Drivers Club Points: I exchange mine for coffee. Don't lose your points. Register today." The sky above is overcast, contributing to the overall cloudy ambiance of the scene. No people are visible in the photograph.

[INFO] Example structured output (what we want our model to learn to predict):
{'is_food_or_drink': True, 'tags': ['di', 'fa'], 'food_items': [], 'drink_items': ['coffee']}

[INFO] Example output condensed (we'll train our model to predict the condensed output since it uses less tokens than JSON):
food_or_drink: 1
tags: di, fa
foods: 
drinks: coffee</code></pre>
</div>
</div>
<p>Because we‚Äôd like to use our model to potentially filter a large corpus of data, we get it assign various tags to the text as well.</p>
<p>These are as follows.</p>
<div id="a4195702-aa43-42c6-9006-c43d1c02696e" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Our fine-tuned model will assign tags to text so we can easily filter them by type in the future</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>tags_dict <span class="op">=</span> {<span class="st">'np'</span>: <span class="st">'nutrition_panel'</span>,</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a> <span class="st">'il'</span>: <span class="st">'ingredient list'</span>,</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a> <span class="st">'me'</span>: <span class="st">'menu'</span>,</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a> <span class="st">'re'</span>: <span class="st">'recipe'</span>,</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a> <span class="st">'fi'</span>: <span class="st">'food_items'</span>,</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a> <span class="st">'di'</span>: <span class="st">'drink_items'</span>,</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a> <span class="st">'fa'</span>: <span class="st">'food_advertistment'</span>,</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a> <span class="st">'fp'</span>: <span class="st">'food_packaging'</span>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="format-the-dataset-into-llm-style-inputsoutputs" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="format-the-dataset-into-llm-style-inputsoutputs"><span class="header-section-number">11.1</span> Format the dataset into LLM-style inputs/outputs</h3>
<p>Right now we have examples of string-based inputs and structured outputs.</p>
<p>However, our LLMs generally want things in the format of:</p>
<pre><code>{"user": "Hello my name is Daniel",
"system": "Hi Daniel, I'm an LLM"}</code></pre>
<p>In other words, they want structure around the intputs and outputs rather than just raw information.</p>
<blockquote class="blockquote">
<p><strong>Resource:</strong> See the dataset formats and types in the TRL docs: https://huggingface.co/docs/trl/en/dataset_formats</p>
</blockquote>
<div id="6d909b36" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>random_sample</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>{'sequence': 'The image depicts a classic British red double-decker bus adorned with a "Wedding Special" sign at the back, alongside a contact number, 01474-353-896, and a license plate reading JJ04020. The bus, highlighted by a gold trim separating its two levels, is parked within the vicinity of a Shell gas station, with the station\'s pumps and yellow Shell emblem visible on the left. The bus rests closely to the curb, making it appear as though maneuvering in this space might be challenging. Another vehicle is seen fueling up nearby. A prominently placed sign in the bottom right corner reads, "Shell Drivers Club Points: I exchange mine for coffee. Don\'t lose your points. Register today." The sky above is overcast, contributing to the overall cloudy ambiance of the scene. No people are visible in the photograph.',
 'image_url': 'https://live.staticflickr.com/821/26423687377_ea486a390d_h.jpg',
 'class_label': 'not_food',
 'source': 'pixmo_cap_dataset',
 'char_len': 811.0,
 'word_count': 134.0,
 'syn_or_real': 'real',
 'uuid': 'bcc462d9-ae44-4fcc-9bd4-5786ee3774bc',
 'gpt-oss-120b-label': "{'is_food_or_drink': True, 'tags': ['di', 'fa'], 'food_items': [], 'drink_items': ['coffee']}",
 'gpt-oss-120b-label-condensed': 'food_or_drink: 1\ntags: di, fa\nfoods: \ndrinks: coffee',
 'target_food_names_to_use': None,
 'caption_detail_level': None,
 'num_foods': None,
 'target_image_point_of_view': None}</code></pre>
</div>
</div>
<div id="16769fa3" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_to_conversation(sample):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Helper function to convert an input sample to conversation style."""</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"messages"</span>: [</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: sample[<span class="st">"sequence"</span>]}, <span class="co"># Load the sequence from the dataset</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: sample[<span class="st">"gpt-oss-120b-label-condensed"</span>]} <span class="co"># Load the gpt-oss-120b generated label</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>sample_to_conversation(random_sample)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>{'messages': [{'role': 'user',
   'content': 'The image depicts a classic British red double-decker bus adorned with a "Wedding Special" sign at the back, alongside a contact number, 01474-353-896, and a license plate reading JJ04020. The bus, highlighted by a gold trim separating its two levels, is parked within the vicinity of a Shell gas station, with the station\'s pumps and yellow Shell emblem visible on the left. The bus rests closely to the curb, making it appear as though maneuvering in this space might be challenging. Another vehicle is seen fueling up nearby. A prominently placed sign in the bottom right corner reads, "Shell Drivers Club Points: I exchange mine for coffee. Don\'t lose your points. Register today." The sky above is overcast, contributing to the overall cloudy ambiance of the scene. No people are visible in the photograph.'},
  {'role': 'system',
   'content': 'food_or_drink: 1\ntags: di, fa\nfoods: \ndrinks: coffee'}]}</code></pre>
</div>
</div>
<div id="09cca9d2" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Map our sample_to_conversation function to dataset </span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> dataset.<span class="bu">map</span>(sample_to_conversation,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>                      batched<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"train"</span>][<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d78fb79d6b08419b834c9d54a21e17dd","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>{'sequence': 'A mouth-watering photograph captures a delectable dish centered on a rectangular white porcelain plate, resting on a rustic wooden tabletop indoors. In the background, a wooden cutting board with a long handle subtly enhances the setting. The plate is adorned with several generously-sized, cheese-stuffed peppers that have been roasted to perfection, their blistered skins marked by charred black spots. Split down the middle, the peppers reveal a creamy white cheese filling, enriched with a blend of aromatic herbs. Once stuffed, the peppers have been closed and roasted, achieving a luscious, smoky flavor.\n\nThe dish is elegantly garnished with vibrant cherry tomato halves, freshly chopped green herbs, and delicate sprinkles of small diced red onions. A light, possibly citrus-infused dressing, hinted by a sheen of oil or lime juice, gently coats the ensemble, adding an extra layer of freshness. The meticulous presentation and vivid colors make this image not only a feast for the stomach but also a feast for the eyes.',
 'image_url': 'http://i.imgur.com/X7cM9Df.jpg',
 'class_label': 'food',
 'source': 'pixmo_cap_dataset',
 'char_len': 1028.0,
 'word_count': 160.0,
 'syn_or_real': 'real',
 'uuid': '6720d6e0-5912-41e7-be50-85a2b63bfef9',
 'gpt-oss-120b-label': "{'is_food_or_drink': True, 'tags': ['fi', 'fa'], 'food_items': ['cheese-stuffed peppers', 'cherry tomato halves', 'green herbs', 'diced red onions', 'citrus-infused dressing', 'oil', 'lime juice', 'cheese'], 'drink_items': []}",
 'gpt-oss-120b-label-condensed': 'food_or_drink: 1\ntags: fi, fa\nfoods: cheese-stuffed peppers, cherry tomato halves, green herbs, diced red onions, citrus-infused dressing, oil, lime juice, cheese\ndrinks:',
 'target_food_names_to_use': None,
 'caption_detail_level': None,
 'num_foods': None,
 'target_image_point_of_view': None,
 'messages': [{'content': 'A mouth-watering photograph captures a delectable dish centered on a rectangular white porcelain plate, resting on a rustic wooden tabletop indoors. In the background, a wooden cutting board with a long handle subtly enhances the setting. The plate is adorned with several generously-sized, cheese-stuffed peppers that have been roasted to perfection, their blistered skins marked by charred black spots. Split down the middle, the peppers reveal a creamy white cheese filling, enriched with a blend of aromatic herbs. Once stuffed, the peppers have been closed and roasted, achieving a luscious, smoky flavor.\n\nThe dish is elegantly garnished with vibrant cherry tomato halves, freshly chopped green herbs, and delicate sprinkles of small diced red onions. A light, possibly citrus-infused dressing, hinted by a sheen of oil or lime juice, gently coats the ensemble, adding an extra layer of freshness. The meticulous presentation and vivid colors make this image not only a feast for the stomach but also a feast for the eyes.',
   'role': 'user'},
  {'content': 'food_or_drink: 1\ntags: fi, fa\nfoods: cheese-stuffed peppers, cherry tomato halves, green herbs, diced red onions, citrus-infused dressing, oil, lime juice, cheese\ndrinks:',
   'role': 'system'}]}</code></pre>
</div>
</div>
<p>Notice how we now have a <code>"messages"</code> key in our dataset samples.</p>
<div id="f027190f" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a train/test split</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> dataset[<span class="st">"train"</span>].train_test_split(test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>                                            shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>                                            seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Number #1 rule in machine learning</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Always train on the train set and test on the test set</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># This gives us an indication of how our model will perform in the real world</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>dataset</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['sequence', 'image_url', 'class_label', 'source', 'char_len', 'word_count', 'syn_or_real', 'uuid', 'gpt-oss-120b-label', 'gpt-oss-120b-label-condensed', 'target_food_names_to_use', 'caption_detail_level', 'num_foods', 'target_image_point_of_view', 'messages'],
        num_rows: 1136
    })
    test: Dataset({
        features: ['sequence', 'image_url', 'class_label', 'source', 'char_len', 'word_count', 'syn_or_real', 'uuid', 'gpt-oss-120b-label', 'gpt-oss-120b-label-condensed', 'target_food_names_to_use', 'caption_detail_level', 'num_foods', 'target_image_point_of_view', 'messages'],
        num_rows: 284
    })
})</code></pre>
</div>
</div>
</section>
<section id="try-the-model-with-a-pipeline" class="level3" data-number="11.2">
<h3 data-number="11.2" class="anchored" data-anchor-id="try-the-model-with-a-pipeline"><span class="header-section-number">11.2</span> Try the model with a pipeline</h3>
<div id="ea2738c3" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>easy_sample <span class="op">=</span> {<span class="st">"role"</span>: <span class="st">"user"</span>, </span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>               <span class="st">"content"</span>: <span class="st">"Hi my name is Daniel"</span>}</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_easy_sample(<span class="bu">input</span>):</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    template <span class="op">=</span> {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: <span class="bu">input</span>}</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> template</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="1a9871a6" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline </span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load model and use it as a pipeline</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>,</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>                model<span class="op">=</span>model,</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>                tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>input_text <span class="op">=</span> <span class="st">"Hi my name is Daniel. Please reply to me with a machine learning poem."</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>easy_sample <span class="op">=</span> create_easy_sample(<span class="bu">input</span><span class="op">=</span>input_text)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>input_prompt <span class="op">=</span> pipe.tokenizer.apply_chat_template([easy_sample], <span class="co"># pipeline tokenizer wants a list of inputs</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>                                                  tokenize<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>                                                  add_generation_prompt<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] This is the input prompt: </span><span class="sc">{</span>input_prompt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>default_outputs <span class="op">=</span> pipe(input_prompt,</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>                       max_new_tokens<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>                       disable_compile<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Input:</span><span class="ch">\n</span><span class="sc">{</span>input_text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output from </span><span class="sc">{</span>MODEL_NAME<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(default_outputs[<span class="dv">0</span>][<span class="st">"generated_text"</span>][<span class="bu">len</span>(input_prompt):])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Device set to use cuda:0</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] This is the input prompt: &lt;bos&gt;&lt;start_of_turn&gt;user
Hi my name is Daniel. Please reply to me with a machine learning poem.&lt;end_of_turn&gt;
&lt;start_of_turn&gt;model

[INFO] Input:
Hi my name is Daniel. Please reply to me with a machine learning poem.

[INFO] Output from google/gemma-3-270m-it:

Hi Daniel,
Your poem is a beautiful and thoughtful reflection on the journey of learning. It captures a sense of wonder and the endless possibilities that come with it.
</code></pre>
</div>
</div>
<p>Example machine learning poem generated by Gemma 3 270M (not too bad):</p>
<pre><code>Okay, Daniel, here's a machine learning poem. I've tried to capture a feeling of wonder and a bit of mystery.

The algorithm learns,
A silent, tireless quest.
Through data streams, it flows,
A symphony of thought.
Each point a new layer,
A learning bloom,
A future bright and clear.

It analyzes the data,
No single clue it knows.
It weaves a pattern true,
A story in the hue.
The world unfolds anew,
With subtle, complex view.

It's not just numbers,
But feeling, a soul.
A tapestry of grace,
A hopeful, vibrant space.
A learning, growing deep,
Secrets it will keep.</code></pre>
</section>
<section id="try-the-model-on-one-of-our-sequences" class="level3" data-number="11.3">
<h3 data-number="11.3" class="anchored" data-anchor-id="try-the-model-on-one-of-our-sequences"><span class="header-section-number">11.3</span> Try the model on one of our sequences</h3>
<div id="39394d8b" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a random sample</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>random_idx <span class="op">=</span> get_random_idx(dataset[<span class="st">"train"</span>])</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>random_train_sample <span class="op">=</span> dataset[<span class="st">"train"</span>][random_idx]</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the chat template</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>input_prompt <span class="op">=</span> pipe.tokenizer.apply_chat_template(conversation<span class="op">=</span>random_train_sample[<span class="st">"messages"</span>][:<span class="dv">1</span>],</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>                                                  tokenize<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>                                                  add_generation_prompt<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's run the default model on our input</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>default_outputs <span class="op">=</span> pipe(text_inputs<span class="op">=</span>input_prompt, max_new_tokens<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="co"># View and compare the outputs</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Input:</span><span class="ch">\n</span><span class="sc">{</span>input_prompt<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output:</span><span class="ch">\n</span><span class="sc">{</span>default_outputs[<span class="dv">0</span>][<span class="st">'generated_text'</span>][<span class="bu">len</span>(input_prompt):]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Input:
&lt;bos&gt;&lt;start_of_turn&gt;user
I)$=~(n;("`U++7Dr_w?-r9&amp;e&lt;.s%E`[,(YA,m=e1wdJ26*1zfC(iX-i,G(Y9f7@W`;'0GIJ.ip(SkfnRH\v4wPQ6Db n#f&amp;"rR#.png&lt;end_of_turn&gt;
&lt;start_of_turn&gt;model


[INFO] Output:
I$ = ~(n; ("`U++7Dr_w?-r9&amp;e&lt;.s%E`[,(YA,m=e1wdJ26*1zfC(iX-i,G(Y9f7@W`;'0GIJ.ip(SkfnRH\v4wPQ6Db n#f&amp;"rR#.png`)
</code></pre>
</div>
</div>
<p>By default the model produces a fairly generic response.</p>
<p>This is expected and good.</p>
<p>It means the model has a good baseline understanding of language.</p>
<p>If it responded with pure garbage, we might have an uphill battle.</p>
<p>However, this response type is not what we want. We want our model to respond with <strong>structured data</strong> based on the input.</p>
<p>Good news is we can adjust the patterns in our model to do just that.</p>
</section>
<section id="lets-try-to-prompt-the-model" class="level3" data-number="11.4">
<h3 data-number="11.4" class="anchored" data-anchor-id="lets-try-to-prompt-the-model"><span class="header-section-number">11.4</span> Let‚Äôs try to prompt the model</h3>
<p>We want a model to extract food and drink items from text.</p>
<p>By default the model will just reply to any text input with a generic response.</p>
<p>However, we can try and get our ideal outputs via prompting.</p>
<div id="a1f93224" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>prompt_instruction <span class="op">=</span> <span class="st">"""Given the following target input text from an image caption, please extract the food and drink items to a list. </span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="st">If there are no food or drink items, return an empty list.</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="st">Return in the following format:</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="st">food_items: [item_1, item_2, item_3]</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="st">drink_items: [item_4, item_5]</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="st">For example:</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="st">Input text: Hello my name is Daniel.</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="st">Output:</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="st">food_items: []</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="st">drink_items: []</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="st">Example 2:</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="st">Input text: A plate of rice cakes, salmon, cottage cheese and small cherry tomatoes with a cup of tea.</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="st">Output:</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="st">food_items: ['rice cakes', 'salmon', 'cottage cheese', 'cherry tomatoes']</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="st">drink_items: ['cup of tea']</span></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="st">Return only the formatted output and nothing else.</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a><span class="st">Target input text: &lt;targ_input_text&gt;"""</span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_input_message_content(<span class="bu">input</span>):</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>    original_content <span class="op">=</span> <span class="bu">input</span>[<span class="st">"messages"</span>][:<span class="dv">1</span>][<span class="dv">0</span>][<span class="st">"content"</span>]</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>    new_content <span class="op">=</span> prompt_instruction.replace(<span class="st">"&lt;targ_input_text&gt;"</span>, original_content)</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>    new_input <span class="op">=</span> [{<span class="st">"content"</span>: new_content,</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>                  <span class="st">"role"</span>: <span class="st">"user"</span>}]</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_input</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'[INFO] Original content:</span><span class="ch">\n</span><span class="sc">{</span>random_train_sample[<span class="st">"messages"</span>][:<span class="dv">1</span>][<span class="dv">0</span>][<span class="st">"content"</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'[INFO] New content with instructions in prompt:'</span>)</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(update_input_message_content(<span class="bu">input</span><span class="op">=</span>random_train_sample)[<span class="dv">0</span>][<span class="st">"content"</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Original content:
I)$=~(n;("`U++7Dr_w?-r9&amp;e&lt;.s%E`[,(YA,m=e1wdJ26*1zfC(iX-i,G(Y9f7@W`;'0GIJ.ip(SkfnRH\v4wPQ6Db n#f&amp;"rR#.png

[INFO] New content with instructions in prompt:
Given the following target input text from an image caption, please extract the food and drink items to a list. 
If there are no food or drink items, return an empty list.

Return in the following format:
food_items: [item_1, item_2, item_3]
drink_items: [item_4, item_5]

For example:
Input text: Hello my name is Daniel.
Output:
food_items: []
drink_items: []

Example 2:
Input text: A plate of rice cakes, salmon, cottage cheese and small cherry tomatoes with a cup of tea.
Output:
food_items: ['rice cakes', 'salmon', 'cottage cheese', 'cherry tomatoes']
drink_items: ['cup of tea']

Return only the formatted output and nothing else.

Target input text: I)$=~(n;("`U++7Dr_w?-r9&amp;e&lt;.s%E`[,(YA,m=e1wdJ26*1zfC(iX-i,G(Y9f7@W`;'0GIJ.ip(SkfnRH\v4wPQ6Db n#f&amp;"rR#.png</code></pre>
</div>
</div>
<div id="22ed0f84" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the chat template</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>updated_input_prompt <span class="op">=</span> update_input_message_content(<span class="bu">input</span><span class="op">=</span>random_train_sample)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>input_prompt <span class="op">=</span> pipe.tokenizer.apply_chat_template(conversation<span class="op">=</span>updated_input_prompt,</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>                                                  tokenize<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>                                                  add_generation_prompt<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's run the default model on our input</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>default_outputs <span class="op">=</span> pipe(text_inputs<span class="op">=</span>input_prompt, </span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>                       max_new_tokens<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="co"># View and compare the outputs</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Input:</span><span class="ch">\n</span><span class="sc">{</span>input_prompt<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output:</span><span class="ch">\n</span><span class="sc">{</span>default_outputs[<span class="dv">0</span>][<span class="st">'generated_text'</span>][<span class="bu">len</span>(input_prompt):]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Input:
&lt;bos&gt;&lt;start_of_turn&gt;user
Given the following target input text from an image caption, please extract the food and drink items to a list. 
If there are no food or drink items, return an empty list.

Return in the following format:
food_items: [item_1, item_2, item_3]
drink_items: [item_4, item_5]

For example:
Input text: Hello my name is Daniel.
Output:
food_items: []
drink_items: []

Example 2:
Input text: A plate of rice cakes, salmon, cottage cheese and small cherry tomatoes with a cup of tea.
Output:
food_items: ['rice cakes', 'salmon', 'cottage cheese', 'cherry tomatoes']
drink_items: ['cup of tea']

Return only the formatted output and nothing else.

Target input text: I)$=~(n;("`U++7Dr_w?-r9&amp;e&lt;.s%E`[,(YA,m=e1wdJ26*1zfC(iX-i,G(Y9f7@W`;'0GIJ.ip(SkfnRH\v4wPQ6Db n#f&amp;"rR#.png&lt;end_of_turn&gt;
&lt;start_of_turn&gt;model


[INFO] Output:
```python
def extract_food_and_drink_items(text):
    food_items = []
    drink_items = []

    try:
        if text == "I)$=~(n;(": "U++7Dr_w?-r9&amp;e&lt;.s%E`[,(YA,m=e1wdJ26*1zfC(iX-i,G(Y9f7@W`;'0GIJ.ip(SkfnRH\v4wPQ6Db n#f&amp;"rR#.png"): "U++7Dr_w?-r9&amp;e&lt;.s%E`[,(YA,m=e1wdJ26*1zfC(iX-i,G(Y9f7@W`;'0GIJ.ip(SkfnRH\v4wPQ6Db n#f&amp;"rR#.png"): "U++7Dr_w?-r9&amp;e&lt;.s%E`[,(FjY,m=e1wdJ26*1zfC(iX-i,G(Y9f7@W`;'0GIJ.ip(Sk</code></pre>
</div>
</div>
<div id="ff2b1392" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This is our input</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(random_train_sample[<span class="st">"messages"</span>][<span class="dv">0</span>][<span class="st">"content"</span>])</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co"># This is our ideal output: </span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(random_train_sample[<span class="st">"messages"</span>][<span class="dv">1</span>][<span class="st">"content"</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>I)$=~(n;("`U++7Dr_w?-r9&amp;e&lt;.s%E`[,(YA,m=e1wdJ26*1zfC(iX-i,G(Y9f7@W`;'0GIJ.ip(SkfnRH\v4wPQ6Db n#f&amp;"rR#.png

food_or_drink: 0
tags: 
foods: 
drinks:</code></pre>
</div>
</div>
<p>Okay looks like our small LLM doesn‚Äôt do what we want it to do‚Ä¶ it starts to reply with Python text or unreliably extracts foods and drinks from text in a non-uniform format.</p>
<p>No matter, we can fine-tune it so it does our specific task!</p>
</section>
</section>
<section id="fine-tuning-our-model" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="fine-tuning-our-model"><span class="header-section-number">12</span> Fine-tuning our model</h2>
<p>Steps:</p>
<ol type="1">
<li>Setup SFTConfig (Supervised Fine-tuning Config) - https://huggingface.co/docs/trl/v0.26.2/en/sft_trainer#trl.SFTConfig</li>
<li>Use SFTTrainer to train our model on our supervised samples (from our dataset above) - https://huggingface.co/docs/trl/v0.26.2/en/sft_trainer#trl.SFTTrainer</li>
</ol>
<div id="c5f29e96" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting up our SFTConfig</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trl <span class="im">import</span> SFTConfig</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>torch_dtype <span class="op">=</span> model.dtype</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>CHECKPOINT_DIR_NAME <span class="op">=</span> <span class="st">"./checkpoint_models"</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>BASE_LEARNING_RATE <span class="op">=</span> <span class="fl">5e-5</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Using dtype: </span><span class="sc">{</span>torch_dtype<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Using learning rate: </span><span class="sc">{</span>BASE_LEARNING_RATE<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>sft_config <span class="op">=</span> SFTConfig(</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span>CHECKPOINT_DIR_NAME,</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    max_length<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    packing<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">16</span>, <span class="co"># Note: you can change this depending on the amount of VRAM your GPU has</span></span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>    gradient_checkpointing<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>    optim<span class="op">=</span><span class="st">"adamw_torch_fused"</span>, <span class="co"># Note: if you try "adamw", you will get an error</span></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span>BASE_LEARNING_RATE,</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">True</span> <span class="cf">if</span> torch_dtype <span class="op">==</span> torch.float16 <span class="cf">else</span> <span class="va">False</span>,</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>    bf16<span class="op">=</span><span class="va">True</span> <span class="cf">if</span> torch_dtype <span class="op">==</span> torch.float16 <span class="cf">else</span> <span class="va">False</span>,</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>    lr_scheduler_type<span class="op">=</span><span class="st">"constant"</span>,</span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>    push_to_hub<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="va">None</span></span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a><span class="co"># There are a lot of settings in the sft_config, so feel free to uncomment this and inspect it if you want</span></span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a><span class="co">#sft_config</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Using dtype: torch.bfloat16
[INFO] Using learning rate: 5e-05</code></pre>
</div>
</div>
<p>Config setup, now we can train our model with <code>SFTTrainer</code>!</p>
<div id="a6a74f51" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Supervised Fine-Tuning = provide input and desired output samples</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trl <span class="im">import</span> SFTTrainer</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Trainer object</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> SFTTrainer(</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>sft_config,</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>dataset[<span class="st">"train"</span>],</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>dataset[<span class="st">"test"</span>],</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    processing_class<span class="op">=</span>tokenizer </span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"bf86e768819247929b0c637c6ff12e3c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0cfaaf23ebc743ddb73dda02064da40e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d33d0d6982314303b25568da7c3d1c50","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"cee5ceef556046b5b45c74d5aaed2dfb","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The model is already on multiple devices. Skipping the move to device specified in `args`.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 2, 'pad_token_id': 0}.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
      
      <progress value="213" max="213" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [213/213 05:18, Epoch 3/3]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
<th data-quarto-table-cell-role="th">Entropy</th>
<th data-quarto-table-cell-role="th">Num Tokens</th>
<th data-quarto-table-cell-role="th">Mean Token Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1.761000</td>
<td>2.275403</td>
<td>2.180556</td>
<td>170969.000000</td>
<td>0.572724</td>
</tr>
<tr class="even">
<td>2</td>
<td>1.500400</td>
<td>2.291959</td>
<td>1.943576</td>
<td>341938.000000</td>
<td>0.574516</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1.542500</td>
<td>2.369414</td>
<td>1.751302</td>
<td>512907.000000</td>
<td>0.573615</td>
</tr>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>TrainOutput(global_step=213, training_loss=1.8445255733991452, metrics={'train_runtime': 321.0184, 'train_samples_per_second': 10.616, 'train_steps_per_second': 0.664, 'total_flos': 873751312465920.0, 'train_loss': 1.8445255733991452, 'epoch': 3.0})</code></pre>
</div>
</div>
<p>Woohoo! Looks like our training accuracy went up.</p>
<p>Let‚Äôs inspect the loss curves.</p>
<div id="0e176da4" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Access the log history</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>log_history <span class="op">=</span> trainer.state.log_history</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract training / validation loss</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>train_losses <span class="op">=</span> [log[<span class="st">"loss"</span>] <span class="cf">for</span> log <span class="kw">in</span> log_history <span class="cf">if</span> <span class="st">"loss"</span> <span class="kw">in</span> log]</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>epoch_train <span class="op">=</span> [log[<span class="st">"epoch"</span>] <span class="cf">for</span> log <span class="kw">in</span> log_history <span class="cf">if</span> <span class="st">"loss"</span> <span class="kw">in</span> log]</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>eval_losses <span class="op">=</span> [log[<span class="st">"eval_loss"</span>] <span class="cf">for</span> log <span class="kw">in</span> log_history <span class="cf">if</span> <span class="st">"eval_loss"</span> <span class="kw">in</span> log]</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>epoch_eval <span class="op">=</span> [log[<span class="st">"epoch"</span>] <span class="cf">for</span> log <span class="kw">in</span> log_history <span class="cf">if</span> <span class="st">"eval_loss"</span> <span class="kw">in</span> log]</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the training loss</span></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_train, train_losses, label<span class="op">=</span><span class="st">"Training Loss"</span>)</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_eval, eval_losses, label<span class="op">=</span><span class="st">"Validation Loss"</span>)</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Training and Validation Loss per Epoch"</span>)</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_llm_full_fine_tune_tutorial_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="ab9d8bbd" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>trainer.save_model()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="8abb4c35" class="cell" data-execution_count="39">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove all the checkpoint folders (since we've already saved the best model)</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>rm <span class="op">-</span>rf .<span class="op">/</span>checkpoint_models<span class="op">/</span>checkpoint<span class="op">-*/*</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>rm <span class="op">-</span>rf .<span class="op">/</span>checkpoint_models<span class="op">/</span>checkpoint<span class="op">-*</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)</code></pre>
</div>
</div>
</section>
<section id="load-the-trained-model-back-in-and-see-how-it-performs" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="load-the-trained-model-back-in-and-see-how-it-performs"><span class="header-section-number">13</span> Load the trained model back in and see how it performs</h2>
<p>We‚Äôve now fine-tuned our own Gemma 3 270M to do a specific task, let‚Äôs load it back in and see how it performs.</p>
<div id="d2d5d374" class="cell" data-execution_count="40">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>CHECKPOINT_DIR_NAME</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>'./checkpoint_models'</code></pre>
</div>
</div>
<div id="c2ebfeae" class="cell" data-execution_count="41">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the fine-tuned model and see how it goes</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load trained model</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>loaded_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    pretrained_model_name_or_path<span class="op">=</span>CHECKPOINT_DIR_NAME,</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>    dtype<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    attn_implementation<span class="op">=</span><span class="st">"eager"</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="739c608d" class="cell" data-execution_count="42">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>loaded_model_pipeline <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>,</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>                                 model<span class="op">=</span>loaded_model,</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>                                 tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>loaded_model_pipeline</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Device set to use cuda:0</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>&lt;transformers.pipelines.text_generation.TextGenerationPipeline at 0xef0124802930&gt;</code></pre>
</div>
</div>
<div id="8db929ad" class="cell" data-execution_count="43">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"test"</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>Dataset({
    features: ['sequence', 'image_url', 'class_label', 'source', 'char_len', 'word_count', 'syn_or_real', 'uuid', 'gpt-oss-120b-label', 'gpt-oss-120b-label-condensed', 'target_food_names_to_use', 'caption_detail_level', 'num_foods', 'target_image_point_of_view', 'messages'],
    num_rows: 284
})</code></pre>
</div>
</div>
<p>Let‚Äôs now perform inference with our fine-tuned model on a random sample from the test dataset (our model has never seen these samples).</p>
<div id="ee88d7fa" class="cell" data-execution_count="28">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a random sample</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>random_test_idx <span class="op">=</span> get_random_idx(dataset[<span class="st">"test"</span>])</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>random_test_sample <span class="op">=</span> dataset[<span class="st">"test"</span>][random_test_idx]</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the chat template</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>input_prompt <span class="op">=</span> pipe.tokenizer.apply_chat_template(conversation<span class="op">=</span>random_test_sample[<span class="st">"messages"</span>][:<span class="dv">1</span>],</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>                                                  tokenize<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>                                                  add_generation_prompt<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's run the default model on our input</span></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>default_outputs <span class="op">=</span> loaded_model_pipeline(text_inputs<span class="op">=</span>input_prompt, </span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>                                        max_new_tokens<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a><span class="co"># View and compare the outputs</span></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Input:</span><span class="ch">\n</span><span class="sc">{</span>input_prompt<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output:</span><span class="ch">\n</span><span class="sc">{</span>default_outputs[<span class="dv">0</span>][<span class="st">'generated_text'</span>][<span class="bu">len</span>(input_prompt):]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Input:
&lt;bos&gt;&lt;start_of_turn&gt;user
The image shows a package of "Helpful Harvest" dried vegetables. The ingredients include a variety of vegetables such as zucchini, carrots, mushrooms, onions, capsicum, and celery. The product is made from 100% vegetables and includes wonky and overly abundant produce where possible. The package notes that the ingredients and percentages may differ due to seasonal availability.

The allergens listed are milk, eggs, soy, sesame seeds, hazelnuts, pistachios, cashews, and almonds. The nutrition information indicates that the package contains 4 servings, with each serving size being 10g. The average quantities per serving are as follows:
- Energy: 125 kJ
- Protein: 1.9 g
- Gluten: 0 g (marked as gluten-free)
- Fat, total: 0.3 g
- Saturated fat: 0 g
- Carbohydrate: 3.4 g
- Sugars: 3.4 g
- Fibre: 3.4 g
- Sodium: 2.1 g

The average quantities per 100g are:
- Energy: 1250 kJ
- Protein: 18.7 g
- Fat, total: 2.8 g
- Saturated fat: 0 g
- Carbohydrate: 34.3 g
- Sugars: 34.3 g
- Fibre: 20.9 g
- Sodium: 282 mg

The package also mentions that 10g dry is equivalent to approximately 75-100g fresh when rehydrated, and each serve is a minimum of 1 adult serve of vegetables. The product is gluten-free, as indicated by a handwritten note on the package.&lt;end_of_turn&gt;
&lt;start_of_turn&gt;model


[INFO] Output:
food_or_drink: 1
tags: np, il, fi, fp
foods: zucchini, carrots, mushrooms, onions, capsicum, celery, wonky and overly abundant produce, milk, eggs, soy, sesame seeds, hazelnuts, pistachios, cashews, almonds
drinks:</code></pre>
</div>
</div>
<div id="1f424585" class="cell" data-execution_count="29">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(random_test_sample[<span class="st">"gpt-oss-120b-label-condensed"</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>food_or_drink: 1
tags: np, il, fi, fp
foods: zucchini, carrots, mushrooms, onions, capsicum, celery, milk, eggs, soy, sesame seeds, hazelnuts, pistachios, cashews, almonds
drinks:</code></pre>
</div>
</div>
</section>
<section id="counting-the-number-of-parameters-in-our-model" class="level2" data-number="14">
<h2 data-number="14" class="anchored" data-anchor-id="counting-the-number-of-parameters-in-our-model"><span class="header-section-number">14</span> Counting the number of parameters in our model</h2>
<div id="56c4e8ce" class="cell" data-execution_count="30">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_model_num_params(model):</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns the number of trainable, non-trainable and total parameters of a PyTorch model.</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    trainable_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    non_trainable_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> <span class="kw">not</span> p.requires_grad)</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>    total_params <span class="op">=</span> trainable_params <span class="op">+</span> non_trainable_params</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"trainable_params"</span>: trainable_params,</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"non_trainable_params"</span>: non_trainable_params,</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"total_params"</span>: total_params}</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Get parameters of our fine-tuned model</span></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>model_params <span class="op">=</span> get_model_num_params(loaded_model)</span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Trainable parameters: </span><span class="sc">{</span>model_params[<span class="st">'trainable_params'</span>]<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Non-trainable parameters: </span><span class="sc">{</span>model_params[<span class="st">'non_trainable_params'</span>]<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total parameters: </span><span class="sc">{</span>model_params[<span class="st">'total_params'</span>]<span class="sc">:,}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Trainable parameters: 268,098,176
Non-trainable parameters: 0
Total parameters: 268,098,176</code></pre>
</div>
</div>
<div id="10c365fe" class="cell" data-execution_count="31">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Our model is 270M parameters, GPT-OSS-120B is 120B parameters</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="dv">120_000_000_000</span> <span class="op">/</span> <span class="dv">270_000_000</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>444.44444444444446</code></pre>
</div>
</div>
<p>By fine-tuning Gemma 3 270M we distill the capabilities of a 120B parameter model into a model 444x smaller.</p>
</section>
<section id="uploading-our-fine-tuned-model-to-the-hugging-face-hub" class="level2" data-number="15">
<h2 data-number="15" class="anchored" data-anchor-id="uploading-our-fine-tuned-model-to-the-hugging-face-hub"><span class="header-section-number">15</span> Uploading our fine-tuned model to the Hugging Face Hub</h2>
<p>We can upload our fine-tuned model to the Hugging Face Hub so other people can use it and test it out.</p>
<p>First, let‚Äôs load it in ourselves and confirm it works how we‚Äôd like it to.</p>
<div id="60366c1e" class="cell" data-execution_count="32">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>MODEL_PATH <span class="op">=</span> <span class="st">"./checkpoint_models/"</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model into a pipeline</span></span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>loaded_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    pretrained_model_name_or_path<span class="op">=</span>MODEL_PATH,</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>    dtype<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>    attn_implementation<span class="op">=</span><span class="st">"eager"</span></span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>    MODEL_PATH</span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a pipeline from the loaded model</span></span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>loaded_model_pipeline <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>,</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>                                 model<span class="op">=</span>loaded_model,</span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>                                 tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the loaded model on raw text (this won't work as well as formatted text)</span></span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a>test_input_message <span class="op">=</span> <span class="st">"Hello my name is Daniel!"</span></span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a>loaded_model_pipeline(test_input_message)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.
The tokenizer you are loading from './checkpoint_models/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
Device set to use cuda:0</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>[{'generated_text': "Hello my name is Daniel! I work as a software developer with a passion for creating user-friendly and intuitive web applications. My team, the team at Global Solutions Inc. (which I'm a part of), has a vibrant and collaborative team culture, where we share a common goal of making technology accessible for everyone. We believe that technology should serve our users, and we're committed to providing the best possible solutions."}]</code></pre>
</div>
</div>
<p>Let‚Äôs create a helper function to format our input text into message format.</p>
<div id="4546c352-9764-4cff-b1e9-3db235a3b1b1" class="cell" data-execution_count="33">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_message(<span class="bu">input</span>):</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: <span class="bu">input</span>}]</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>input_formatted <span class="op">=</span> format_message(<span class="bu">input</span><span class="op">=</span>test_input_message)</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>input_formatted</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>[{'role': 'user', 'content': 'Hello my name is Daniel!'}]</code></pre>
</div>
</div>
<p>Now we can turn it into a prompt with our <code>tokenizer</code> and the <code>apply_chat_template</code> method.</p>
<div id="c7686504-0d33-4751-a8b4-55107719ef9d" class="cell" data-execution_count="34">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>input_prompt <span class="op">=</span> loaded_model_pipeline.tokenizer.apply_chat_template(conversation<span class="op">=</span>input_formatted,</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>                                                                   tokenize<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>                                                                   add_generation_prompt<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>input_prompt</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>'&lt;bos&gt;&lt;start_of_turn&gt;user\nHello my name is Daniel!&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model\n'</code></pre>
</div>
</div>
<p>Beautiful! Time to run our fine-tuned model!</p>
<div id="41583a8d-13a8-4d57-b7d4-99f7c6619843" class="cell" data-execution_count="35">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>loaded_model_outputs <span class="op">=</span> loaded_model_pipeline(text_inputs<span class="op">=</span>input_prompt,</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>                                             max_new_tokens<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="co"># View and compare the outputs</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Input:</span><span class="ch">\n</span><span class="sc">{</span>input_prompt<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output:</span><span class="ch">\n</span><span class="sc">{</span>loaded_model_outputs[<span class="dv">0</span>][<span class="st">'generated_text'</span>][<span class="bu">len</span>(input_prompt):]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Input:
&lt;bos&gt;&lt;start_of_turn&gt;user
Hello my name is Daniel!&lt;end_of_turn&gt;
&lt;start_of_turn&gt;model


[INFO] Output:
food_or_drink: 0
tags: 
foods: 
drinks:</code></pre>
</div>
</div>
<p>Okay let‚Äôs make another helper function to predict on any given sample input.</p>
<p>We‚Äôll also return the inference time of our model so we can see how long things take.</p>
<div id="2341bd46-23d7-424a-b59d-d1d52481890b" class="cell" data-execution_count="36">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pred_on_text(input_text):</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>    raw_output <span class="op">=</span> loaded_model_pipeline(text_inputs<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>                                                    <span class="st">"content"</span>: input_text}],</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>                                       max_new_tokens<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>                                       disable_compile<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> time.time()</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>    total_time <span class="op">=</span> <span class="bu">round</span>(end_time <span class="op">-</span> start_time, <span class="dv">4</span>)</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>    generated_text <span class="op">=</span> raw_output[<span class="dv">0</span>][<span class="st">"generated_text"</span>][<span class="dv">1</span>][<span class="st">"content"</span>]</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> generated_text, raw_output, total_time</span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-17"><a href="#cb74-17" aria-hidden="true" tabindex="-1"></a>pred_on_text(input_text<span class="op">=</span><span class="st">"British Breakfast with baked beans, fried eggs, black pudding, sausages, bacon, mushrooms, a cup of tea and toast and fried tomatoes"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>('food_or_drink: 1\ntags: fi\nfoods: baked beans, fried eggs, black pudding, sausages, bacon, mushrooms, toast, fried tomatoes\ndrinks: tea',
 [{'generated_text': [{'role': 'user',
     'content': 'British Breakfast with baked beans, fried eggs, black pudding, sausages, bacon, mushrooms, a cup of tea and toast and fried tomatoes'},
    {'role': 'assistant',
     'content': 'food_or_drink: 1\ntags: fi\nfoods: baked beans, fried eggs, black pudding, sausages, bacon, mushrooms, toast, fried tomatoes\ndrinks: tea'}]}],
 0.5977)</code></pre>
</div>
</div>
<p>Nice! Looks like our model is working well enough (of course we could always improve it over time with more testing and different samples).</p>
<p>Let‚Äôs upload it to the Hugging Face Hub.</p>
<p>We can do so using the <a href="https://huggingface.co/docs/huggingface_hub/en/index"><code>huggingface_hub</code> library</a>.</p>
<div id="b536ac2f-fd95-441f-a708-b1b685e07e33" class="cell" data-execution_count="44">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> HfApi, create_repo</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>api <span class="op">=</span> HfApi()</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Give our model a name (this is in the format [Hugging Face Username]/[Target Model Name]</span></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>repo_id <span class="op">=</span> <span class="st">"mrdbourke/FoodExtract-gemma-3-270m-fine-tune-v1"</span></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the repo</span></span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>create_repo(repo_id, </span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>            repo_type<span class="op">=</span><span class="st">"model"</span>, </span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>            exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Upload the entire model folder containing our model files</span></span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>api.upload_folder(</span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>    folder_path<span class="op">=</span><span class="st">"./checkpoint_models/"</span>,</span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a>    repo_id<span class="op">=</span>repo_id,</span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>    repo_type<span class="op">=</span><span class="st">"model"</span></span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"52bc5c3791a04d24b416f4cae180ba9d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a144f19d4d0c453981c78ff425878dca","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>CommitInfo(commit_url='https://huggingface.co/mrdbourke/FoodExtract-gemma-3-270m-fine-tune-v1/commit/825d826ada01faf86dcbd0c5c4d77504f3fa5d04', commit_message='Upload folder using huggingface_hub', commit_description='', oid='825d826ada01faf86dcbd0c5c4d77504f3fa5d04', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mrdbourke/FoodExtract-gemma-3-270m-fine-tune-v1', endpoint='https://huggingface.co', repo_type='model', repo_id='mrdbourke/FoodExtract-gemma-3-270m-fine-tune-v1'), pr_revision=None, pr_num=None)</code></pre>
</div>
</div>
<p>Woohoo! Our model is officially on the Hugging Face Hub.</p>
<p>Now not only can we redownload it and use it again, others can download it and use it for themselves (of course you can make the model private if you like too).</p>
</section>
<section id="turn-our-model-into-a-demo" class="level2" data-number="16">
<h2 data-number="16" class="anchored" data-anchor-id="turn-our-model-into-a-demo"><span class="header-section-number">16</span> Turn our model into a demo</h2>
<p>Right now our model seems to be working quite well for our specific use case.</p>
<p>However, it takes some coding to be able to use it.</p>
<p>What if we wanted to allow someone who wasn‚Äôt familiar with programming to try it out?</p>
<p>To do so, we can turn our model into a <a href="https://www.gradio.app">Gradio</a> demo and upload it to <a href="https://huggingface.co/spaces">Hugging Face Spaces</a> (a place to share all kinds of small applications).</p>
<p>Gradio allows us to turn our model into an easy to use and sharable demo anyone can try.</p>
<p>Gradio demos work on the premise of: input (text) -&gt; function (our model) -&gt; output (text)</p>
<p>We‚Äôve already go a function ready with <code>pred_on_text</code> so we can wrap this with some Gradio code.</p>
<p>To create a sharable demo, we‚Äôll need the following files:</p>
<ul>
<li><code>app.py</code> - Entry point for our app, all of our application code will go in here.</li>
<li><code>README.md</code> - Tells people what our app does.
<ul>
<li><strong>Note:</strong> Hugging Face Spaces use a special ‚Äúfront matter‚Äù (text at the start of a <code>README.md</code> file) to add various attributes to a Hugging Face Space, we‚Äôll see this below.</li>
</ul></li>
<li><code>requirements.txt</code> - Tells Hugging Face Spaces what our app requires.
<ul>
<li><code>torch</code>, <code>transformers</code>, <code>gradio</code>, <code>accelerate</code></li>
</ul></li>
</ul>
<p>Let‚Äôs make a folder to store our demo application.</p>
<div id="fa4f64de-af13-47c7-ba5f-9b176bb58880" class="cell" data-execution_count="45">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir demos<span class="op">/</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir demos<span class="op">/</span>FoodExtract</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)</code></pre>
</div>
</div>
<section id="creating-the-app.py-file" class="level3" data-number="16.1">
<h3 data-number="16.1" class="anchored" data-anchor-id="creating-the-app.py-file"><span class="header-section-number">16.1</span> Creating the <code>app.py</code> file</h3>
<p>When running our app on Hugging Face Spaces, we have the option to run our model on a GPU thanks to Hugging Face‚Äôs <a href="https://huggingface.co/docs/hub/en/spaces-zerogpu">ZeroGPU</a> feature.</p>
<p>This is optional, however, it‚Äôs highly recommend you run a model such as Gemma 3 270M on the GPU as we‚Äôll see significant speedups.</p>
<p>You can run a function on a GPU by importing <code>spaces</code> and then using the <code>@spaces.GPU</code> decorator on your target function.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spaces</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="at">@spaces.GPU</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> function_to_run_on_the_gpu():</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>To ensure your model runs on the GPU, be sure to select a ZeroGPU instance in your Hugging Face Space settings.</p>
<div id="26b9719a-a52f-4a06-a5e7-e5e25c0f05cd" class="cell" data-execution_count="46">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile demos<span class="op">/</span>FoodExtract<span class="op">/</span>app.py</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load dependencies</span></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spaces <span class="co"># Optional: run our model on the GPU (this will be much faster inference)</span></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a><span class="at">@spaces.GPU</span> <span class="co"># Optional: run our model on the GPU (this will be much faster inference)</span></span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pred_on_text(input_text):</span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a>    raw_output <span class="op">=</span> loaded_model_pipeline(text_inputs<span class="op">=</span>[{<span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a>                                                    <span class="st">"content"</span>: input_text}],</span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a>                                       max_new_tokens<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb81-21"><a href="#cb81-21" aria-hidden="true" tabindex="-1"></a>                                       disable_compile<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb81-22"><a href="#cb81-22" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> time.time()</span>
<span id="cb81-23"><a href="#cb81-23" aria-hidden="true" tabindex="-1"></a>    total_time <span class="op">=</span> <span class="bu">round</span>(end_time <span class="op">-</span> start_time, <span class="dv">4</span>)</span>
<span id="cb81-24"><a href="#cb81-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-25"><a href="#cb81-25" aria-hidden="true" tabindex="-1"></a>    generated_text <span class="op">=</span> raw_output[<span class="dv">0</span>][<span class="st">"generated_text"</span>][<span class="dv">1</span>][<span class="st">"content"</span>]</span>
<span id="cb81-26"><a href="#cb81-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-27"><a href="#cb81-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> generated_text, raw_output, total_time</span>
<span id="cb81-28"><a href="#cb81-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-29"><a href="#cb81-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model (from our Hugging Face Repo)</span></span>
<span id="cb81-30"><a href="#cb81-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: You may have to replace my username `mrdbourke` for your own</span></span>
<span id="cb81-31"><a href="#cb81-31" aria-hidden="true" tabindex="-1"></a>MODEL_PATH <span class="op">=</span> <span class="st">"mrdbourke/FoodExtract-gemma-3-270m-fine-tune-v1"</span></span>
<span id="cb81-32"><a href="#cb81-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-33"><a href="#cb81-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model into a pipeline</span></span>
<span id="cb81-34"><a href="#cb81-34" aria-hidden="true" tabindex="-1"></a>loaded_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb81-35"><a href="#cb81-35" aria-hidden="true" tabindex="-1"></a>    pretrained_model_name_or_path<span class="op">=</span>MODEL_PATH,</span>
<span id="cb81-36"><a href="#cb81-36" aria-hidden="true" tabindex="-1"></a>    dtype<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb81-37"><a href="#cb81-37" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb81-38"><a href="#cb81-38" aria-hidden="true" tabindex="-1"></a>    attn_implementation<span class="op">=</span><span class="st">"eager"</span></span>
<span id="cb81-39"><a href="#cb81-39" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb81-40"><a href="#cb81-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-41"><a href="#cb81-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the tokenizer</span></span>
<span id="cb81-42"><a href="#cb81-42" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(</span>
<span id="cb81-43"><a href="#cb81-43" aria-hidden="true" tabindex="-1"></a>    MODEL_PATH</span>
<span id="cb81-44"><a href="#cb81-44" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb81-45"><a href="#cb81-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-46"><a href="#cb81-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Create model pipeline</span></span>
<span id="cb81-47"><a href="#cb81-47" aria-hidden="true" tabindex="-1"></a>loaded_model_pipeline <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>,</span>
<span id="cb81-48"><a href="#cb81-48" aria-hidden="true" tabindex="-1"></a>                                 model<span class="op">=</span>loaded_model,</span>
<span id="cb81-49"><a href="#cb81-49" aria-hidden="true" tabindex="-1"></a>                                 tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb81-50"><a href="#cb81-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-51"><a href="#cb81-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the demo</span></span>
<span id="cb81-52"><a href="#cb81-52" aria-hidden="true" tabindex="-1"></a>description <span class="op">=</span> <span class="st">"""Extract food and drink items from text with a fine-tuned SLM (Small Language Model) or more specifically a fine-tuned [Gemma 3 270M](https://huggingface.co/google/gemma-3-270m-it).</span></span>
<span id="cb81-53"><a href="#cb81-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-54"><a href="#cb81-54" aria-hidden="true" tabindex="-1"></a><span class="st">Our model has been fine-tuned on the [FoodExtract-1k dataset](https://huggingface.co/datasets/mrdbourke/FoodExtract-1k). </span></span>
<span id="cb81-55"><a href="#cb81-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-56"><a href="#cb81-56" aria-hidden="true" tabindex="-1"></a><span class="st">* Input (str): Raw text strings or image captions (e.g. "A photo of a dog sitting on a beach" or "A breakfast plate with bacon, eggs and toast")</span></span>
<span id="cb81-57"><a href="#cb81-57" aria-hidden="true" tabindex="-1"></a><span class="st">* Output (str): Generated text with food/not_food classification as well as noun extracted food and drink items and various food tags.</span></span>
<span id="cb81-58"><a href="#cb81-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-59"><a href="#cb81-59" aria-hidden="true" tabindex="-1"></a><span class="st">For example:</span></span>
<span id="cb81-60"><a href="#cb81-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-61"><a href="#cb81-61" aria-hidden="true" tabindex="-1"></a><span class="st">* Input: "For breakfast I had eggs, bacon and toast and a glass of orange juice"</span></span>
<span id="cb81-62"><a href="#cb81-62" aria-hidden="true" tabindex="-1"></a><span class="st">* Output: </span></span>
<span id="cb81-63"><a href="#cb81-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-64"><a href="#cb81-64" aria-hidden="true" tabindex="-1"></a><span class="st">```</span></span>
<span id="cb81-65"><a href="#cb81-65" aria-hidden="true" tabindex="-1"></a><span class="st">food_or_drink: 1</span></span>
<span id="cb81-66"><a href="#cb81-66" aria-hidden="true" tabindex="-1"></a><span class="st">tags: fi, di</span></span>
<span id="cb81-67"><a href="#cb81-67" aria-hidden="true" tabindex="-1"></a><span class="st">foods: eggs, bacon, toast</span></span>
<span id="cb81-68"><a href="#cb81-68" aria-hidden="true" tabindex="-1"></a><span class="st">drinks: orange juice</span></span>
<span id="cb81-69"><a href="#cb81-69" aria-hidden="true" tabindex="-1"></a><span class="st">```</span></span>
<span id="cb81-70"><a href="#cb81-70" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb81-71"><a href="#cb81-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-72"><a href="#cb81-72" aria-hidden="true" tabindex="-1"></a>demo <span class="op">=</span> gr.Interface(fn<span class="op">=</span>pred_on_text,</span>
<span id="cb81-73"><a href="#cb81-73" aria-hidden="true" tabindex="-1"></a>                    inputs<span class="op">=</span>gr.TextArea(lines<span class="op">=</span><span class="dv">4</span>, label<span class="op">=</span><span class="st">"Input Text"</span>),</span>
<span id="cb81-74"><a href="#cb81-74" aria-hidden="true" tabindex="-1"></a>                    outputs<span class="op">=</span>[gr.TextArea(lines<span class="op">=</span><span class="dv">4</span>, label<span class="op">=</span><span class="st">"Generated Text"</span>),</span>
<span id="cb81-75"><a href="#cb81-75" aria-hidden="true" tabindex="-1"></a>                             gr.TextArea(lines<span class="op">=</span><span class="dv">7</span>, label<span class="op">=</span><span class="st">"Raw Output"</span>),</span>
<span id="cb81-76"><a href="#cb81-76" aria-hidden="true" tabindex="-1"></a>                             gr.Number(label<span class="op">=</span><span class="st">"Generation Time (s)"</span>)],</span>
<span id="cb81-77"><a href="#cb81-77" aria-hidden="true" tabindex="-1"></a>                    title<span class="op">=</span><span class="st">"üç≥ Structured FoodExtract with a Fine-Tuned Gemma 3 270M"</span>,</span>
<span id="cb81-78"><a href="#cb81-78" aria-hidden="true" tabindex="-1"></a>                    description<span class="op">=</span>description,</span>
<span id="cb81-79"><a href="#cb81-79" aria-hidden="true" tabindex="-1"></a>                    examples<span class="op">=</span>[[<span class="st">"Hello world! This is my first fine-tuned LLM!"</span>],</span>
<span id="cb81-80"><a href="#cb81-80" aria-hidden="true" tabindex="-1"></a>                              [<span class="st">"A plate of food with grilled barramundi, salad with avocado, olives, tomatoes and Italian dressing"</span>],</span>
<span id="cb81-81"><a href="#cb81-81" aria-hidden="true" tabindex="-1"></a>                              [<span class="st">"British Breakfast with baked beans, fried eggs, black pudding, sausages, bacon, mushrooms, a cup of tea and toast and fried tomatoes"</span>],</span>
<span id="cb81-82"><a href="#cb81-82" aria-hidden="true" tabindex="-1"></a>                              [<span class="st">"Steak tacos"</span>],</span>
<span id="cb81-83"><a href="#cb81-83" aria-hidden="true" tabindex="-1"></a>                              [<span class="st">"A photo of a dog sitting on a beach"</span>]]</span>
<span id="cb81-84"><a href="#cb81-84" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb81-85"><a href="#cb81-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-86"><a href="#cb81-86" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb81-87"><a href="#cb81-87" aria-hidden="true" tabindex="-1"></a>    demo.launch(share<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Writing demos/FoodExtract/app.py</code></pre>
</div>
</div>
</section>
<section id="create-the-readme.md-file" class="level3" data-number="16.2">
<h3 data-number="16.2" class="anchored" data-anchor-id="create-the-readme.md-file"><span class="header-section-number">16.2</span> Create the <code>README.md</code> file</h3>
<p>The <code>README.md</code> file will tell people what our app does.</p>
<p>We could add more information here if we wanted to but for now we‚Äôll keep it simple.</p>
<p>Notice the special text at the top of the file below (the text between the <code>---</code>), these are some settings for the Space, you can see the <a href="https://huggingface.co/docs/hub/en/spaces-config-reference">settings for these in the docs</a>.</p>
<div id="33cb083d-8258-4a5d-a54e-52d22c6a5789" class="cell" data-execution_count="47">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile demos<span class="op">/</span>FoodExtract<span class="op">/</span>README.md</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="op">---</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>title: FoodExtract Fine<span class="op">-</span>tuned LLM Structued Data Extractor</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>emoji: üìù‚û°Ô∏èüçü</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>colorFrom: green</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>colorTo: blue</span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>sdk: gradio</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>app_file: app.py</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>pinned: false</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>license: apache<span class="op">-</span><span class="fl">2.0</span></span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a><span class="op">---</span></span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a><span class="co">Fine-tuned Gemma 3 270M to extract food and drink items from raw text.</span></span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a><span class="co">Input can be any form of real text and output will be a formatted string such as the following:</span></span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a><span class="co">```</span></span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a><span class="co">food_or_drink: 1</span></span>
<span id="cb83-20"><a href="#cb83-20" aria-hidden="true" tabindex="-1"></a><span class="co">tags: fi, re</span></span>
<span id="cb83-21"><a href="#cb83-21" aria-hidden="true" tabindex="-1"></a><span class="co">foods: tacos, milk, red apple, pineapple, cherries, fried chicken, steak, mayonnaise</span></span>
<span id="cb83-22"><a href="#cb83-22" aria-hidden="true" tabindex="-1"></a><span class="co">drinks: iced latte, matcha latte</span></span>
<span id="cb83-23"><a href="#cb83-23" aria-hidden="true" tabindex="-1"></a><span class="co">```</span></span>
<span id="cb83-24"><a href="#cb83-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-25"><a href="#cb83-25" aria-hidden="true" tabindex="-1"></a><span class="co">The tags map to the following items:</span></span>
<span id="cb83-26"><a href="#cb83-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-27"><a href="#cb83-27" aria-hidden="true" tabindex="-1"></a><span class="co">```</span></span>
<span id="cb83-28"><a href="#cb83-28" aria-hidden="true" tabindex="-1"></a><span class="co">tags_dict = {'np': 'nutrition_panel',</span></span>
<span id="cb83-29"><a href="#cb83-29" aria-hidden="true" tabindex="-1"></a><span class="co"> 'il': 'ingredient list',</span></span>
<span id="cb83-30"><a href="#cb83-30" aria-hidden="true" tabindex="-1"></a><span class="co"> 'me': 'menu',</span></span>
<span id="cb83-31"><a href="#cb83-31" aria-hidden="true" tabindex="-1"></a><span class="co"> 're': 'recipe',</span></span>
<span id="cb83-32"><a href="#cb83-32" aria-hidden="true" tabindex="-1"></a><span class="co"> 'fi': 'food_items',</span></span>
<span id="cb83-33"><a href="#cb83-33" aria-hidden="true" tabindex="-1"></a><span class="co"> 'di': 'drink_items',</span></span>
<span id="cb83-34"><a href="#cb83-34" aria-hidden="true" tabindex="-1"></a><span class="co"> 'fa': 'food_advertistment',</span></span>
<span id="cb83-35"><a href="#cb83-35" aria-hidden="true" tabindex="-1"></a><span class="co"> 'fp': 'food_packaging'}</span></span>
<span id="cb83-36"><a href="#cb83-36" aria-hidden="true" tabindex="-1"></a><span class="co">```</span></span>
<span id="cb83-37"><a href="#cb83-37" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Writing demos/FoodExtract/README.md</code></pre>
</div>
</div>
</section>
<section id="create-the-requirements.txt-file" class="level3" data-number="16.3">
<h3 data-number="16.3" class="anchored" data-anchor-id="create-the-requirements.txt-file"><span class="header-section-number">16.3</span> Create the <code>requirements.txt</code> file</h3>
<p>This will tell the Hugging Face Space what libraries we‚Äôd like it to run inside.</p>
<div id="d7e17c94-07bf-4c40-a045-f64f58256a0a" class="cell" data-execution_count="48">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile demos<span class="op">/</span>FoodExtract<span class="op">/</span>requirements.txt</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>transformers</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>gradio</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>torch</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>accelerate</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Writing demos/FoodExtract/requirements.txt</code></pre>
</div>
</div>
</section>
<section id="upload-our-demo-to-the-hugging-face-hub" class="level3" data-number="16.4">
<h3 data-number="16.4" class="anchored" data-anchor-id="upload-our-demo-to-the-hugging-face-hub"><span class="header-section-number">16.4</span> Upload our demo to the Hugging Face Hub</h3>
<p>We can upload our demo to the Hugging Face Hub in a similar way to uploading our model.</p>
<p>We could also upload it file by file via the Hugging Face Spaces interface.</p>
<p>But let‚Äôs stick to the code-first approach.</p>
<div id="a4988447-8a2a-40da-bffe-4636ab960b9c" class="cell" data-execution_count="49">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Import the required methods for uploading to the Hugging Face Hub</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> (</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>    create_repo,</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>    get_full_repo_name,</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    upload_file, <span class="co"># for uploading a single file (if necessary)</span></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    upload_folder <span class="co"># for uploading multiple files (in a folder)</span></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Define the parameters we'd like to use for the upload</span></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD <span class="op">=</span> <span class="st">"demos/FoodExtract/"</span></span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>HF_TARGET_SPACE_NAME <span class="op">=</span> <span class="st">"FoodExtract-v1"</span></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>HF_REPO_TYPE <span class="op">=</span> <span class="st">"space"</span> <span class="co"># we're creating a Hugging Face Space</span></span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>HF_SPACE_SDK <span class="op">=</span> <span class="st">"gradio"</span></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>HF_TOKEN <span class="op">=</span> <span class="st">""</span> <span class="co"># optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)</span></span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Create a Space repository on Hugging Face Hub </span></span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Creating repo on Hugging Face Hub with name: </span><span class="sc">{</span>HF_TARGET_SPACE_NAME<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>create_repo(</span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>    repo_id<span class="op">=</span>HF_TARGET_SPACE_NAME,</span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)</span></span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>    repo_type<span class="op">=</span>HF_REPO_TYPE,</span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a>    private<span class="op">=</span><span class="va">False</span>, <span class="co"># set to True if you don't want your Space to be accessible to others</span></span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a>    space_sdk<span class="op">=</span>HF_SPACE_SDK,</span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a>    exist_ok<span class="op">=</span><span class="va">True</span>, <span class="co"># set to False if you want an error to raise if the repo_id already exists </span></span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-27"><a href="#cb87-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})</span></span>
<span id="cb87-28"><a href="#cb87-28" aria-hidden="true" tabindex="-1"></a>full_hf_repo_name <span class="op">=</span> get_full_repo_name(model_id<span class="op">=</span>HF_TARGET_SPACE_NAME)</span>
<span id="cb87-29"><a href="#cb87-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Full Hugging Face Hub repo name: </span><span class="sc">{</span>full_hf_repo_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb87-30"><a href="#cb87-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-31"><a href="#cb87-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Upload our demo folder</span></span>
<span id="cb87-32"><a href="#cb87-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Uploading </span><span class="sc">{</span>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD<span class="sc">}</span><span class="ss"> to repo: </span><span class="sc">{</span>full_hf_repo_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb87-33"><a href="#cb87-33" aria-hidden="true" tabindex="-1"></a>folder_upload_url <span class="op">=</span> upload_folder(</span>
<span id="cb87-34"><a href="#cb87-34" aria-hidden="true" tabindex="-1"></a>    repo_id<span class="op">=</span>full_hf_repo_name,</span>
<span id="cb87-35"><a href="#cb87-35" aria-hidden="true" tabindex="-1"></a>    folder_path<span class="op">=</span>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,</span>
<span id="cb87-36"><a href="#cb87-36" aria-hidden="true" tabindex="-1"></a>    path_in_repo<span class="op">=</span><span class="st">"."</span>, <span class="co"># upload our folder to the root directory ("." means "base" or "root", this is the default)</span></span>
<span id="cb87-37"><a href="#cb87-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># token=HF_TOKEN, # optional: set token manually</span></span>
<span id="cb87-38"><a href="#cb87-38" aria-hidden="true" tabindex="-1"></a>    repo_type<span class="op">=</span>HF_REPO_TYPE,</span>
<span id="cb87-39"><a href="#cb87-39" aria-hidden="true" tabindex="-1"></a>    commit_message<span class="op">=</span><span class="st">"Uploading FoodExtract demo app.py"</span></span>
<span id="cb87-40"><a href="#cb87-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb87-41"><a href="#cb87-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Demo folder successfully uploaded with commit URL: </span><span class="sc">{</span>folder_upload_url<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Creating repo on Hugging Face Hub with name: FoodExtract-v1
[INFO] Full Hugging Face Hub repo name: mrdbourke/FoodExtract-v1
[INFO] Uploading demos/FoodExtract/ to repo: mrdbourke/FoodExtract-v1
[INFO] Demo folder successfully uploaded with commit URL: https://huggingface.co/spaces/mrdbourke/FoodExtract-v1/tree/main/.</code></pre>
</div>
</div>
<p>Nice!</p>
<p>It looks like our demo upload worked!</p>
<p>We can try it out via the URL (in my case, it‚Äôs: <a href="https://huggingface.co/spaces/mrdbourke/FoodExtract-v1">https://huggingface.co/spaces/mrdbourke/FoodExtract-v1</a>). And we can even embed it right in our notebook.</p>
<div id="5dc1a559-079a-49cd-9197-2b1b06f64c5b" class="cell" data-execution_count="50">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>html_code <span class="op">=</span> <span class="st">"""&lt;iframe</span></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a><span class="st">    src="https://mrdbourke-foodextract-v1.hf.space"</span></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a><span class="st">    frameborder="0"</span></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a><span class="st">    width="850"</span></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a><span class="st">    height="1000"</span></span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a><span class="st">&gt;&lt;/iframe&gt;"""</span></span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>display(HTML(html_code))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<iframe src="https://mrdbourke-foodextract-v1.hf.space" frameborder="0" width="850" height="1000"></iframe>
</div>
</div>
<p>How cool!</p>
<p>We‚Äôve now got a sharable demo of a fine-tuned LLM which anyone can try out for themselves.</p>
</section>
</section>
<section id="bonus-speeding-up-our-model-with-batched-inference" class="level2" data-number="17">
<h2 data-number="17" class="anchored" data-anchor-id="bonus-speeding-up-our-model-with-batched-inference"><span class="header-section-number">17</span> Bonus: Speeding up our model with batched inference</h2>
<p>Right now our model only inferences on one sample at a time but as is the case with many machine learning models, we could perform inference on multiple samples (also referred to as a batch) to significantly improve throughout.</p>
<p>In batched inference mode, your model performs predictions on X number of samples at once, this can dramatically improve sample throughput.</p>
<p>The number of samples you can predict on at once will depend on a few factors:</p>
<ul>
<li>The size of your model (e.g.&nbsp;if your model is quite large, it may only be able to predict on 1 sample at time)</li>
<li>The size of your compute VRAM (e.g.&nbsp;if your compute VRAM is already saturated, add multiple samples at a time may result in errors)</li>
<li>The size of your samples (if one of your samples is 100x the size of others, this may cause errors with batched inference)</li>
</ul>
<p>To find an optimal batch size for our setup, we can run an experiment:</p>
<ul>
<li>Loop through different batch sizes and measure the throughput for each batch size.
<ul>
<li>Why do we do this?
<ul>
<li>It‚Äôs hard to tell the ideal batch size ahead of time.</li>
<li>So we experiment from say 1, 2, 4, 8, 16, 32, 64 batch sizes and see which performs best.</li>
<li>Just because we may get a speed up from using batch size 8, doesn‚Äôt mean 64 will be better.</li>
</ul></li>
</ul></li>
</ul>
<div id="74535c69-d30a-4d95-aa0d-b477ac0d6f2c" class="cell" data-execution_count="51">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"mrdbourke/FoodExtract-1k"</span>)</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Number of samples in the dataset: </span><span class="sc">{</span><span class="bu">len</span>(dataset[<span class="st">'train'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_to_conversation(sample):</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"messages"</span>: [</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: sample[<span class="st">"sequence"</span>]}, <span class="co"># Load the sequence from the dataset</span></span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: sample[<span class="st">"gpt-oss-120b-label-condensed"</span>]} <span class="co"># Load the gpt-oss-120b generated label</span></span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Map our sample_to_conversation function to dataset </span></span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> dataset.<span class="bu">map</span>(sample_to_conversation,</span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a>                      batched<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-19"><a href="#cb90-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a train/test split</span></span>
<span id="cb90-20"><a href="#cb90-20" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> dataset[<span class="st">"train"</span>].train_test_split(test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb90-21"><a href="#cb90-21" aria-hidden="true" tabindex="-1"></a>                                            shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb90-22"><a href="#cb90-22" aria-hidden="true" tabindex="-1"></a>                                            seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb90-23"><a href="#cb90-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-24"><a href="#cb90-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Number #1 rule in machine learning</span></span>
<span id="cb90-25"><a href="#cb90-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Always train on the train set and test on the test set</span></span>
<span id="cb90-26"><a href="#cb90-26" aria-hidden="true" tabindex="-1"></a><span class="co"># This gives us an indication of how our model will perform in the real world</span></span>
<span id="cb90-27"><a href="#cb90-27" aria-hidden="true" tabindex="-1"></a>dataset</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Number of samples in the dataset: 1420</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['sequence', 'image_url', 'class_label', 'source', 'char_len', 'word_count', 'syn_or_real', 'uuid', 'gpt-oss-120b-label', 'gpt-oss-120b-label-condensed', 'target_food_names_to_use', 'caption_detail_level', 'num_foods', 'target_image_point_of_view', 'messages'],
        num_rows: 1136
    })
    test: Dataset({
        features: ['sequence', 'image_url', 'class_label', 'source', 'char_len', 'word_count', 'syn_or_real', 'uuid', 'gpt-oss-120b-label', 'gpt-oss-120b-label-condensed', 'target_food_names_to_use', 'caption_detail_level', 'num_foods', 'target_image_point_of_view', 'messages'],
        num_rows: 284
    })
})</code></pre>
</div>
</div>
<div id="33695408-65b3-4708-a008-30acd385f46c" class="cell" data-execution_count="52">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Need to turn our samples into batches (e.g. lists of samples)</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Formatting test samples into list prompts..."</span>)</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>test_input_prompts <span class="op">=</span> [</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>    loaded_model_pipeline.tokenizer.apply_chat_template(</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>        item[<span class="st">"messages"</span>][:<span class="dv">1</span>],</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>        tokenize<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>        add_generation_prompt<span class="op">=</span><span class="va">True</span></span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> dataset[<span class="st">"test"</span>]</span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Number of test sample prompts: </span><span class="sc">{</span><span class="bu">len</span>(test_input_prompts)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a>test_input_prompts[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Formatting test samples into list prompts...
[INFO] Number of test sample prompts: 284</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>'&lt;bos&gt;&lt;start_of_turn&gt;user\nLiving Planet Goat Milk Whole Milk, 1 Litre, GMO Free, Australian Dairy, 8.75g Protein Per Serve, Good Source of Calcium.&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model\n'</code></pre>
</div>
</div>
<div id="63b3e66c-a5ba-4056-bd9a-97cc9aab7f74" class="cell" data-execution_count="53">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Need to perform batched inference and time each step</span></span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>all_outputs <span class="op">=</span> []</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's write a list of batch sizes to test</span></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>chunk_sizes_to_test <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">128</span>]</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a>timing_dict <span class="op">=</span> {}</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop through each batch size and time the inference</span></span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> CHUNK_SIZE <span class="kw">in</span> chunk_sizes_to_test:</span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"[INFO] Making predictions with batch size: </span><span class="sc">{</span>CHUNK_SIZE<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> chunk_number <span class="kw">in</span> tqdm(<span class="bu">range</span>(<span class="bu">round</span>(<span class="bu">len</span>(test_input_prompts) <span class="op">/</span> CHUNK_SIZE))):</span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a>        batched_inputs <span class="op">=</span> test_input_prompts[(CHUNK_SIZE <span class="op">*</span> chunk_number): CHUNK_SIZE <span class="op">*</span> (chunk_number <span class="op">+</span> <span class="dv">1</span>)]</span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a>        batched_outputs <span class="op">=</span> loaded_model_pipeline(text_inputs<span class="op">=</span>batched_inputs,</span>
<span id="cb96-19"><a href="#cb96-19" aria-hidden="true" tabindex="-1"></a>                                                batch_size<span class="op">=</span>CHUNK_SIZE,</span>
<span id="cb96-20"><a href="#cb96-20" aria-hidden="true" tabindex="-1"></a>                                                max_new_tokens<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb96-21"><a href="#cb96-21" aria-hidden="true" tabindex="-1"></a>                                                disable_compile<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb96-22"><a href="#cb96-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb96-23"><a href="#cb96-23" aria-hidden="true" tabindex="-1"></a>        all_outputs <span class="op">+=</span> batched_outputs</span>
<span id="cb96-24"><a href="#cb96-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-25"><a href="#cb96-25" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> time.time()</span>
<span id="cb96-26"><a href="#cb96-26" aria-hidden="true" tabindex="-1"></a>    total_time <span class="op">=</span> end_time <span class="op">-</span> start_time</span>
<span id="cb96-27"><a href="#cb96-27" aria-hidden="true" tabindex="-1"></a>    timing_dict[CHUNK_SIZE] <span class="op">=</span> total_time</span>
<span id="cb96-28"><a href="#cb96-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb96-29"><a href="#cb96-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"[INFO] Total time for batch size </span><span class="sc">{</span>CHUNK_SIZE<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>total_time<span class="sc">:.2f}</span><span class="ss">s"</span>)</span>
<span id="cb96-30"><a href="#cb96-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">80</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Making predictions with batch size: 1</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"62b087fbd1eb464f89c0f8506bcd8e06","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[INFO] Total time for batch size 1: 144.40s
================================================================================


[INFO] Making predictions with batch size: 4</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9ee4cb8980e24ef6a30f822ef2b7458c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[INFO] Total time for batch size 4: 52.49s
================================================================================


[INFO] Making predictions with batch size: 8</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b9c442e564d04cd7a02a3966ddec2a9a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[INFO] Total time for batch size 8: 36.89s
================================================================================


[INFO] Making predictions with batch size: 16</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d17dec5389744578a9e7a73eb2e9e89b","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[INFO] Total time for batch size 16: 29.80s
================================================================================


[INFO] Making predictions with batch size: 32</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2521bfb949474120b3e932218882ba15","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[INFO] Total time for batch size 32: 33.65s
================================================================================


[INFO] Making predictions with batch size: 64</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0a9da3a479a341af85204f7d2fb15379","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[INFO] Total time for batch size 64: 31.30s
================================================================================


[INFO] Making predictions with batch size: 128</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"60f64f8a2efd4412a059cfb912e8c730","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
[INFO] Total time for batch size 128: 48.09s
================================================================================

</code></pre>
</div>
</div>
<p>Batched inference complete! Let‚Äôs make a plot comparing different batch sizes.</p>
<div id="b29098f5-13ff-47ee-b1bd-13ec50aa914f" class="cell" data-execution_count="54">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> timing_dict</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a>total_samples <span class="op">=</span> <span class="bu">len</span>(dataset[<span class="st">"test"</span>])</span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a>batch_sizes <span class="op">=</span> <span class="bu">list</span>(data.keys())</span>
<span id="cb106-9"><a href="#cb106-9" aria-hidden="true" tabindex="-1"></a>inference_times <span class="op">=</span> <span class="bu">list</span>(data.values())</span>
<span id="cb106-10"><a href="#cb106-10" aria-hidden="true" tabindex="-1"></a>samples_per_second <span class="op">=</span> [total_samples <span class="op">/</span> time <span class="cf">for</span> bs, time <span class="kw">in</span> data.items()]</span>
<span id="cb106-11"><a href="#cb106-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-12"><a href="#cb106-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create side-by-side plots</span></span>
<span id="cb106-13"><a href="#cb106-13" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb106-14"><a href="#cb106-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-15"><a href="#cb106-15" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Left plot: Total Inference Time ---</span></span>
<span id="cb106-16"><a href="#cb106-16" aria-hidden="true" tabindex="-1"></a>ax1.bar([<span class="bu">str</span>(bs) <span class="cf">for</span> bs <span class="kw">in</span> batch_sizes], inference_times, color<span class="op">=</span><span class="st">'steelblue'</span>)</span>
<span id="cb106-17"><a href="#cb106-17" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Batch Size'</span>)</span>
<span id="cb106-18"><a href="#cb106-18" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Total Inference Time (s)'</span>)</span>
<span id="cb106-19"><a href="#cb106-19" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Inference Time by Batch Size'</span>)</span>
<span id="cb106-20"><a href="#cb106-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-21"><a href="#cb106-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, v <span class="kw">in</span> <span class="bu">enumerate</span>(inference_times):</span>
<span id="cb106-22"><a href="#cb106-22" aria-hidden="true" tabindex="-1"></a>    ax1.text(i, v <span class="op">+</span> <span class="dv">1</span>, <span class="ss">f'</span><span class="sc">{</span>v<span class="sc">:.1f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb106-23"><a href="#cb106-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-24"><a href="#cb106-24" aria-hidden="true" tabindex="-1"></a><span class="co"># --- ARROW LOGIC (Left) ---</span></span>
<span id="cb106-25"><a href="#cb106-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Identify Start (Slowest) and End (Fastest)</span></span>
<span id="cb106-26"><a href="#cb106-26" aria-hidden="true" tabindex="-1"></a>start_val <span class="op">=</span> <span class="bu">max</span>(inference_times)</span>
<span id="cb106-27"><a href="#cb106-27" aria-hidden="true" tabindex="-1"></a>end_val <span class="op">=</span> <span class="bu">min</span>(inference_times)</span>
<span id="cb106-28"><a href="#cb106-28" aria-hidden="true" tabindex="-1"></a>start_idx <span class="op">=</span> inference_times.index(start_val)</span>
<span id="cb106-29"><a href="#cb106-29" aria-hidden="true" tabindex="-1"></a>end_idx <span class="op">=</span> inference_times.index(end_val)</span>
<span id="cb106-30"><a href="#cb106-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-31"><a href="#cb106-31" aria-hidden="true" tabindex="-1"></a>speedup <span class="op">=</span> start_val <span class="op">/</span> end_val</span>
<span id="cb106-32"><a href="#cb106-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-33"><a href="#cb106-33" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Draw Arrow (No Text)</span></span>
<span id="cb106-34"><a href="#cb106-34" aria-hidden="true" tabindex="-1"></a><span class="co"># connectionstyle "rad=-0.3" arcs the arrow upwards</span></span>
<span id="cb106-35"><a href="#cb106-35" aria-hidden="true" tabindex="-1"></a>ax1.annotate(<span class="st">""</span>,</span>
<span id="cb106-36"><a href="#cb106-36" aria-hidden="true" tabindex="-1"></a>             xy<span class="op">=</span>(end_idx, end_val<span class="op">+</span>(<span class="fl">0.5</span><span class="op">*</span>end_val)),</span>
<span id="cb106-37"><a href="#cb106-37" aria-hidden="true" tabindex="-1"></a>             xytext<span class="op">=</span>(start_idx<span class="op">+</span><span class="fl">0.25</span>, start_val<span class="op">+</span><span class="dv">10</span>),</span>
<span id="cb106-38"><a href="#cb106-38" aria-hidden="true" tabindex="-1"></a>             arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">"-&gt;"</span>, color<span class="op">=</span><span class="st">'green'</span>, lw<span class="op">=</span><span class="fl">1.5</span>, connectionstyle<span class="op">=</span><span class="st">"arc3,rad=-0.3"</span>))</span>
<span id="cb106-39"><a href="#cb106-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-40"><a href="#cb106-40" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Place Text at Midpoint</span></span>
<span id="cb106-41"><a href="#cb106-41" aria-hidden="true" tabindex="-1"></a>mid_x <span class="op">=</span> (start_idx <span class="op">+</span> end_idx) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb106-42"><a href="#cb106-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Place text slightly above the highest point of the two bars</span></span>
<span id="cb106-43"><a href="#cb106-43" aria-hidden="true" tabindex="-1"></a>text_y <span class="op">=</span> <span class="bu">max</span>(start_val, end_val) <span class="op">+</span> (<span class="bu">max</span>(inference_times) <span class="op">*</span> <span class="fl">0.1</span>)</span>
<span id="cb106-44"><a href="#cb106-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-45"><a href="#cb106-45" aria-hidden="true" tabindex="-1"></a>ax1.text(mid_x<span class="op">+</span><span class="fl">0.5</span>, text_y<span class="op">-</span><span class="dv">150</span>, <span class="ss">f"</span><span class="sc">{</span>speedup<span class="sc">:.1f}</span><span class="ss">x speedup"</span>,</span>
<span id="cb106-46"><a href="#cb106-46" aria-hidden="true" tabindex="-1"></a>         ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb106-47"><a href="#cb106-47" aria-hidden="true" tabindex="-1"></a>         bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">"round,pad=0.3"</span>, fc<span class="op">=</span><span class="st">"white"</span>, ec<span class="op">=</span><span class="st">"none"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>))</span>
<span id="cb106-48"><a href="#cb106-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-49"><a href="#cb106-49" aria-hidden="true" tabindex="-1"></a>ax1.set_ylim(<span class="dv">0</span>, <span class="bu">max</span>(inference_times) <span class="op">*</span> <span class="fl">1.35</span>) <span class="co"># Increase headroom for text</span></span>
<span id="cb106-50"><a href="#cb106-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-51"><a href="#cb106-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-52"><a href="#cb106-52" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Right plot: Samples per Second ---</span></span>
<span id="cb106-53"><a href="#cb106-53" aria-hidden="true" tabindex="-1"></a>ax2.bar([<span class="bu">str</span>(bs) <span class="cf">for</span> bs <span class="kw">in</span> batch_sizes], samples_per_second, color<span class="op">=</span><span class="st">'coral'</span>)</span>
<span id="cb106-54"><a href="#cb106-54" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Batch Size'</span>)</span>
<span id="cb106-55"><a href="#cb106-55" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Samples per Second'</span>)</span>
<span id="cb106-56"><a href="#cb106-56" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Throughput by Batch Size'</span>)</span>
<span id="cb106-57"><a href="#cb106-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-58"><a href="#cb106-58" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, v <span class="kw">in</span> <span class="bu">enumerate</span>(samples_per_second):</span>
<span id="cb106-59"><a href="#cb106-59" aria-hidden="true" tabindex="-1"></a>    ax2.text(i, v <span class="op">+</span> <span class="fl">0.05</span>, <span class="ss">f'</span><span class="sc">{</span>v<span class="sc">:.2f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb106-60"><a href="#cb106-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-61"><a href="#cb106-61" aria-hidden="true" tabindex="-1"></a><span class="co"># --- ARROW LOGIC (Right) ---</span></span>
<span id="cb106-62"><a href="#cb106-62" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Identify Start (Slowest) and End (Fastest)</span></span>
<span id="cb106-63"><a href="#cb106-63" aria-hidden="true" tabindex="-1"></a>start_val_t <span class="op">=</span> <span class="bu">min</span>(samples_per_second)</span>
<span id="cb106-64"><a href="#cb106-64" aria-hidden="true" tabindex="-1"></a>end_val_t <span class="op">=</span> <span class="bu">max</span>(samples_per_second)</span>
<span id="cb106-65"><a href="#cb106-65" aria-hidden="true" tabindex="-1"></a>start_idx_t <span class="op">=</span> samples_per_second.index(start_val_t)</span>
<span id="cb106-66"><a href="#cb106-66" aria-hidden="true" tabindex="-1"></a>end_idx_t <span class="op">=</span> samples_per_second.index(end_val_t)</span>
<span id="cb106-67"><a href="#cb106-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-68"><a href="#cb106-68" aria-hidden="true" tabindex="-1"></a>speedup_t <span class="op">=</span> end_val_t <span class="op">/</span> start_val_t</span>
<span id="cb106-69"><a href="#cb106-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-70"><a href="#cb106-70" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Draw Arrow (No Text)</span></span>
<span id="cb106-71"><a href="#cb106-71" aria-hidden="true" tabindex="-1"></a>ax2.annotate(<span class="st">""</span>,</span>
<span id="cb106-72"><a href="#cb106-72" aria-hidden="true" tabindex="-1"></a>             xy<span class="op">=</span>(end_idx_t<span class="op">-</span>(<span class="fl">0.05</span><span class="op">*</span>end_idx_t), end_val_t<span class="op">+</span>(<span class="fl">0.025</span><span class="op">*</span>end_val_t)),</span>
<span id="cb106-73"><a href="#cb106-73" aria-hidden="true" tabindex="-1"></a>             xytext<span class="op">=</span>(start_idx_t, start_val_t<span class="op">+</span><span class="fl">0.6</span>),</span>
<span id="cb106-74"><a href="#cb106-74" aria-hidden="true" tabindex="-1"></a>             arrowprops<span class="op">=</span><span class="bu">dict</span>(arrowstyle<span class="op">=</span><span class="st">"-&gt;"</span>, color<span class="op">=</span><span class="st">'green'</span>, lw<span class="op">=</span><span class="fl">1.5</span>, connectionstyle<span class="op">=</span><span class="st">"arc3,rad=-0.3"</span>))</span>
<span id="cb106-75"><a href="#cb106-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-76"><a href="#cb106-76" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Place Text at Midpoint</span></span>
<span id="cb106-77"><a href="#cb106-77" aria-hidden="true" tabindex="-1"></a>mid_x_t <span class="op">=</span> (start_idx_t <span class="op">+</span> end_idx_t) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb106-78"><a href="#cb106-78" aria-hidden="true" tabindex="-1"></a>text_y_t <span class="op">=</span> <span class="bu">max</span>(start_val_t, end_val_t) <span class="op">+</span> (<span class="bu">max</span>(samples_per_second) <span class="op">*</span> <span class="fl">0.1</span>)</span>
<span id="cb106-79"><a href="#cb106-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-80"><a href="#cb106-80" aria-hidden="true" tabindex="-1"></a>ax2.text(mid_x_t<span class="op">-</span><span class="fl">0.5</span>, text_y_t<span class="op">-</span><span class="fl">4.5</span>, <span class="ss">f"</span><span class="sc">{</span>speedup_t<span class="sc">:.1f}</span><span class="ss">x speedup"</span>,</span>
<span id="cb106-81"><a href="#cb106-81" aria-hidden="true" tabindex="-1"></a>         ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb106-82"><a href="#cb106-82" aria-hidden="true" tabindex="-1"></a>         bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">"round,pad=0.3"</span>, fc<span class="op">=</span><span class="st">"white"</span>, ec<span class="op">=</span><span class="st">"none"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>))</span>
<span id="cb106-83"><a href="#cb106-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-84"><a href="#cb106-84" aria-hidden="true" tabindex="-1"></a>ax2.set_ylim(<span class="dv">0</span>, <span class="bu">max</span>(samples_per_second) <span class="op">*</span> <span class="fl">1.35</span>) <span class="co"># Increase headroom</span></span>
<span id="cb106-85"><a href="#cb106-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-86"><a href="#cb106-86" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">"Inference with Fine-Tuned Gemma 3 270M on NVIDIA DGX Spark"</span>)</span>
<span id="cb106-87"><a href="#cb106-87" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb106-88"><a href="#cb106-88" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'inference_benchmark.png'</span>, dpi<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb106-89"><a href="#cb106-89" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_llm_full_fine_tune_tutorial_files/figure-html/cell-49-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We get a 4-6x speedup when using batches!</p>
<p>At 10 samples per second, that means we can inference on ~800k samples in a day.</p>
<div id="7a6e07e9" class="cell" data-execution_count="62">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>samples_per_second <span class="op">=</span> <span class="bu">round</span>(<span class="bu">len</span>(dataset[<span class="st">"test"</span>]) <span class="op">/</span> <span class="bu">min</span>(timing_dict.values()), <span class="dv">2</span>)</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>seconds_in_a_day <span class="op">=</span> <span class="dv">86_400</span></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>samples_per_day <span class="op">=</span> seconds_in_a_day <span class="op">*</span> samples_per_second</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Number of samples per second: </span><span class="sc">{</span>samples_per_second<span class="sc">}</span><span class="ss"> | Number of samples per day: </span><span class="sc">{</span>samples_per_day<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Number of samples per second: 9.53 | Number of samples per day: 823392.0</code></pre>
</div>
</div>
</section>
<section id="bonus-performing-evaluations-on-our-model" class="level2" data-number="18">
<h2 data-number="18" class="anchored" data-anchor-id="bonus-performing-evaluations-on-our-model"><span class="header-section-number">18</span> Bonus: Performing evaluations on our model</h2>
<p>We can evaluate our model directly against the original labels from <code>gpt-oss-120b</code> and see how it stacks up.</p>
<p>For example, we could compare the following:</p>
<ul>
<li>F1 Score</li>
<li>Precision</li>
<li>Recall</li>
<li>Pure accuracy</li>
</ul>
<p>Have these metrics as well as samples which are different to the ground truth would allow us to further explore where our model needs improvements.</p>
</section>
<section id="next-steps" class="level2" data-number="19">
<h2 data-number="19" class="anchored" data-anchor-id="next-steps"><span class="header-section-number">19</span> Next steps</h2>
<p>There are several avenues we could approach next.</p>
<ol type="1">
<li><p>Improve the data sampling - if our model makes mistakes, could we improve the input data? For example, more samples on certain tasks where the model is weaker.</p></li>
<li><p>Quantisation - right now our model is ~500MB, could we make this smaller with the help of INT8 quantisation? (this could also help speed up inference time)</p></li>
<li><p>Output compression - our output is compressed to a simple YAML-like format, but is there a better compression option to generate even less tokens?</p></li>
<li><p>Test the model on a large unseen sample - practice using the model on a large corpus of image captions (e.g.&nbsp;10,000+) and randomly inspect them to find failure cases to boost the model performance</p></li>
</ol>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/www\.learnhuggingface\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mrdbourke/learn-huggingface/issues" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.mrdbourke.com">
      <i class="bi bi-globe" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mrdbourke/learn-huggingface">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/channel/UCr8O8l5cCX85Oem1d18EezQ">
      <i class="bi bi-youtube" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/mrdbourke">
      <i class="bi bi-twitter-x" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/mrdbourke/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>