<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Learn how to create a custom object detection model with Hugging Face Transformers.">

<title>[Work in Progress] Object Detection with Hugging Face Transformers Tutorial â€“ Learn Hugging Face ðŸ¤—</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/learn-hf-favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-1092c56c6cadf2eb47b1bc8063ab382a.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-da507f57f029983fc305e97b0a7fb62f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="[Work in Progress] Object Detection with Hugging Face Transformers Tutorial â€“ Learn Hugging Face ðŸ¤—">
<meta property="og:description" content="Learn how to create a custom object detection model with Hugging Face Transformers.">
<meta property="og:image" content="https://www.learnhuggingface.com/notebooks/hugging_face_object_detection_tutorial_files/figure-html/cell-11-output-1.png">
<meta property="og:site_name" content="Learn Hugging Face ðŸ¤—">
<meta property="og:image:height" content="89">
<meta property="og:image:width" content="790">
<meta name="twitter:title" content="[Work in Progress] Object Detection with Hugging Face Transformers Tutorial â€“ Learn Hugging Face ðŸ¤—">
<meta name="twitter:description" content="Learn how to create a custom object detection model with Hugging Face Transformers.">
<meta name="twitter:image" content="https://www.learnhuggingface.com/notebooks/hugging_face_object_detection_tutorial_files/figure-html/cell-11-output-1.png">
<meta name="twitter:image-height" content="89">
<meta name="twitter:image-width" content="790">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Learn Hugging Face ðŸ¤—</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../extras/setup.html"> 
<span class="menu-text">Setup</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../extras/glossary.html"> 
<span class="menu-text">Glossary</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com/mrdbourke/learn-huggingface" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notebooks/hugging_face_object_detection_tutorial.html">Computer Vision</a></li><li class="breadcrumb-item"><a href="../notebooks/hugging_face_object_detection_tutorial.html">(Work in progress) Build a custom object detection model and demo</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Natural Language Processing (NLP)</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/hugging_face_text_classification_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Build a custom text classification model and demo</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Computer Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/hugging_face_object_detection_tutorial.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">(Work in progress) Build a custom object detection model and demo</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#tk---overview" id="toc-tk---overview" class="nav-link active" data-scroll-target="#tk---overview"><span class="header-section-number">1</span> TK - Overview</a>
  <ul class="collapse">
  <li><a href="#tk---what-were-going-to-build" id="toc-tk---what-were-going-to-build" class="nav-link" data-scroll-target="#tk---what-were-going-to-build"><span class="header-section-number">1.1</span> TK - What weâ€™re going to build</a></li>
  <li><a href="#tk---what-is-object-detection" id="toc-tk---what-is-object-detection" class="nav-link" data-scroll-target="#tk---what-is-object-detection"><span class="header-section-number">1.2</span> TK - What is object detection?</a></li>
  <li><a href="#tk---why-train-your-own-object-detection-models" id="toc-tk---why-train-your-own-object-detection-models" class="nav-link" data-scroll-target="#tk---why-train-your-own-object-detection-models"><span class="header-section-number">1.3</span> TK - Why train your own object detection models?</a></li>
  <li><a href="#tk---workflow-were-going-to-follow" id="toc-tk---workflow-were-going-to-follow" class="nav-link" data-scroll-target="#tk---workflow-were-going-to-follow"><span class="header-section-number">1.4</span> TK - Workflow weâ€™re going to follow</a></li>
  </ul></li>
  <li><a href="#tk---importing-necessary-libraries" id="toc-tk---importing-necessary-libraries" class="nav-link" data-scroll-target="#tk---importing-necessary-libraries"><span class="header-section-number">2</span> TK - Importing necessary libraries</a></li>
  <li><a href="#getting-a-dataset" id="toc-getting-a-dataset" class="nav-link" data-scroll-target="#getting-a-dataset"><span class="header-section-number">3</span> Getting a dataset</a>
  <ul class="collapse">
  <li><a href="#loading-the-dataset" id="toc-loading-the-dataset" class="nav-link" data-scroll-target="#loading-the-dataset"><span class="header-section-number">3.1</span> Loading the dataset</a></li>
  <li><a href="#viewing-a-single-sample-from-our-data" id="toc-viewing-a-single-sample-from-our-data" class="nav-link" data-scroll-target="#viewing-a-single-sample-from-our-data"><span class="header-section-number">3.2</span> Viewing a single sample from our data</a></li>
  <li><a href="#extracting-the-category-names-from-our-data" id="toc-extracting-the-category-names-from-our-data" class="nav-link" data-scroll-target="#extracting-the-category-names-from-our-data"><span class="header-section-number">3.3</span> Extracting the category names from our data</a></li>
  <li><a href="#creating-a-mapping-from-numbers-to-labels" id="toc-creating-a-mapping-from-numbers-to-labels" class="nav-link" data-scroll-target="#creating-a-mapping-from-numbers-to-labels"><span class="header-section-number">3.4</span> Creating a mapping from numbers to labels</a></li>
  <li><a href="#creating-a-colour-palette" id="toc-creating-a-colour-palette" class="nav-link" data-scroll-target="#creating-a-colour-palette"><span class="header-section-number">3.5</span> Creating a colour palette</a></li>
  </ul></li>
  <li><a href="#tk---plotting-a-single-image-and-visualizing-the-boxes" id="toc-tk---plotting-a-single-image-and-visualizing-the-boxes" class="nav-link" data-scroll-target="#tk---plotting-a-single-image-and-visualizing-the-boxes"><span class="header-section-number">4</span> TK - Plotting a single image and visualizing the boxes</a></li>
  <li><a href="#different-bounding-box-formats" id="toc-different-bounding-box-formats" class="nav-link" data-scroll-target="#different-bounding-box-formats"><span class="header-section-number">5</span> Different bounding box formats</a>
  <ul class="collapse">
  <li><a href="#absolute-or-normalized-format" id="toc-absolute-or-normalized-format" class="nav-link" data-scroll-target="#absolute-or-normalized-format"><span class="header-section-number">5.1</span> Absolute or normalized format?</a></li>
  <li><a href="#which-bounding-box-format-should-you-use" id="toc-which-bounding-box-format-should-you-use" class="nav-link" data-scroll-target="#which-bounding-box-format-should-you-use"><span class="header-section-number">5.2</span> Which bounding box format should you use?</a></li>
  </ul></li>
  <li><a href="#getting-an-object-detection-model" id="toc-getting-an-object-detection-model" class="nav-link" data-scroll-target="#getting-an-object-detection-model"><span class="header-section-number">6</span> Getting an object detection model</a>
  <ul class="collapse">
  <li><a href="#places-to-get-object-detection-models" id="toc-places-to-get-object-detection-models" class="nav-link" data-scroll-target="#places-to-get-object-detection-models"><span class="header-section-number">6.1</span> Places to get object detection models</a></li>
  <li><a href="#downloading-our-model-from-hugging-face" id="toc-downloading-our-model-from-hugging-face" class="nav-link" data-scroll-target="#downloading-our-model-from-hugging-face"><span class="header-section-number">6.2</span> Downloading our model from Hugging Face</a></li>
  <li><a href="#loading-our-models-processor" id="toc-loading-our-models-processor" class="nav-link" data-scroll-target="#loading-our-models-processor"><span class="header-section-number">6.3</span> Loading our modelâ€™s processor</a></li>
  <li><a href="#discussing-how-to-convert-our-annotations-into-coco-format" id="toc-discussing-how-to-convert-our-annotations-into-coco-format" class="nav-link" data-scroll-target="#discussing-how-to-convert-our-annotations-into-coco-format"><span class="header-section-number">6.4</span> Discussing how to convert our annotations into COCO format</a></li>
  <li><a href="#tk---creating-dataclasses-to-represent-the-coco-bounding-box-format" id="toc-tk---creating-dataclasses-to-represent-the-coco-bounding-box-format" class="nav-link" data-scroll-target="#tk---creating-dataclasses-to-represent-the-coco-bounding-box-format"><span class="header-section-number">6.5</span> TK - Creating dataclasses to represent the COCO bounding box format</a></li>
  <li><a href="#creating-a-function-to-format-our-annotations-as-coco-format" id="toc-creating-a-function-to-format-our-annotations-as-coco-format" class="nav-link" data-scroll-target="#creating-a-function-to-format-our-annotations-as-coco-format"><span class="header-section-number">6.6</span> Creating a function to format our annotations as COCO format</a></li>
  <li><a href="#tk---preprocess-a-single-image-and-set-of-coco-format-annotations" id="toc-tk---preprocess-a-single-image-and-set-of-coco-format-annotations" class="nav-link" data-scroll-target="#tk---preprocess-a-single-image-and-set-of-coco-format-annotations"><span class="header-section-number">6.7</span> TK - Preprocess a single image and set of COCO format annotations</a></li>
  <li><a href="#tk---creating-a-function-to-build-our-model" id="toc-tk---creating-a-function-to-build-our-model" class="nav-link" data-scroll-target="#tk---creating-a-function-to-build-our-model"><span class="header-section-number">6.8</span> TK - Creating a function to build our model</a></li>
  <li><a href="#post-process-a-single-output" id="toc-post-process-a-single-output" class="nav-link" data-scroll-target="#post-process-a-single-output"><span class="header-section-number">6.9</span> Post process a single output</a></li>
  </ul></li>
  <li><a href="#tk---fine-tune-the-model-to-our-dataset" id="toc-tk---fine-tune-the-model-to-our-dataset" class="nav-link" data-scroll-target="#tk---fine-tune-the-model-to-our-dataset"><span class="header-section-number">7</span> TK - Fine-tune the model to our dataset</a>
  <ul class="collapse">
  <li><a href="#tk---preprocess-dataset-for-model" id="toc-tk---preprocess-dataset-for-model" class="nav-link" data-scroll-target="#tk---preprocess-dataset-for-model"><span class="header-section-number">7.1</span> TK - Preprocess dataset for model</a></li>
  <li><a href="#tk---split-the-data" id="toc-tk---split-the-data" class="nav-link" data-scroll-target="#tk---split-the-data"><span class="header-section-number">7.2</span> TK - Split the data</a></li>
  <li><a href="#tk---create-a-collation-function" id="toc-tk---create-a-collation-function" class="nav-link" data-scroll-target="#tk---create-a-collation-function"><span class="header-section-number">7.3</span> TK - Create a collation function</a></li>
  </ul></li>
  <li><a href="#tk---setup-trainingarguments-trainer" id="toc-tk---setup-trainingarguments-trainer" class="nav-link" data-scroll-target="#tk---setup-trainingarguments-trainer"><span class="header-section-number">8</span> TK - Setup TrainingArguments + Trainer</a></li>
  <li><a href="#tk---make-predictions-on-the-test-dataset" id="toc-tk---make-predictions-on-the-test-dataset" class="nav-link" data-scroll-target="#tk---make-predictions-on-the-test-dataset"><span class="header-section-number">9</span> TK - Make predictions on the test dataset</a>
  <ul class="collapse">
  <li><a href="#tk---predict-on-image-from-filepath" id="toc-tk---predict-on-image-from-filepath" class="nav-link" data-scroll-target="#tk---predict-on-image-from-filepath"><span class="header-section-number">9.1</span> TK - Predict on image from filepath</a></li>
  </ul></li>
  <li><a href="#tk---upload-our-trained-model-to-hugging-face-hub" id="toc-tk---upload-our-trained-model-to-hugging-face-hub" class="nav-link" data-scroll-target="#tk---upload-our-trained-model-to-hugging-face-hub"><span class="header-section-number">10</span> TK - Upload our trained model to Hugging Face Hub</a></li>
  <li><a href="#creating-a-demo-of-our-model-with-gradio" id="toc-creating-a-demo-of-our-model-with-gradio" class="nav-link" data-scroll-target="#creating-a-demo-of-our-model-with-gradio"><span class="header-section-number">11</span> Creating a demo of our model with Gradio</a>
  <ul class="collapse">
  <li><a href="#tk---upload-demo-to-hugging-face-spaces-to-get-it-live" id="toc-tk---upload-demo-to-hugging-face-spaces-to-get-it-live" class="nav-link" data-scroll-target="#tk---upload-demo-to-hugging-face-spaces-to-get-it-live"><span class="header-section-number">11.1</span> TK - Upload demo to Hugging Face Spaces to get it live</a></li>
  <li><a href="#tk---testing-the-hosted-demo" id="toc-tk---testing-the-hosted-demo" class="nav-link" data-scroll-target="#tk---testing-the-hosted-demo"><span class="header-section-number">11.2</span> TK - Testing the hosted demo</a></li>
  </ul></li>
  <li><a href="#tk---improve-our-model-with-data-augmentation" id="toc-tk---improve-our-model-with-data-augmentation" class="nav-link" data-scroll-target="#tk---improve-our-model-with-data-augmentation"><span class="header-section-number">12</span> TK - Improve our model with data augmentation</a>
  <ul class="collapse">
  <li><a href="#load-dataset" id="toc-load-dataset" class="nav-link" data-scroll-target="#load-dataset"><span class="header-section-number">12.1</span> Load dataset</a></li>
  <li><a href="#setup-model" id="toc-setup-model" class="nav-link" data-scroll-target="#setup-model"><span class="header-section-number">12.2</span> Setup model</a></li>
  <li><a href="#tk---setup-and-visualize-transforms-augmentations" id="toc-tk---setup-and-visualize-transforms-augmentations" class="nav-link" data-scroll-target="#tk---setup-and-visualize-transforms-augmentations"><span class="header-section-number">12.3</span> tk - Setup and visualize transforms (augmentations)</a></li>
  <li><a href="#tk---visualize-transforms" id="toc-tk---visualize-transforms" class="nav-link" data-scroll-target="#tk---visualize-transforms"><span class="header-section-number">12.4</span> TK - Visualize transforms</a></li>
  <li><a href="#tk---create-function-to-preprocess-and-transform-batch-of-examples" id="toc-tk---create-function-to-preprocess-and-transform-batch-of-examples" class="nav-link" data-scroll-target="#tk---create-function-to-preprocess-and-transform-batch-of-examples"><span class="header-section-number">12.5</span> TK - Create function to preprocess and transform batch of examples</a></li>
  <li><a href="#tk---save-the-trained-model" id="toc-tk---save-the-trained-model" class="nav-link" data-scroll-target="#tk---save-the-trained-model"><span class="header-section-number">12.6</span> TK - Save the trained model</a></li>
  </ul></li>
  <li><a href="#tk---upload-augmentation-model-to-hugging-face-hub" id="toc-tk---upload-augmentation-model-to-hugging-face-hub" class="nav-link" data-scroll-target="#tk---upload-augmentation-model-to-hugging-face-hub"><span class="header-section-number">13</span> TK - Upload Augmentation Model to Hugging Face Hub</a></li>
  <li><a href="#tk---compare-results-of-different-models" id="toc-tk---compare-results-of-different-models" class="nav-link" data-scroll-target="#tk---compare-results-of-different-models"><span class="header-section-number">14</span> TK - Compare results of different models</a></li>
  <li><a href="#tk---create-demo-with-augmentation-model" id="toc-tk---create-demo-with-augmentation-model" class="nav-link" data-scroll-target="#tk---create-demo-with-augmentation-model"><span class="header-section-number">15</span> TK - Create demo with Augmentation Model</a>
  <ul class="collapse">
  <li><a href="#tk---make-a-prediction-on-a-random-test-sample-with-model-using-data-aug-model" id="toc-tk---make-a-prediction-on-a-random-test-sample-with-model-using-data-aug-model" class="nav-link" data-scroll-target="#tk---make-a-prediction-on-a-random-test-sample-with-model-using-data-aug-model"><span class="header-section-number">15.1</span> TK - Make a prediction on a random test sample with model using data aug model</a></li>
  </ul></li>
  <li><a href="#tk---model-v3---cleaning-up-predictions-with-nms-non-max-suppression" id="toc-tk---model-v3---cleaning-up-predictions-with-nms-non-max-suppression" class="nav-link" data-scroll-target="#tk---model-v3---cleaning-up-predictions-with-nms-non-max-suppression"><span class="header-section-number">16</span> TK - Model V3 - Cleaning up predictions with NMS (Non-max Suppression)</a>
  <ul class="collapse">
  <li><a href="#tk---nms-filtering-logic-to-do" id="toc-tk---nms-filtering-logic-to-do" class="nav-link" data-scroll-target="#tk---nms-filtering-logic-to-do"><span class="header-section-number">16.1</span> TK - NMS filtering logic to do</a></li>
  <li><a href="#tk---simple-nms---keep-only-highest-scoring-class-per-prediction" id="toc-tk---simple-nms---keep-only-highest-scoring-class-per-prediction" class="nav-link" data-scroll-target="#tk---simple-nms---keep-only-highest-scoring-class-per-prediction"><span class="header-section-number">16.2</span> TK - Simple NMS - Keep only highest scoring class per prediction</a></li>
  <li><a href="#tk---greedy-iou-filtering---intersection-over-union---if-a-pair-of-boxes-have-an-iou-over-a-certain-threshold-keep-the-box-with-the-higher-score" id="toc-tk---greedy-iou-filtering---intersection-over-union---if-a-pair-of-boxes-have-an-iou-over-a-certain-threshold-keep-the-box-with-the-higher-score" class="nav-link" data-scroll-target="#tk---greedy-iou-filtering---intersection-over-union---if-a-pair-of-boxes-have-an-iou-over-a-certain-threshold-keep-the-box-with-the-higher-score"><span class="header-section-number">16.3</span> TK - Greedy IoU Filtering - Intersection over Union - If a pair of boxes have an IoU over a certain threshold, keep the box with the higher score</a></li>
  </ul></li>
  <li><a href="#tk---create-a-demo-with-simple-nms-filtering-only-keep-the-highest-scoring-boxes-per-image" id="toc-tk---create-a-demo-with-simple-nms-filtering-only-keep-the-highest-scoring-boxes-per-image" class="nav-link" data-scroll-target="#tk---create-a-demo-with-simple-nms-filtering-only-keep-the-highest-scoring-boxes-per-image"><span class="header-section-number">17</span> TK - Create a Demo with Simple NMS Filtering (only keep the highest scoring boxes per image)</a>
  <ul class="collapse">
  <li><a href="#tk---upload-our-demo-to-the-hugging-face-hub" id="toc-tk---upload-our-demo-to-the-hugging-face-hub" class="nav-link" data-scroll-target="#tk---upload-our-demo-to-the-hugging-face-hub"><span class="header-section-number">17.1</span> TK - Upload our demo to the Hugging Face Hub</a></li>
  <li><a href="#tk---embed-the-space-to-test-the-model" id="toc-tk---embed-the-space-to-test-the-model" class="nav-link" data-scroll-target="#tk---embed-the-space-to-test-the-model"><span class="header-section-number">17.2</span> tK - Embed the Space to Test the Model</a></li>
  </ul></li>
  <li><a href="#extensions-extra-curriculum" id="toc-extensions-extra-curriculum" class="nav-link" data-scroll-target="#extensions-extra-curriculum"><span class="header-section-number">18</span> Extensions + Extra-Curriculum</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">19</span> Summary</a></li>
  <li><a href="#extra-resources" id="toc-extra-resources" class="nav-link" data-scroll-target="#extra-resources"><span class="header-section-number">20</span> Extra resources</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mrdbourke/learn-huggingface/issues" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notebooks/hugging_face_object_detection_tutorial.html">Computer Vision</a></li><li class="breadcrumb-item"><a href="../notebooks/hugging_face_object_detection_tutorial.html">(Work in progress) Build a custom object detection model and demo</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">[Work in Progress] Object Detection with Hugging Face Transformers Tutorial</h1>
</div>

<div>
  <div class="description">
    Learn how to create a custom object detection model with Hugging Face Transformers.
  </div>
</div>


<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<p>Details:</p>
<ul>
<li><strong>Goal:</strong> build several custom object detection models + deploy them as demos</li>
<li><strong>Status:</strong> Work in progress (code works but the annotations are in need of work)</li>
<li>See example finished project demo: <a href="https://huggingface.co/spaces/mrdbourke/trashify_demo_v3">https://huggingface.co/spaces/mrdbourke/trashify_demo_v3</a></li>
</ul>
<p>In progress:</p>
<ul>
<li>Going through and adding headings/annotations to each different section
<ul>
<li>Start with overview + introduction + getting started, then go from there (e.g.&nbsp;intro the project and show where weâ€™re going to end up) âœ…</li>
</ul></li>
</ul>
<p>Later:</p>
<ul>
<li>Record video version of the materials</li>
</ul>
<p><a target="_blank" href="https://colab.research.google.com/github/mrdbourke/learn-huggingface/blob/main/notebooks/hugging_face_object_detection_tutorial.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<p><a href="https://github.com/mrdbourke/learn-huggingface/blob/main/notebooks/hugging_face_object_detection_tutorial.ipynb">Source code on GitHub</a> | <a href="https://www.learnhuggingface.com/notebooks/hugging_face_object_detection_tutorial">Online book version</a> | <a href="https://www.learnhuggingface.com/extras/setup">Setup guide</a> | Video Course (coming soon)</p>
<section id="tk---overview" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="tk---overview"><span class="header-section-number">1</span> TK - Overview</h2>
<p>TK - Make an intro about being on the Trashify ðŸš® team with a mission to make the world a cleaner place, trashify = using ML to incentivize people to pick up trash in their local area</p>
<p>Welcome to the Learn Hugging Face Object Detection project!</p>
<p>Inside this project, weâ€™ll learn bits and pieces about the Hugging Face ecosystem as well as how to build our own custom object detection model.</p>
<p>Weâ€™ll start with a collection of images with bounding box files as our dataset, fine-tune an existing computer vision model to detect items in an image and then share our model as a demo others can use.</p>
TK image - update cover image for object detection <!-- <figure style="text-align: center;">
    <!-- figtemplate --> <img src="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-text-classification/00-project-food-not-food-overview.png" alt="
Project overview image for 'Food Not Food' classification at Nutrify, a food app. The project involves building and deploying a binary text classification model to identify food-related text using Hugging Face Datasets, Transformers, and deploying with Hugging Face Hub/Spaces and Gradio. Examples include labels for 'A photo of sushi rolls on a white plate' (food), 'A serving of chicken curry in a blue bowl' (food), and 'A yellow tractor driving over a grassy hill' (not food). The process is visually depicted from data collection to model training and demo deployment." style="width: 100%; max-width: 900px; height: auto;">
<figcaption>
Weâ€™re going to put on our internship hats and build a food not food text classification model using tools from the Hugging Face ecosystem.
</figcaption>

<p>â€“&gt;</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Feel to keep reading through the notebook but if youâ€™d like to run the code yourself, be sure to go through the <a href="https://www.learnhuggingface.com/extras/setup">setup guide</a> first.</p>
</div>
</div>
<section id="tk---what-were-going-to-build" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="tk---what-were-going-to-build"><span class="header-section-number">1.1</span> TK - What weâ€™re going to build</h3>
<p>Weâ€™re going to be bulding Trashify ðŸš®, an <strong>object detection model</strong> which incentivises people to pick up trash in their local area by detecting <code>bin</code>, <code>trash</code>, <code>hand</code>.</p>
<p>If all three items are detected, a person gets +1 point!</p>
<p>For example, say you were going for a walk around your neighbourhood and took a photo of yourself picking up a piece (with your <strong>hand</strong> or <strong>trash arm</strong>) of <strong>trash</strong> and putting it in the <strong>bin</strong>, you would get a point.</p>
<p>With this object detection model, you could deploy it to an application which would automatically detect the target classes and then save the result to an online leaderboard.</p>
<p>The incentive would be to score the most points, in turn, picking up the most piecces of trash, in a given area.</p>
<p>More specifically, weâ€™re going to follow the following steps:</p>
<ol type="1">
<li><strong><a href="https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images">Data</a>: Problem defintion and dataset preparation</strong> - Getting a dataset/setting up the problem space.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/en/model_doc/conditional_detr">Model</a>: Finding, training and evaluating a model</strong> - Finding an object detection model suitable for our problem on Hugging Face and customizing it to our own dataset.</li>
<li><strong><a href="https://huggingface.co/spaces/mrdbourke/trashify_demo_v3">Demo</a>: Creating a demo and put our model into the real world</strong> - Sharing our trained model in a way others can access and use.</li>
</ol>
<p>By the end of this project, youâ€™ll have a trained model and <a href="https://huggingface.co/spaces/mrdbourke/trashify_demo_v3">demo on Hugging Face</a> you can share with others:</p>
<div id="cell-5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>HTML(<span class="st">"""</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;iframe</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="st">    src="https://mrdbourke-trashify-demo-v3.hf.space"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="st">    frameborder="0"</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="st">    width="850"</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="st">    height="850"</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="st">&gt;&lt;/iframe&gt;</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">

<iframe src="https://mrdbourke-trashify-demo-v3.hf.space" frameborder="0" width="850" height="850"></iframe>
</div>
</div>
</section>
<section id="tk---what-is-object-detection" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="tk---what-is-object-detection"><span class="header-section-number">1.2</span> TK - What is object detection?</h3>
<p>Object detection is the process of identifying and locating an item in an image.</p>
<p>Where <em>item</em> can mean almost anything.</p>
<p>For example:</p>
<ul>
<li>Detecting car <strong>licence plates</strong> in a video feed (videos are a series of images) for a parking lot entrance.</li>
<li>Detecting <strong>delivery people</strong> walking towards your front door on a security camera.</li>
<li>Detecting <strong>defects</strong> on a manufacturing line.</li>
<li>Detecting <a href="https://ieeexplore.ieee.org/abstract/document/9968423"><strong>pot holes</strong> in the road</a> so repair works can automatically be scheduled.</li>
<li>Detecting <strong>small pests (Varroa Mite)</strong> on the bodies of bees.</li>
<li>Detecting <a href="https://ai.meta.com/blog/pytorch-drives-next-gen-intelligent-farming-machines/"><strong>weeds</strong> in a field</a> so you know what to remove and what to keep.</li>
</ul>
<p>â€“</p>
<p>TK - add examples of actual trash identification projects, see:</p>
<ul>
<li>Google using machine learning for trash identification â€” <a href="https://sustainability.google/operating-sustainably/stories/circular-economy-marketplace/">https://sustainability.google/operating-sustainably/stories/circular-economy-marketplace/</a></li>
<li>Trashify website for identifying trash â€” <a href="https://www.trashify.tech/">https://www.trashify.tech/</a></li>
<li>Waste management with deep learning â€” <a href="https://www.sciencedirect.com/science/article/abs/pii/S0956053X23001915">https://www.sciencedirect.com/science/article/abs/pii/S0956053X23001915</a></li>
<li>Label Studio being used for labelling a trash dataset â€” <a href="https://labelstud.io/blog/ameru-labeling-for-a-greener-world/">https://labelstud.io/blog/ameru-labeling-for-a-greener-world/</a></li>
</ul>
<p>â€“</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Note:</strong> Object detection is also sometimes referred to as <em>image localization</em> or <em>object localization</em>. For consistency, I will use the term object detection, however, either of these terms could substitute.</p>
</div>
</div>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td>* TK image - examples of where object detection is used</td>
</tr>
</tbody>
</table>
<p><strong>Image classification</strong> deals with classifying an image as a whole into a single <code>class</code>, object detection endeavours to find the specific target item and <em>where</em> it is in an image.</p>
<p>One of the most common ways of showing where an item is in an image is by displaying a <strong>bounding box</strong> (a rectangle-like box around the target item).</p>
<p>An object detection model will often take an input image tensor in the shape <code>[3, 640, 640]</code> (<code>[colour_channels, height, width]</code>) and output a tensor in the form <code>[class_name, x_min, y_min, x_max, y_max]</code> or <code>[class_name, x1, y1, x2, y2]</code> (this is two ways to write the same example format, there are more formats, weâ€™ll see these below in <a href="#tbl-bbox-formats" class="quarto-xref">Table&nbsp;1</a>).</p>
<p>Where:</p>
<ul>
<li><code>class_name</code> = The classification of the target item (e.g.&nbsp;<code>"car"</code>, <code>"person"</code>, <code>"banana"</code>, <code>"piece_of_trash"</code>, this could be almost anything).</li>
<li><code>x_min</code> = The <code>x</code> value of the top left corner of the box.</li>
<li><code>y_min</code> = The <code>y</code> value of the top left corner of the box.</li>
<li><code>x_max</code> = The <code>x</code> value of the bottom right corner of the box.</li>
<li><code>y_max</code> = The <code>y</code> value of the bottom right corner of the box.</li>
</ul>
<p>â€“ TK image â€“ example of a bounding box on an image</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Object detection bounding box formats
</div>
</div>
<div class="callout-body-container callout-body">
<p>When you get into the world of object detection, you will find that there are several different bounding box formats.</p>
<p>There are three major formats you should be familiar with: <code>XYXY</code>, <code>XYWH</code>, <code>CXCYWH</code> (there are more but these are the most common).</p>
<p>Knowing which bounding box format youâ€™re working with can be the difference between a good model and a <em>very</em> poor model (wrong bounding boxes = wrong outcome).</p>
<p>Weâ€™ll get hands-on with a couple of these in this project.</p>
<p>But for an in-depth example of all three, I created a <a href="https://www.learnml.io/posts/a-guide-to-bounding-box-formats/">guide on different bounding box formats and how to draw them</a>, reading this should give a good intuition behind each style of bounding box.</p>
</div>
</div>
</section>
<section id="tk---why-train-your-own-object-detection-models" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="tk---why-train-your-own-object-detection-models"><span class="header-section-number">1.3</span> TK - Why train your own object detection models?</h3>
<p>You can customize <strong>pre-trained models</strong> for object detection as well as API-powered models and LLMs such as <a href="https://ai.google.dev/gemini-api/docs/vision?lang=python#bbox">Gemini</a>, <a href="https://landing.ai/agentic-object-detection">LandingAI</a> and <a href="https://github.com/IDEA-Research/DINO-X-API">DINO-X</a>.</p>
<p>Depending on your requirements, there are several pros and cons for using your own model versus using an API.</p>
<p>Training/fine-tuning your own model:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Pros</th>
<th style="text-align: left;">Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Control:</strong> Full control over model lifecycle.</td>
<td style="text-align: left;">Can be complex to get setup.</td>
</tr>
<tr class="even">
<td style="text-align: left;">No usage limits (aside from compute constraints).</td>
<td style="text-align: left;">Requires dedicated compute resources for training/inference.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Can train once and deploy everywhere/whenever you want (for example, Tesla deploying a model to all self-driving cars).</td>
<td style="text-align: left;">Requires maintenance over time to ensure performance remains up to par.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Privacy:</strong> Data can be kept in-house/app and doesnâ€™t need to go to a third party.</td>
<td style="text-align: left;">Can require longer development cycles compared to using existing APIs.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Speed:</strong> Customizing a small model for a specific use case often means it runs much faster on local hardware, for example, modern object detection models can achieve 70-100+ FPS (frames per second) on modern GPU hardware.</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Using a pre-built model API:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Pros</th>
<th style="text-align: left;">Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Ease of use:</strong> often can be setup within a few lines of code.</td>
<td style="text-align: left;">If the model API goes down, your service goes down.</td>
</tr>
<tr class="even">
<td style="text-align: left;">No maintenance of compute resources.</td>
<td style="text-align: left;">Data is required to be sent to a third-party for processing.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Access to the most advanced models.</td>
<td style="text-align: left;">The API may have usage limits per day/time period.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Can scale if usage increases.</td>
<td style="text-align: left;">Can be much slower than using dedicated models due to requiring an API call.</td>
</tr>
</tbody>
</table>
<p>For this project, weâ€™re going to focus on fine-tuning our own model.</p>
</section>
<section id="tk---workflow-were-going-to-follow" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="tk---workflow-were-going-to-follow"><span class="header-section-number">1.4</span> TK - Workflow weâ€™re going to follow</h3>
<p>The good news for us is that the Hugging Face ecosystem makes working on custom machine learning projects an absolute blast.</p>
<p>And workflow is reproducible across several kinds of projects.</p>
<p>Start with data (or skip this step and go straight to a model) -&gt; get/customize a model -&gt; build and share a demo.</p>
<p>With this in mind, our motto is <em>data, model, demo!</em></p>
<p>More specifically, weâ€™re going to follow the rough workflow of:</p>
<ol type="1">
<li>Create, preprocess and load data using <a href="https://huggingface.co/docs/datasets/index">Hugging Face Datasets</a>.</li>
<li>Define the model weâ€™d like use with <a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection"><code>transformers.AutoModelForObjectDetection</code></a> (or another similar model class).</li>
<li>Define training arguments (these are hyperparameters for our model) with <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments"><code>transformers.TrainingArguments</code></a>.</li>
<li>Pass <code>TrainingArguments</code> from 3 and target datasets to an instance of <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer"><code>transformers.Trainer</code></a>.</li>
<li>Train the model by calling <a href="https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.Trainer.train"><code>Trainer.train()</code></a>.</li>
<li>Save the model (to our local machine or to the Hugging Face Hub).</li>
<li>Evaluate the trained model by making and inspecting predctions on the test data.</li>
<li>Turn the model into a shareable demo.</li>
</ol>
<p>I say rough because machine learning projects are often non-linear in nature.</p>
<p>As in, because machine learning projects involve many experiments, they can kind of be all over the place.</p>
<p>But this worfklow will give us some good guidelines to follow.</p>
<figure style="text-align: center; display: inline-block;" class="figure">
<!-- figtemplate -->
<img src="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-text-classification/01-hugging-face-workflow.png" alt="The diagram shows the Hugging Face model development workflow, which includes the following steps: start with an idea or problem, get data ready (turn into tensors/create data splits), pick a pretrained model (to suit your problem), train/fine-tune the model on your custom data, evaluate the model, improve through experimentation, save and upload the fine-tuned model to the Hugging Face Hub, and turn your model into a shareable demo. Tools used in this workflow are Datasets/Tokenizers, Transformers/PEFT/Accelerate/timm, Hub/Spaces/Gradio, and Evaluate." style="width: 100%; max-width: 900px; height: auto;" class="figure-img">
<figcaption style="width: 100%; box-sizing: border-box;">
A general Hugging Face workflow from idea to shared model and demo using tools from the Hugging Face ecosystem. Youâ€™ll notice some of the steps donâ€™t match with our workflow outline above. This is because the text-based workflow outline above breaks some of the steps down for educational purposes. These kind of workflows are not set in stone and are more of guide than specific directions. See information on each of the tools in the <a href="https://huggingface.co">Hugging Face documentation</a>.
</figcaption>
</figure>
</section>
</section>
<section id="tk---importing-necessary-libraries" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="tk---importing-necessary-libraries"><span class="header-section-number">2</span> TK - Importing necessary libraries</h2>
<p>Letâ€™s get started!</p>
<p>First, weâ€™ll import the required libraries.</p>
<p>If youâ€™re running on your local computer, be sure to check out the getting <a href="https://www.learnhuggingface.com/extras/setup">setup guide</a> to make sure you have everything you need.</p>
<p>If youâ€™re using Google Colab, many of them the following libraries will be installed by default.</p>
<p>However, weâ€™ll have to install a few extras to get everything working.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If youâ€™re running on Google Colab, this notebook will work best with access to a GPU. To enable a GPU, go to <code>Runtime</code> âž¡ï¸ <code>Change runtime type</code> âž¡ï¸ <code>Hardware accelerator</code> âž¡ï¸ <code>GPU</code>.</p>
</div>
</div>
<p>Weâ€™ll need to install the following libraries from the Hugging Face ecosystem:</p>
<ul>
<li><a href="https://huggingface.co/docs/transformers/en/installation"><code>transformers</code></a> - comes pre-installed on Google Colab but if youâ€™re running on your local machine, you can install it via <code>pip install transformers</code>.</li>
<li><a href="https://huggingface.co/docs/datasets/installation"><code>datasets</code></a> - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via <code>pip install datasets</code>.</li>
<li><a href="https://huggingface.co/docs/evaluate/installation"><code>evaluate</code></a> - a library for evaluating machine learning model performance with various metrics, you can install it via <code>pip install evaluate</code>.</li>
<li><a href="https://huggingface.co/docs/accelerate/basic_tutorials/install"><code>accelerate</code></a> - a library for training machine learning models faster, you can install it via <code>pip install accelerate</code>.</li>
<li><a href="https://www.gradio.app/guides/quickstart#installation"><code>gradio</code></a> - a library for creating interactive demos of machine learning models, you can install it via <code>pip install gradio</code>.</li>
</ul>
<p>And the following library is not part of the Hugging Face ecosystem but it is helpful for evaluating our models:</p>
<ul>
<li><a href="https://lightning.ai/docs/torchmetrics/stable/"><code>torchmetrics</code></a> - a library containing many evaluation metrics compatible with PyTorch/Transformers, you can install it via <code>pip install torchmetrics</code>.</li>
</ul>
<p>We can also check the versions of our software with <code>package_name.__version__</code>.</p>
<div id="cell-10" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install/import dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="im">import</span> datasets, evaluate, accelerate</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">ModuleNotFoundError</span>:</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="op">!</span>pip install <span class="op">-</span>U datasets evaluate accelerate gradio <span class="co"># -U stands for "upgrade" so we'll get the latest version by default</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="im">import</span> datasets, evaluate, accelerate</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Required for evaluation</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Can install with !pip install torchmetrics[detection]</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchmetrics</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pycocotools</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Check versions (as long as you've got the following versions or higher, you should be good)</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using transformers version: </span><span class="sc">{</span>transformers<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using datasets version: </span><span class="sc">{</span>datasets<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using torch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using torchmetrics version: </span><span class="sc">{</span>torchmetrics<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using transformers version: 4.48.3
Using datasets version: 3.1.0
Using torch version: 2.6.0+cu124
Using torchmetrics version: 1.4.1</code></pre>
</div>
</div>
<p>Wonderful, as long as your versions are the same or higher to the versions above, you should be able to run the code below.</p>
</section>
<section id="getting-a-dataset" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="getting-a-dataset"><span class="header-section-number">3</span> Getting a dataset</h2>
<p>Okay, now weâ€™re got the required libraries, letâ€™s get a dataset.</p>
<p>Getting a dataset is one of the most important things a machine learning project.</p>
<p>The dataset you often determines the type of model you use as well as the quality of the outputs of that model.</p>
<p>Meaning, if you have a high quality dataset, chances are, your future model could also have high quality outputs.</p>
<p>It also means if your dataset is of poor quality, your model will likely also have poor quality outputs.</p>
<p>For an object detection problem, your dataset will likely come in the form of a group of images as well as a file with annotations belonging to those images.</p>
<p>For example, you might have the following setup:</p>
<pre><code>folder_of_images/
    image_1.jpeg
    image_2.jpeg
    image_3.jpeg
annotations.json</code></pre>
<p>Where the <code>annotations.json</code> contains details about the contains of each image:</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>annotations.json</strong></pre>
</div>
<div class="sourceCode" id="cb5" data-filename="annotations.json"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ot">[</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">{</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="er">'image_path'</span><span class="fu">:</span> <span class="er">'image_</span><span class="dv">1</span><span class="er">.jpeg'</span><span class="fu">,</span> </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="er">'image_id'</span><span class="fu">:</span> <span class="dv">42</span><span class="fu">,</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="er">'annotations'</span><span class="fu">:</span> </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>            <span class="fu">{</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>                <span class="er">'file_name'</span><span class="fu">:</span> <span class="ot">[</span><span class="er">'image_</span><span class="dv">1</span><span class="er">.jpeg'</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>                <span class="er">'image_id'</span><span class="fu">:</span> <span class="ot">[</span><span class="dv">42</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>                <span class="er">'category_id'</span><span class="fu">:</span> <span class="ot">[</span><span class="dv">1</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>                <span class="er">'bbox'</span><span class="fu">:</span> <span class="ot">[</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>                            <span class="ot">[</span><span class="fl">360.20001220703125</span><span class="ot">,</span> <span class="fl">528.5</span><span class="ot">,</span> <span class="fl">177.1999969482422</span><span class="ot">,</span> <span class="fl">261.79998779296875</span><span class="ot">],</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>                        <span class="ot">]</span><span class="fu">,</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>                <span class="er">'area'</span><span class="fu">:</span> <span class="ot">[</span><span class="fl">46390.9609375</span><span class="ot">]</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            <span class="fu">},</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="er">'label_source'</span><span class="fu">:</span> <span class="er">'manual_prodigy_label'</span><span class="fu">,</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="er">'image_source'</span><span class="fu">:</span> <span class="er">'manual_taken_photo'</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">}</span><span class="ot">,</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="er">...(more</span> <span class="er">labels</span> <span class="er">down</span> <span class="er">here)</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="ot">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Donâ€™t worry too much about the exact meaning of everything in the above <code>annotations.json</code> file for now (this is only one example, there are many different ways object detection information could be displayed).</p>
<p>The main point is that each target image is paired with an assosciated label.</p>
<p>Now like all good machine learning cooking shows, Iâ€™ve prepared a dataset from earlier.</p>
<p>TK image - dataset on Hugging Face</p>
<p>Itâ€™s stored on Hugging Face Datasets (also called the Hugging Face Hub) under the name <a href="https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images"><code>mrdbourke/trashify_manual_labelled_images</code></a>.</p>
<p>This is a dataset Iâ€™ve collected manually by hand (yes, by picking up 1000+ pieces of trash and photographing it) as well as labelled by hand (by drawing boxes on each image with a labelling tool called <a href="https://prodi.gy/features/computer-vision">Prodigy</a>).</p>
<section id="loading-the-dataset" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="loading-the-dataset"><span class="header-section-number">3.1</span> Loading the dataset</h3>
<p>To load a dataset stored on the Hugging Face Hub we can use the <a href="https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset"><code>datasets.load_dataset(path=NAME_OR_PATH_OF_DATASET)</code></a> function and pass it the name/path of the dataset we want to load.</p>
<p>In our case, our dataset name is <code>mrdbourke/trashify_manual_labelled_images</code> (you can also change this for your own dataset).</p>
<p>And since our dataset is hosted on Hugging Face, when we run the following code for the first time, it will download it.</p>
<p>If your target dataset is quite large, this download may take a while.</p>
<p>However, once the dataset is downloaded, subsequent reloads will be mush faster.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Getting information about a function/method">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Getting information about a function/method
</div>
</div>
<div class="callout-body-container callout-body">
<p>One way to find out what a function or method does is to lookup the documentation.</p>
<p>Another way is to write the function/method name with a question mark afterwards.</p>
<p>For example:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>load_dataset?</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Give it a try.</p>
<p>You should see some helpful information about what inputs the method takes and how they are used.</p>
</div>
</div>
<p>Letâ€™s load our dataset and check it out.</p>
<div id="cell-14" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load our Trashify dataset</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(path<span class="op">=</span><span class="st">"mrdbourke/trashify_manual_labelled_images"</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],
        num_rows: 1128
    })
})</code></pre>
</div>
</div>
<p>Beautiful!</p>
<p>We can see that there is a <code>train</code> split of the dataset already which currently contains all of the samples (<code>1128</code> in total).</p>
<p>There are also some <code>features</code> that come with our dataset which are related to our object detection goal.</p>
<div id="cell-16" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Length of original dataset: </span><span class="sc">{</span><span class="bu">len</span>(dataset[<span class="st">'train'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Dataset features:"</span>) </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pprint <span class="im">import</span> pprint</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>pprint(dataset[<span class="st">'train'</span>].features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Length of original dataset: 1128
[INFO] Dataset features:
{'annotations': Sequence(feature={'area': Value(dtype='float32', id=None),
                                  'bbox': Sequence(feature=Value(dtype='float32',
                                                                 id=None),
                                                   length=4,
                                                   id=None),
                                  'category_id': ClassLabel(names=['bin',
                                                                   'hand',
                                                                   'not_bin',
                                                                   'not_hand',
                                                                   'not_trash',
                                                                   'trash',
                                                                   'trash_arm'],
                                                            id=None),
                                  'file_name': Value(dtype='string', id=None),
                                  'image_id': Value(dtype='int64', id=None),
                                  'iscrowd': Value(dtype='int64', id=None)},
                         length=-1,
                         id=None),
 'image': Image(mode=None, decode=True, id=None),
 'image_id': Value(dtype='int64', id=None),
 'image_source': Value(dtype='string', id=None),
 'label_source': Value(dtype='string', id=None)}</code></pre>
</div>
</div>
<p>Nice!</p>
<p>We can see our dataset <code>features</code> contain the following fields:</p>
<ul>
<li><code>annotations</code> - A sequence of values including a <code>bbox</code> field (short for bounding box) as well as <code>category_id</code> field which contains the target objects weâ€™d like to identify in our images (<code>['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']</code>).</li>
<li><code>image</code> - This contains the target image assosciated with a given set of <code>annotations</code> (in our case, images and annotations have been uploaded to the Hugging Face Hub together).</li>
<li><code>image_id</code> - A unique ID assigned to a given sample.</li>
<li><code>image_source</code> - Where the image came from (all of our images have been manually collected).</li>
<li><code>label_source</code> - Where the image label came from (all of our images have been manually labelled).</li>
</ul>
</section>
<section id="viewing-a-single-sample-from-our-data" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="viewing-a-single-sample-from-our-data"><span class="header-section-number">3.2</span> Viewing a single sample from our data</h3>
<p>Now weâ€™ve seen the features, letâ€™s check out a single sample from our dataset.</p>
<p>We can index on a single sample of the <code>"train"</code> set just like indexing on a Python list.</p>
<div id="cell-19" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View a single sample of the dataset</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"train"</span>][<span class="dv">42</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,
 'image_id': 745,
 'annotations': {'file_name': ['094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg',
   '094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg',
   '094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg'],
  'image_id': [745, 745, 745],
  'category_id': [5, 1, 0],
  'bbox': [[333.1000061035156,
    611.2000122070312,
    244.89999389648438,
    321.29998779296875],
   [504.0, 612.9000244140625, 451.29998779296875, 650.7999877929688],
   [202.8000030517578,
    366.20001220703125,
    532.9000244140625,
    555.4000244140625]],
  'iscrowd': [0, 0, 0],
  'area': [78686.3671875, 293706.03125, 295972.65625]},
 'label_source': 'manual_prodigy_label',
 'image_source': 'manual_taken_photo'}</code></pre>
</div>
</div>
<p>We see a few more details here compared to just looking at the features.</p>
<p>We notice the <code>image</code> is a <a href="https://pillow.readthedocs.io/en/stable/reference/Image.html"><code>PIL.Image</code></a> with size <code>960x1280</code> (width x height).</p>
<p>And the <code>file_name</code> is a UUID (Universially Unique Identifier, made with <a href="https://docs.python.org/3/library/uuid.html#uuid.uuid4"><code>uuid.uuid4()</code></a>).</p>
<p>The <code>bbox</code> field in the <code>annotations</code> key contains a list of bounding boxes assosciated with the image.</p>
<p>In this case, there are 3 different bounding boxes.</p>
<p>With the <code>category_id</code> values of <code>5</code>, <code>1</code>, <code>0</code> (weâ€™ll map these to class names shortly).</p>
<p>Letâ€™s inspect a single bounding box.</p>
<div id="cell-21" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"train"</span>][<span class="dv">42</span>][<span class="st">"annotations"</span>][<span class="st">"bbox"</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>[333.1000061035156, 611.2000122070312, 244.89999389648438, 321.29998779296875]</code></pre>
</div>
</div>
<p>This array gives us the coordinates of a single bounding box in the format <code>XYWH</code>.</p>
<p>Where:</p>
<ul>
<li><code>X</code> is the x-coordinate of the top left corner of the box (<code>333.1</code>).</li>
<li><code>Y</code> is the y-coordinate of the top left corner of the box (<code>611.2</code>).</li>
<li><code>W</code> is the width of the box (<code>244.9</code>).</li>
<li><code>H</code> is the height of the box (<code>321.3</code>).</li>
</ul>
<p>All of these values are in absolute pixel values (meaning an x-coordinate of <code>333.1</code> is <code>333.1</code> pixels across on the x-axis).</p>
<p>How do I know this?</p>
<p>I know this because I created the box labels and this is the default value Prodigy (the labelling tool I used) outputs boxes.</p>
<p>However, if you were to come across another bouding box dataset, one of the first steps would be to <strong>figure out what format your bounding boxes are in</strong>.</p>
<p>Weâ€™ll see more on bounding box formats shortly.</p>
</section>
<section id="extracting-the-category-names-from-our-data" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="extracting-the-category-names-from-our-data"><span class="header-section-number">3.3</span> Extracting the category names from our data</h3>
<p>Before we start to visualize our sample image and bounding boxes, letâ€™s extract the category names from our dataset.</p>
<p>We can do so by accessing the <code>features</code> attribute our of <code>dataset</code> and then following it through to find the <code>category_id</code> feature, this contains a list of our text-based class names.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When working with different categories, itâ€™s good practice to get a list or mapping (e.g.&nbsp;a Python dictionary) from category name to ID and vice versa.</p>
<p>For example:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Category to ID</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>{<span class="st">"class_name"</span>: <span class="dv">0</span>}</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># ID to Category</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>{<span class="dv">0</span>: <span class="st">"class_name"</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Not all datasets will have this implemented in an easy to access way, so it might take a bit of research to get it created.</p>
</div>
</div>
<p>Letâ€™s access the class names in our dataset and save them to a variable <code>categories</code>.</p>
<div id="cell-24" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the categories from the dataset</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: This requires the dataset to have been uploaded with this information setup, not all datasets will have this available.</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>categories <span class="op">=</span> dataset[<span class="st">"train"</span>].features[<span class="st">"annotations"</span>].feature[<span class="st">"category_id"</span>]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the names attribute</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>categories.names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']</code></pre>
</div>
</div>
<p>Beautiful!</p>
<p>We get the following class names:</p>
<ul>
<li><code>bin</code> - A rubbish bin or trash can.</li>
<li><code>hand</code> - A personâ€™s hand.</li>
<li><code>not_bin</code> - Negative version of <code>bin</code> for items that look like a <code>bin</code> but shouldnâ€™t be identified as one.</li>
<li><code>not_hand</code> - Negative version of <code>hand</code> for items that look like a <code>hand</code> but shouldnâ€™t be identified as one.</li>
<li><code>not_trash</code> - Negative version of <code>trash</code> for items that look like <code>trash</code> but shouldnâ€™t be identified as it.</li>
<li><code>trash</code> - An item of trash you might find on a walk such as an old plastic bottle, food wrapper, cigarette butt or used coffee cup.</li>
<li><code>trash_arm</code> - A mechanical arm used for picking up trash.</li>
</ul>
<p>The goal of our computer vision model will be: given an image, detect items belonging to these target classes if they are present.</p>
</section>
<section id="creating-a-mapping-from-numbers-to-labels" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="creating-a-mapping-from-numbers-to-labels"><span class="header-section-number">3.4</span> Creating a mapping from numbers to labels</h3>
<p>Now weâ€™ve got our text-based class names, letâ€™s create a mapping from label to ID and ID to label.</p>
<p>For each of these, Hugging Face use the terminology <code>label2id</code> and <code>id2label</code> respectively.</p>
<div id="cell-27" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Map ID's to class names and vice versa</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>id2label <span class="op">=</span> {i: class_name <span class="cf">for</span> i, class_name <span class="kw">in</span> <span class="bu">enumerate</span>(categories.names)}</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>label2id <span class="op">=</span> {value: key <span class="cf">for</span> key, value <span class="kw">in</span> id2label.items()}</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Label to ID mapping:</span><span class="ch">\n</span><span class="sc">{</span>label2id<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ID to label mapping:</span><span class="ch">\n</span><span class="sc">{</span>id2label<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># id2label, label2id</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Label to ID mapping:
{'bin': 0, 'hand': 1, 'not_bin': 2, 'not_hand': 3, 'not_trash': 4, 'trash': 5, 'trash_arm': 6}

ID to label mapping:
{0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash', 6: 'trash_arm'}</code></pre>
</div>
</div>
</section>
<section id="creating-a-colour-palette" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="creating-a-colour-palette"><span class="header-section-number">3.5</span> Creating a colour palette</h3>
<p>Ok we know which class name matches to which ID, now letâ€™s create a dictionary of different colours we can use to display our bounding boxes.</p>
<p>Itâ€™s one thing to plot bounding boxes, itâ€™s another thing to make them look nice.</p>
<p>And we always want our plots looking nice!</p>
<p>Weâ€™ll colour the positive classes <code>bin</code>, <code>hand</code>, <code>trash</code>, <code>trash_arm</code> in nice bright colours.</p>
<p>And the negative classes <code>not_bin</code>, <code>not_hand</code>, <code>not_trash</code> in a light red colour to indicate theyâ€™re the negative versions.</p>
<p>Our colour dictionary will map <code>class_name</code> -&gt; <code>(red, green, blue)</code> (or <a href="https://en.wikipedia.org/wiki/RGB_color_model">RGB</a>) colour values.</p>
<div id="cell-29" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make colour dictionary</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>colour_palette <span class="op">=</span> {</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'bin'</span>: (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">224</span>),         <span class="co"># Bright Blue (High contrast with greenery) in format (red, green, blue)</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'not_bin'</span>: (<span class="dv">255</span>, <span class="dv">80</span>, <span class="dv">80</span>),   <span class="co"># Light Red to indicate negative class</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'hand'</span>: (<span class="dv">148</span>, <span class="dv">0</span>, <span class="dv">211</span>),      <span class="co"># Dark Purple (Contrasts well with skin tones)</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'not_hand'</span>: (<span class="dv">255</span>, <span class="dv">80</span>, <span class="dv">80</span>),  <span class="co"># Light Red to indicate negative class</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'trash'</span>: (<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">0</span>),       <span class="co"># Bright Green (For trash-related items)</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'not_trash'</span>: (<span class="dv">255</span>, <span class="dv">80</span>, <span class="dv">80</span>), <span class="co"># Light Red to indicate negative class</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'trash_arm'</span>: (<span class="dv">255</span>, <span class="dv">140</span>, <span class="dv">0</span>), <span class="co"># Deep Orange (Highly visible)</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Letâ€™s check out what these colours look like!</p>
<p>Itâ€™s the ABV motto: <em>Always Be Visualizing!</em></p>
<p>We can plot our colours with <code>matplotlib</code>.</p>
<p>Weâ€™ll just have to write a small function to normalize our colour values from <code>[0, 255]</code> to <code>[0, 1]</code> (<code>matplotlib</code> expects our colour values to be between 0 and 1).</p>
<div id="cell-31" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize RGB values to 0-1 range</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normalize_rgb(rgb_tuple):</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">tuple</span>(x<span class="op">/</span><span class="dv">255</span> <span class="cf">for</span> x <span class="kw">in</span> rgb_tuple)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Turn colors into normalized RGB values for matplotlib</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>colors_and_labels_rgb <span class="op">=</span> [(key, normalize_rgb(value)) <span class="cf">for</span> key, value <span class="kw">in</span> colour_palette.items()]</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure and axis</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">7</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten the axis array for easier iteration</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> ax.flatten()</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot each color square</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, (label, color) <span class="kw">in</span> <span class="bu">enumerate</span>(colors_and_labels_rgb):</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    ax[idx].add_patch(plt.Rectangle(xy<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>), </span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>                                    width<span class="op">=</span><span class="dv">1</span>, </span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>                                    height<span class="op">=</span><span class="dv">1</span>, </span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>                                    facecolor<span class="op">=</span>color))</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    ax[idx].set_title(label)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    ax[idx].set_xlim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    ax[idx].set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    ax[idx].axis(<span class="st">'off'</span>)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Sensational!</p>
<p>Now we know what colours to look out for when we visualize our bounding boxes.</p>
</section>
</section>
<section id="tk---plotting-a-single-image-and-visualizing-the-boxes" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="tk---plotting-a-single-image-and-visualizing-the-boxes"><span class="header-section-number">4</span> TK - Plotting a single image and visualizing the boxes</h2>
<p>Okay, okay, finally time to plot an image!</p>
<p>Letâ€™s take a random sample from our <code>dataset</code> and plot the image as well as the box on it.</p>
<p>To save some space in our notebook (plotting many images can increase the size of our notebook dramatically), weâ€™ll create two small helper functions:</p>
<ol type="1">
<li><code>half_image</code> - Halves the size of a given image.</li>
<li><code>half_boxes</code> - Divides the input coordinates of a given input box by 2.</li>
</ol>
<p>These functions arenâ€™t 100% necessary in our workflow.</p>
<p>Theyâ€™re just to make the images slightly smaller so they fit better in the notebook.</p>
<div id="cell-34" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> PIL</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> half_image(image: PIL.Image) <span class="op">-&gt;</span> PIL.Image:</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Resizes a given input image by half and returns the smaller version.</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image.resize(size<span class="op">=</span>(image.size[<span class="dv">0</span>] <span class="op">//</span> <span class="dv">2</span>, image.size[<span class="dv">1</span>] <span class="op">//</span> <span class="dv">2</span>))</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> half_boxes(boxes):</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Halves an array of input boxes and returns them. Necessary for plotting them on a half-sized image.</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="co">    For example:</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="co">    boxes = [100, 100, 100, 100]</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="co">    half_boxes = half_boxes(boxes)</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="co">    print(half_boxes)</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; [50, 50, 50, 50]</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(boxes) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the functions </span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>image_test <span class="op">=</span> dataset[<span class="st">"train"</span>][<span class="dv">42</span>][<span class="st">"image"</span>]</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>image_test_half <span class="op">=</span> half_image(image_test)</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Original image size: </span><span class="sc">{</span>image_test<span class="sc">.</span>size<span class="sc">}</span><span class="ss"> | Half image size: </span><span class="sc">{</span>image_test_half<span class="sc">.</span>size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>boxes_test <span class="op">=</span> [<span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>]</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Original boxes: </span><span class="sc">{</span>boxes_test<span class="sc">}</span><span class="ss"> | Half boxes: </span><span class="sc">{</span>half_boxes(boxes_test)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Original image size: (960, 1280) | Half image size: (480, 640)
[INFO] Original boxes: [100, 100, 100, 100] | Half boxes: [50 50 50 50]</code></pre>
</div>
</div>
<p>To plot an image and its assosciated boxes, weâ€™ll do the following steps:</p>
<ol type="1">
<li>Select a random sample from the <code>dataset</code>.</li>
<li>Extract the <code>"image"</code> (our image is in <code>PIL</code> format) and <code>"bbox"</code> keys from the random sample.
<ul>
<li>We can also <em>optionally</em> halve the size of our image/boxes to save space. In our case, we will halve our image and boxes.</li>
</ul></li>
<li>Turn the box coordinates into a <code>torch.tensor</code> (weâ€™ll be using <code>torchvision</code> utilities to plot the image and boxes).</li>
<li>Convert the box format from <code>XYXY</code> to <code>XYWH</code> using <a href="https://pytorch.org/vision/main/generated/torchvision.ops.box_convert.html"><code>torchvision.ops.box_convert</code></a> (we do this because <code>torchvision.utils.draw_bounding_boxes</code> requires <code>XYXY</code> format as input).</li>
<li>Get a list of label names (e.g.&nbsp;<code>"bin", "trash"</code>, etc) assosciated with each of the boxes as well as a list of colours to match (these will be from our <code>colour_palette</code>).</li>
<li>Draw the boxes on the target image by:
<ul>
<li>Turning the image into a tensor with <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.functional.pil_to_tensor.html"><code>torchvision.transforms.functional.pil_to_tensor</code></a>.</li>
<li>Draw the bounding boxes on our image tensor with <a href="https://pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html"><code>torchvision.utils.draw_bounding_boxes</code></a>.</li>
<li>Turn the image and bounding box tensors back into a <code>PIL</code> image with <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.functional.pil_to_tensor.html"><code>torchvision.transforms.functional.pil_to_tensor</code></a>.</li>
</ul></li>
</ol>
<p>Phew!</p>
<p>A fair few stepsâ€¦</p>
<p>But weâ€™ve got this!</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the terms <code>XYXY</code> or <code>XYWH</code> or all of the drawing methods sound a bit confusing or intimidating, donâ€™t worry, thereâ€™s a fair bit going on here.</p>
<p>Weâ€™ll cover bounding box formats, such as <code>XYXY</code> shortly.</p>
<p>In the meantime, if you want to learn more about different bounding box formats and how to draw them, I wrote <a href="https://www.learnml.io/posts/a-guide-to-bounding-box-formats/"><em>A Guide to Bounding Box Formats and How to Draw Them</em></a> which you might find helpful.</p>
</div>
</div>
<div id="cell-36" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting a bounding box on a single image</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.ops <span class="im">import</span> box_convert</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> draw_bounding_boxes</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms.functional <span class="im">import</span> pil_to_tensor, to_pil_image </span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Select a random sample from our dataset</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>random_index <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(dataset[<span class="st">"train"</span>]))</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Showing training sample from index: </span><span class="sc">{</span>random_index<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>random_sample <span class="op">=</span> dataset[<span class="st">"train"</span>][random_index]</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Get image and boxes from random sample</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>random_sample_image <span class="op">=</span> random_sample[<span class="st">"image"</span>]</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>random_sample_boxes <span class="op">=</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"bbox"</span>]</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional: Half the image and boxes for space saving (all of the following code will work with/without half size images)</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>half_random_sample_image <span class="op">=</span> half_image(random_sample_image)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>half_random_sample_boxes <span class="op">=</span> half_boxes(random_sample_boxes)</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Turn box coordinates in a tensor</span></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>boxes_xywh <span class="op">=</span> torch.tensor(half_random_sample_boxes)</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Boxes in XYWH format: </span><span class="sc">{</span>boxes_xywh<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Convert boxes from XYWH -&gt; XYXY </span></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a><span class="co"># torchvision.utils.draw_bounding_boxes requires input boxes in XYXY format (X_min, y_min, X_max, y_max)</span></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>boxes_xyxy <span class="op">=</span> box_convert(boxes<span class="op">=</span>boxes_xywh,</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>                         in_fmt<span class="op">=</span><span class="st">"xywh"</span>,</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>                         out_fmt<span class="op">=</span><span class="st">"xyxy"</span>)</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Boxes XYXY: </span><span class="sc">{</span>boxes_xyxy<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Get label names of target boxes and colours to match</span></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>random_sample_label_names <span class="op">=</span> [categories.int2str(x) <span class="cf">for</span> x <span class="kw">in</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"category_id"</span>]]</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>random_sample_colours <span class="op">=</span> [colour_palette[label_name] <span class="cf">for</span> label_name <span class="kw">in</span> random_sample_label_names]</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Label names: </span><span class="sc">{</span>random_sample_label_names<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Colour names: </span><span class="sc">{</span>random_sample_colours<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Draw the boxes on the image as a tensor and then turn it into a PIL image</span></span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>to_pil_image(</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>half_random_sample_image),</span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>boxes_xyxy,</span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>        colors<span class="op">=</span>random_sample_colours,</span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>random_sample_label_names,</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>        label_colors<span class="op">=</span>random_sample_colours</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Showing training sample from index: 301
Boxes in XYWH format: tensor([[ 46., 203., 388., 335.],
        [188., 358.,  97., 109.],
        [249., 331., 151., 123.]], dtype=torch.float64)
Boxes XYXY: tensor([[ 46., 203., 434., 538.],
        [188., 358., 285., 467.],
        [249., 331., 400., 454.]], dtype=torch.float64)
Label names: ['bin', 'trash', 'hand']
Colour names: [(0, 0, 224), (0, 255, 0), (148, 0, 211)]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="11">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-13-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Outstanding!</p>
<p>Our first official bounding boxes plotted on an image!</p>
<p>Now the idea of Trashify ðŸš® is coming to life.</p>
<p>Depending on the random sample youâ€™re looking at, you should see some combination of <code>['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']</code>.</p>
<p>Our goal will be to build an object detection model to replicate these boxes on a given image.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Getting familiar with a dataset: viewing 100 random samples
</div>
</div>
<div class="callout-body-container callout-body">
<p>Whenever working with a new dataset, I find it good practice to view 100+ random samples of the data.</p>
<p>In our case, this would mean viewing 100 random images with their bounding boxes drawn on them.</p>
<p>Doing so starts to build your own intuition of the data.</p>
<p>Using this intuition, along with evaluation metrics, you can start to get a better idea of how your model might be performing later on.</p>
<p>Keep this in mind for any new dataset or problem space youâ€™re working on.</p>
<p>Start by looking at 100+ random samples.</p>
<p>And yes, generally more is better.</p>
<p>So you can practice by running the code cell above a number of times to see the different kinds of images and boxes in the dataset.</p>
<p>Can you think of any scenarios which the dataset might be missing?</p>
</div>
</div>
</section>
<section id="different-bounding-box-formats" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="different-bounding-box-formats"><span class="header-section-number">5</span> Different bounding box formats</h2>
<p>When drawing our bounding box, we discussed the terms <code>XYXY</code> and <code>XYWH</code>.</p>
<p>Well, we didnâ€™t really discuss these at allâ€¦</p>
<p>But thatâ€™s why weâ€™re here.</p>
<p>One of the most confusing things in the world of object detection is the different formats bounding boxes come in.</p>
<p>Are your boxes in <code>XYXY</code>, <code>XYWH</code> or <code>CXCYWH</code>?</p>
<p>Are they in absolute format?</p>
<p>Or normalized format?</p>
<p>Perhaps a table will help us.</p>
<p>The following table contains a non-exhaustive list of some of the most common bounding box formats youâ€™ll come across in the wild.</p>
<div id="tbl-bbox-formats" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-bbox-formats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Different bounding box formats
</figcaption>
<div aria-describedby="tbl-bbox-formats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Box format</strong></th>
<th><strong>Description</strong></th>
<th><strong>Absolute Example</strong></th>
<th><strong>Normalized Example</strong></th>
<th><strong>Source</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>XYXY</td>
<td>Describes the top left corner coordinates <code>(x1, y1)</code> as well as the bottom right corner coordinates of a box. <br> Also referred to as: <br> <code>[x1, y1, x2, y2]</code> <br> or <br> <code>[x_min, y_min, x_max, y_max]</code></td>
<td><code>[8.9, 275.3, 867.5, 964.0]</code></td>
<td><code>[0.009, 0.215, 0.904, 0.753]</code></td>
<td><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00053000000000000000">PASCAL VOC Dataset</a> uses the absolute version of this format, <a href="https://pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html#draw-bounding-boxes"><code>torchvision.utils.draw_bounding_boxes</code></a> defaults to the absolute version of this format.</td>
</tr>
<tr class="even">
<td>XYWH</td>
<td>Describes the top left corner coordinates <code>(x1, y1)</code> as well as the width (<code>box_width</code>) and height (<code>box_height</code>) of the target box. The bottom right corners <code>(x2, y2)</code> are found by adding the width and height to the top left corner coordinates <code>(x1 + box_width, y1 + box_height)</code>. <br> Also referred to as: <br> <code>[x1, y1, box_width, box_height]</code> <br> or <br> <code>[x_min, y_min, box_width, box_height]</code></td>
<td><code>[8.9, 275.3, 858.6, 688.7]</code></td>
<td><code>[0.009, 0.215, 0.894, 0.538]</code></td>
<td>The <a href="https://cocodataset.org/#format-data">COCO (Common Objects in Context) dataset</a> uses the absolute version of this format, see the section under â€œbboxâ€.</td>
</tr>
<tr class="odd">
<td>CXCYWH</td>
<td>Describes the center coordinates of the bounding box <code>(center_x, center_y)</code> as well as the width (<code>box_width</code>) and height (<code>box_height</code>) of the target box. <br> Also referred to as: <br> <code>[center_x, center_y, box_width, box_height]</code></td>
<td><code>[438.2, 619.65, 858.6, 688.7]</code></td>
<td><code>[0.456, 0.484, 0.894, 0.538]</code></td>
<td>Normalized version introduced in the <a href="https://arxiv.org/abs/1804.02767">YOLOv3 (You Only Look Once) paper</a> and is used by many later forms of YOLO.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="absolute-or-normalized-format" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="absolute-or-normalized-format"><span class="header-section-number">5.1</span> Absolute or normalized format?</h3>
<p>In <strong>absolute</strong> coordinate form, bounding box values are in the same format as the width and height dimensions (e.g.&nbsp;our image is <code>960x1280</code> pixels).</p>
<p>For example in <code>XYXY</code> format: <code>["bin", 8.9, 275.3, 867.5, 964.0]</code></p>
<p>An <code>(x1, y1)</code> (or <code>(x_min, y_min)</code>) coordinate of <code>(8.9, 275.3)</code> means the top left corner is <code>8.9</code> pixels in on the x-axis, and <code>275.3</code> pixels down on the y-axis.</p>
<p>In <strong>normalized</strong> coordinate form, values are between <code>[0, 1]</code> and are proportions of the image width and height.</p>
<p>For example in <code>XYXY</code> format: <code>["bin", 0.009, 0.215, 0.904, 0.753]</code></p>
<p>A normalized <code>(x1, y1)</code> (or <code>(x_min, y_min)</code>) coordinate of <code>(0.009, 0.215)</code> means the top left corner is <code>0.009 * image_width</code> pixels in on the x-axis and <code>0.215 * image_height</code> down on the y-axis.</p>
<p>To convert absolute coordinates to normalized, you can divide x-axis values by the image width and y-axis values by the image height.</p>
<p><span class="math display">\[
x_{\text{normalized}} = \frac{x_{\text{absolute}}}{\text{image\_width}} \quad y_{\text{normalized}} = \frac{y_{\text{absolute}}}{\text{image\_height}}
\]</span></p>
</section>
<section id="which-bounding-box-format-should-you-use" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="which-bounding-box-format-should-you-use"><span class="header-section-number">5.2</span> Which bounding box format should you use?</h3>
<p>The bounding box format you use will depend on the framework, model and existing data youâ€™re trying to use.</p>
<p>For example, the take the following frameworks:</p>
<ul>
<li><strong>PyTorch</strong> - If youâ€™re using PyTorch pre-trained models, for example, <a href="https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn"><code>torchvision.models.detection.fasterrcnn_resnet50_fpn</code></a>, youâ€™ll want <strong>absolute</strong> <code>XYXY</code> (<code>[x1, y1, x2, y2]</code>) format.</li>
<li><strong>Hugging Face Transformers</strong> - If youâ€™re using a Hugging Face Transformers model such as <a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr">Conditional DETR</a>, youâ€™ll want to take note that <a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward">outputs from the model can be of one type</a> (e.g.&nbsp;<code>CXCYWH</code>) but they can be <a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrFeatureExtractor.post_process_object_detection">post-processed into another type</a> (e.g.&nbsp;<strong>absolute</strong> <code>XYXY</code>).</li>
<li><strong>Ultralytics YOLO</strong> - If youâ€™re using a YOLO-like model such as <a href="https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format">Ultralytics YOLO</a>, youâ€™ll want <strong>normalized</strong> <code>CXCYWH</code> (<code>[center_x, center_y, width, height]</code>) format.</li>
<li><strong>Google Gemini</strong> - If youâ€™re using <a href="https://ai.google.dev/gemini-api/docs/vision?lang=python#bbox">Google Gemini to predict bounding boxes on your images</a>, then youâ€™ll want to pay attention to the special <code>[y_min, x_min, y_max, x_max]</code> (<code>YXYX</code>) normalized coordinates.</li>
</ul>
<p>Or if you note that someone has said their model is pre-trained on the COCO dataset, chances are the data has been formatted in <code>XYWH</code> format (see <a href="#tbl-bbox-formats" class="quarto-xref">Table&nbsp;1</a>).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>For more on different bounding box formats and how to draw them, see <a href="https://www.learnml.io/posts/a-guide-to-bounding-box-formats/"><em>A Guide to Bounding Box Formats and How to Draw Them</em></a>.</p>
</div>
</div>
<div id="cell-41" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TK - should I functionize the plotting of boxes and image so we can do input/output with tensors + data augmentations on that (E.g. original: image, augmented: image),</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># - is this needed?</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="getting-an-object-detection-model" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="getting-an-object-detection-model"><span class="header-section-number">6</span> Getting an object detection model</h2>
<p>There are two main ways of getting an object detection model:</p>
<ol type="1">
<li><strong>Building it yourself.</strong> For example, constructing it layer by layer, testing it and training it on your target problem.</li>
<li><strong>Using an existing one.</strong> For example, find an existing model on a problem space similar to your own and then adapt it via <strong>transfer learning</strong> (TK - add link to glossary) to your own task.</li>
</ol>
<p>In our case, weâ€™re going to focus on the latter.</p>
<p>Weâ€™ll be taking a pre-trained object detection model and fine-tuning it on our Trashify ðŸš® dataset so it outputs the boxes and labels weâ€™re after.</p>
<section id="places-to-get-object-detection-models" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="places-to-get-object-detection-models"><span class="header-section-number">6.1</span> Places to get object detection models</h3>
<p>Instead of building your own machine learning model from scratch, itâ€™s common practice to take an existing model that works on similar problem space to yours and then <strong>fine-tune</strong> (TK - add link to glossary) it to your own use case.</p>
<p>There are several places to get object detection models:</p>
<div id="tbl-detection-models" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-detection-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Places to get pre-trained object detection models
</figcaption>
<div aria-describedby="tbl-detection-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Location</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://huggingface.co/models?pipeline_tag=object-detection&amp;sort=trending">Hugging Face Hub</a></td>
<td>One the best places on the internet to find open-source machine learning models of nearly any kind. You can find pre-trained object detection models here such as <a href="https://huggingface.co/facebook/detr-resnet-50"><code>facebook/detr-resnet-50</code></a>, a model from Facebook (Meta) and <a href="https://huggingface.co/microsoft/conditional-detr-resnet-50"><code>microsoft/conditional-detr-resnet-50</code></a>, a model from Microsoft and the model weâ€™re going to use as our base model. Many of the models are permissively licensed, meaning you can use them for your own projects.</td>
</tr>
<tr class="even">
<td><a href="https://pytorch.org/vision/stable/models.html"><code>torchvision</code></a></td>
<td>PyTorchâ€™s built-in domain library for computer vision has several <a href="https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection">pre-trained object detection models</a> which you can use in your own workflows.</td>
</tr>
<tr class="odd">
<td><a href="https://paperswithcode.com/task/object-detection">paperswithcode.com/task/object-detection</a></td>
<td>Whilst not a direct place to download object detection models from, paperswithcode contains benchmarks for many machine learning tasks (including object detection) which shows the current state of the art (best performing) models and usually includes links to where to get the code.</td>
</tr>
<tr class="even">
<td><a href="https://github.com/facebookresearch/detectron2">Detectron2</a></td>
<td>Detectron2 is an open-source library to help with many of the tasks in detecting items in images. Inside youâ€™ll find several pre-trained and adaptable models as well as utilities such as data loaders for object detection and segmentation tasks.</td>
</tr>
<tr class="odd">
<td>YOLO Series</td>
<td>A running series of <a href="https://arxiv.org/abs/1506.02640">â€œYou Only Look Onceâ€ models</a>. Usually, the higher the number, the better performing. For example, <a href="https://github.com/ultralytics/ultralytics"><code>YOLOv11</code></a> by Ultralytics should outperform <a href="https://github.com/THU-MIG/yolov10"><code>YOLOv10</code></a>, however, this often requires testing on your own dataset. Beware of the license, it is under the <a href="https://en.wikipedia.org/wiki/GNU_Affero_General_Public_License">AGPL-3.0 license</a> which may cause issues in some organizations.</td>
</tr>
<tr class="even">
<td><a href="https://github.com/open-mmlab/mmdetection"><code>mmdetection</code> library</a></td>
<td>An open-source library from the OpenMMLab which contains many different open-source models as well as detection-specific utilties.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>When you find a pre-trained object detection model, youâ€™ll often see statements such as:</p>
<blockquote class="blockquote">
<p><em>Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images).</em></p>
<p>Source: <a href="https://huggingface.co/microsoft/conditional-detr-resnet-50">https://huggingface.co/microsoft/conditional-detr-resnet-50</a></p>
</blockquote>
<p>This means the model has already been trained on the <a href="https://cocodataset.org/#home">COCO object detection dataset</a> which contains 118,000 images and <a href="https://cocodataset.org/#explore">80 classes</a> such as <code>["cake", "person", "skateboard"...]</code>.</p>
<p>This is a good thing.</p>
<p>It means that the model should have a fairly good starting point when we try to adapt it to our own project.</p>
</section>
<section id="downloading-our-model-from-hugging-face" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="downloading-our-model-from-hugging-face"><span class="header-section-number">6.2</span> Downloading our model from Hugging Face</h3>
<p>For our Trashify ðŸš® project weâ€™re going to be using the pre-trained object detection model <a href="https://huggingface.co/microsoft/conditional-detr-resnet-50"><code>microsoft/conditional-detr-resnet-50</code></a> which was originally introduced in the paper <a href="https://arxiv.org/abs/2108.06152"><em>Conditional DETR for Fast Training Convergence</em></a>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The term â€œDETRâ€ stands for â€œDEtection TRansformerâ€.</p>
<p>Where â€œTransformerâ€ refers to the <a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)">Transformer neural network architecture</a>, specifically the <a href="https://en.wikipedia.org/wiki/Vision_transformer">Vision Transformer</a> (or ViT) rather than the Hugging Face <code>transformers</code> library (quite confusing, yes).</p>
<p>So DETR means â€œperforming detection with the Transformer architectureâ€.</p>
<p>And the â€œResNetâ€ part stands for â€œ<a href="https://en.wikipedia.org/wiki/Residual_neural_network">Residual Neural Network</a>â€ which is a common computer vision backbone. The â€œ50â€ refers to the number of layers in the network. Saying â€œResNet-50â€ means the 50 layer version of ResNet. ResNet-101 and ResNet-18 are two other larger and smaller variants.</p>
</div>
</div>
<p>To use this model, there are some helpful documentation resources we should be aware of:</p>
<div id="tbl-model-docs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-model-docs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Model documentation resources
</figcaption>
<div aria-describedby="tbl-model-docs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Resource</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#conditional-detr">Conditional DETR documentation</a></td>
<td style="text-align: left;">Contains detailed information on each of the <code>transformers.ConditionalDetr</code> classes.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig"><code>transformers.ConditionalDetrConfig</code></a></td>
<td style="text-align: left;">Contains the configuration settings for our model such as number of layers and other hyperparameters.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor"><code>transformers.ConditionalDetrImageProcessor</code></a></td>
<td style="text-align: left;">Contains several preprocessing on post processing functions and settings for data going into and out of our model. Here we can set values such as <code>size</code> in the <a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess"><code>preprocess</code></a> method which will resize our images to a certain size. We can also use the <a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.post_process_object_detection"><code>post_process_object_detection</code></a> method to process the raw outputs of our model into a more usable format.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection"><code>transformers.ConditionalDetrModelForObjectdetection</code></a></td>
<td style="text-align: left;">This will enable us to load the Conditional DETR model weights and enable to pass data through them via the <a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward"><code>forward</code></a> method.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoImageProcessor"><code>transformers.AutoImageProcessor</code></a></td>
<td style="text-align: left;">This will enable us to create an instance of <code>transformers.ConditionalDetrImageProcessor</code> by passing the model name <code>microsoft/conditional-detr-resnet-50</code> to the <code>from_pretrained</code> method. Hugging Face Transformers uses several <a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection">Auto Classes</a> for various problem spaces and models.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection"><code>transformers.AutoModelForObjectDetection</code></a></td>
<td style="text-align: left;">Enables us to load the model architecture and weights for the Conditional DETR architecture by passing the model name <code>microsoft/conditional-detr-resnet-50</code> to the <a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection.from_pretrained"><code>from_pretrained</code> method</a>.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Weâ€™ll get hands-on which each of these throughout the project.</p>
<p>For now, if youâ€™d like to read up more on each, Iâ€™d highly recommend it.</p>
<p>Knowing how to navigate and read through a frameworkâ€™s documentation is a very helpful skill to have.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>There are other object detection models we could try on the Hugging Face Hub such as <a href="https://huggingface.co/facebook/detr-resnet-50"><code>facebook/detr-resnet-50</code></a> or <a href="https://huggingface.co/IDEA-Research/dab-detr-resnet-50-dc5-pat3"><code>IDEA-Research/dab-detr-resnet-50-dc5-pat3</code></a>.</p>
<p>For now, weâ€™ll stick with <code>microsoft/conditional-detr-resnet-50</code>.</p>
<p>Itâ€™s easy to get stuck figuring out which model to use instead of just trying one and seeing how it goes.</p>
<p>Best to get something small working with one model and try another one later as part of a series of experiments to try and improve your results.</p>
</div>
</div>
</section>
<section id="loading-our-models-processor" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="loading-our-models-processor"><span class="header-section-number">6.3</span> Loading our modelâ€™s processor</h3>
<p>To begin, letâ€™s first load our modelâ€™s processor.</p>
<p>Weâ€™ll use this to prepare our input images for the model.</p>
<p>To do so, weâ€™ll use <code>transformers.AutoImageProcessor</code> and pass our target model name to the <code>from_pretrained</code> method.</p>
<div id="cell-46" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoImageProcessor</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"microsoft/conditional-detr-resnet-50"</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># MODEL_NAME = "facebook/detr-resnet-50" # Could also use this model as an another experiment</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the image processor</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoImageProcessor.from_pretrained(pretrained_model_name_or_path<span class="op">=</span>MODEL_NAME)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Check out the image processor</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>image_processor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>ConditionalDetrImageProcessor {
  "do_convert_annotations": true,
  "do_normalize": true,
  "do_pad": true,
  "do_rescale": true,
  "do_resize": true,
  "format": "coco_detection",
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "ConditionalDetrImageProcessor",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "pad_size": null,
  "resample": 2,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 1333,
    "shortest_edge": 800
  }
}</code></pre>
</div>
</div>
<p>Ok, a few things going on here.</p>
<p>These parameters will transform our input images before we pass them to our model.</p>
<p>One of the first things to see is the <code>image_processor</code> is expecting our bounding boxes to be in <a href="https://cocodataset.org/#format-data">COCO (or <code>coco_detection</code>) format</a> (this is the default).</p>
<p>Weâ€™ll see what this looks like later on but our processor wants this format because thatâ€™s the format our model has been trained on (itâ€™s generally best practice to input data to a model in the same way its been trained on, otherwise you might get poor results).</p>
<p>Another thing to notice is that our input images will be resized to the values of the <code>size</code> parameter.</p>
<p>In our case, itâ€™s currently:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co">"size"</span>: {</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"longest_edge"</span>: <span class="dv">1333</span>,</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"shortest_edge"</span>: <span class="dv">800</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Which means that the longest edge will have size less or equal to <code>1333</code> and the shortest edge less or equal to <code>800</code>.</p>
<p>For simplicity, weâ€™ll change this shortly to make both sides the same size.</p>
<p>You can read more about what each of these does in the <a href="https://huggingface.co/docs/transformers/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor"><code>transformers.ConditionalDetrImageProcessor</code> documentation</a>.</p>
<p>Letâ€™s update our instance of <code>transformers.ConditionalDetrImageProcessor</code> with a few custom parameters:</p>
<ul>
<li><code>do_convert_annotations=True</code> - This is the default and it will convert our boxes to the format <code>CXCYWH</code> or <code>(center_x, center_y, width, height)</code> (see <a href="#tbl-bbox-formats" class="quarto-xref">Table&nbsp;1</a>) in the range <code>[0, 1]</code>.</li>
<li><code>size</code> - Weâ€™ll update the <code>size</code> dictionary so all of our images have <code>"longest_edge": 640</code> and <code>"shortest_edge: 640"</code>. Weâ€™ll use a value of <code>640</code> which is a common size in world of object detection. But there are also other sizes such as <code>300x300</code>, <code>480x480</code>, <code>512x512</code>, <code>800x800</code> and more.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Depending on what task youâ€™re working on, you might want to tweak the image resolution youâ€™re working with.</p>
<p>For example, I like this quote from <a href="https://lucasb.eyer.be/articles/vit_cnn_speed.html">Lucas Beyer</a>, a former research scientist at DeepMind and engineer at OpenAI:</p>
<blockquote class="blockquote">
<p>My conservative claim is that you can always stretch to a square, and for:</p>
<p>natural images, meaning most photos, 224pxÂ² is enough; text in photos, phone screens, diagrams and charts, 448pxÂ² is enough; desktop screens and single-page documents, 896pxÂ² is enough.</p>
</blockquote>
<p>Typically, in the case of object detection, youâ€™ll want to use a higher value.</p>
<p>But this is another thing that is open to experimentation.</p>
</div>
</div>
<div id="cell-48" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set image size</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>IMAGE_SIZE <span class="op">=</span> <span class="dv">640</span> <span class="co"># we could try other sizes here: 300x300, 480x480, 512x512, 640x640, 800x800 (best to experiment and see which works best)</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new instance of the image processor with the desired image size</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoImageProcessor.from_pretrained(</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    pretrained_model_name_or_path<span class="op">=</span>MODEL_NAME,</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"coco_detection"</span>, <span class="co"># this is the default</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    do_convert_annotations<span class="op">=</span><span class="va">True</span>, <span class="co"># defaults to True, converts boxes to (center_x, center_y, width, height) in range [0, 1]</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span>{<span class="st">"shortest_edge"</span>: IMAGE_SIZE, </span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>          <span class="st">"longest_edge"</span>: IMAGE_SIZE}</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional: View the docstring of our image_processor.preprocess function</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="co"># image_processor.preprocess?</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Check out our new image processor size</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>image_processor.size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>{'shortest_edge': 640, 'longest_edge': 640}</code></pre>
</div>
</div>
<p>Beautiful!</p>
<p>Now our images will be resized to a square of size <code>640x640</code> when we pass them to our model.</p>
<p>How about we try to preprocess our <code>random_sample</code>?</p>
<p>To do so, we can pass its <code>"image"</code> key and <code>"annotations"</code> key to our <code>image_processor</code>â€™s <a href="https://huggingface.co/docs/transformers/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess"><code>preprocess</code></a> method.</p>
<p>Letâ€™s try!</p>
<div id="cell-50" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Try to process a single image and annotation pair (spoiler: this will error)</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>random_sample_preprocessed <span class="op">=</span> image_processor.preprocess(images<span class="op">=</span>random_sample[<span class="st">"image"</span>],</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>                                                        annotations<span class="op">=</span>random_sample[<span class="st">"annotations"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[15], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Try to process a single image and annotation pair (spoiler: this will error)</span>
<span class="ansi-green-fg">----&gt; 2</span> random_sample_preprocessed <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">image_processor</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">preprocess</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">images</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">random_sample</span><span class="ansi-yellow-bg">[</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">image</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span class="ansi-yellow-bg">                                                        </span><span class="ansi-yellow-bg">annotations</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">random_sample</span><span class="ansi-yellow-bg">[</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">annotations</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/conditional_detr/image_processing_conditional_detr.py:1422</span>, in <span class="ansi-cyan-fg">ConditionalDetrImageProcessor.preprocess</span><span class="ansi-blue-fg">(self, images, annotations, return_segmentation_masks, masks_path, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, do_convert_annotations, image_mean, image_std, do_pad, format, return_tensors, data_format, input_data_format, pad_size, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1420</span> <span style="color:rgb(0,135,0)">format</span> <span style="color:rgb(98,98,98)">=</span> AnnotationFormat(<span style="color:rgb(0,135,0)">format</span>)
<span class="ansi-green-fg ansi-bold">   1421</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> annotations <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>:
<span class="ansi-green-fg">-&gt; 1422</span>     <span class="ansi-yellow-bg">validate_annotations</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">format</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">SUPPORTED_ANNOTATION_FORMATS</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">annotations</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   1424</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> (
<span class="ansi-green-fg ansi-bold">   1425</span>     masks_path <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">   1426</span>     <span style="font-weight:bold;color:rgb(175,0,255)">and</span> <span style="color:rgb(0,135,0)">format</span> <span style="color:rgb(98,98,98)">==</span> AnnotationFormat<span style="color:rgb(98,98,98)">.</span>COCO_PANOPTIC
<span class="ansi-green-fg ansi-bold">   1427</span>     <span style="font-weight:bold;color:rgb(175,0,255)">and</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="color:rgb(0,135,0)">isinstance</span>(masks_path, (pathlib<span style="color:rgb(98,98,98)">.</span>Path, <span style="color:rgb(0,135,0)">str</span>))
<span class="ansi-green-fg ansi-bold">   1428</span> ):
<span class="ansi-green-fg ansi-bold">   1429</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-fg ansi-bold">   1430</span>         <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">The path to the directory containing the mask PNG files should be provided as a</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">   1431</span>         <span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)"> `pathlib.Path` or string object, but is </span><span style="font-weight:bold;color:rgb(175,95,135)">{</span><span style="color:rgb(0,135,0)">type</span>(masks_path)<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)"> instead.</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">   1432</span>     )

File <span class="ansi-green-fg">~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/image_utils.py:851</span>, in <span class="ansi-cyan-fg">validate_annotations</span><span class="ansi-blue-fg">(annotation_format, supported_annotation_formats, annotations)</span>
<span class="ansi-green-fg ansi-bold">    849</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> annotation_format <span style="font-weight:bold;color:rgb(175,0,255)">is</span> AnnotationFormat<span style="color:rgb(98,98,98)">.</span>COCO_DETECTION:
<span class="ansi-green-fg ansi-bold">    850</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> valid_coco_detection_annotations(annotations):
<span class="ansi-green-fg">--&gt; 851</span>         <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-fg ansi-bold">    852</span>             <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts </span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">    853</span>             <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">(batch of images) with the following keys: `image_id` and `annotations`, with the latter </span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">    854</span>             <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">being a list of annotations in the COCO format.</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">    855</span>         )
<span class="ansi-green-fg ansi-bold">    857</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> annotation_format <span style="font-weight:bold;color:rgb(175,0,255)">is</span> AnnotationFormat<span style="color:rgb(98,98,98)">.</span>COCO_PANOPTIC:
<span class="ansi-green-fg ansi-bold">    858</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> valid_coco_panoptic_annotations(annotations):

<span class="ansi-red-fg">ValueError</span>: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: `image_id` and `annotations`, with the latter being a list of annotations in the COCO format.</pre>
</div>
</div>
</div>
<p>Oh no!</p>
<p>We get an error:</p>
<blockquote class="blockquote">
<p>ValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: <code>image_id</code> and <code>annotations</code>, with the latter being a list of annotations in the COCO format.</p>
</blockquote>
<p>Okay so it turns out that our annotations arenâ€™t in the format that the <code>preprocess</code> method was expecting.</p>
<p>Since our pre-trained model was trained on the COCO dataset, the <code>preprocess</code> method expects input data to be in line with the COCO format.</p>
<p>We can fix this later on by adjusting our annotations.</p>
<p>How about we try to preprocess just a single image instead?</p>
<div id="cell-52" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess our target sample</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>random_sample_preprocessed_image_only <span class="op">=</span> image_processor.preprocess(images<span class="op">=</span>random_sample[<span class="st">"image"</span>],</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>                                                                   annotations<span class="op">=</span><span class="va">None</span>, <span class="co"># no annotations this time </span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>                                                                   return_tensors<span class="op">=</span><span class="st">"pt"</span>) <span class="co"># return as PyTorch tensors</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Uncomment to see the full output</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co"># print(random_sample_preprocessed_image_only)</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the keys of the preprocessed image</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(random_sample_preprocessed_image_only.keys())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>dict_keys(['pixel_values', 'pixel_mask'])</code></pre>
</div>
</div>
<p>Nice! It looks like the <code>preprocess</code> method works on a single image.</p>
<p>And it seems like we get a dictionary output with the following keys:</p>
<ul>
<li><code>pixel_values</code> - the processed pixel values of the input image.</li>
<li><code>pixel_mask</code> - a mask multiplier for the pixel values as to whether they should be paid attention to or not (a value of <code>0</code> means the pixel value should be ignored by the model and a value of <code>1</code> means the pixel value should be paid attention to by the model).</li>
</ul>
<p>In our case, all values of the <code>pixel_mask</code> are <code>1</code> since weâ€™re not using any masks.</p>
<p>Letâ€™s check.</p>
<div id="cell-54" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check all values of the pixel_mask are 1</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>torch.<span class="bu">all</span>(random_sample_preprocessed_image_only[<span class="st">"pixel_mask"</span>][<span class="dv">0</span>]) <span class="op">==</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>tensor(True)</code></pre>
</div>
</div>
<p>Beautiful!</p>
<p>Now how about we inspect our processed imageâ€™s shape?</p>
<div id="cell-56" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Uncomment to inspect all preprocessed pixel values</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># print(random_sample_preprocessed_image_only["pixel_values"][0])</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Original image shape: </span><span class="sc">{</span>random_sample[<span class="st">'image'</span>]<span class="sc">.</span>size<span class="sc">}</span><span class="ss"> -&gt; [width, height]"</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Preprocessed image shape: </span><span class="sc">{</span>random_sample_preprocessed_image_only[<span class="st">'pixel_values'</span>][<span class="dv">0</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [colour_channles, height, width]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Original image shape: (960, 1280) -&gt; [width, height]
[INFO] Preprocessed image shape: torch.Size([3, 640, 480]) -&gt; [colour_channles, height, width]</code></pre>
</div>
</div>
<p>Ok wonderful, it looks like our image has been downsized to <code>[3, 640, 480]</code> (3 colour channels, 640 pixels high, 480 pixels wide).</p>
<p>This is down from its original size of <code>[960, 1280]</code> (1280 pixels high, 960 pixels wide).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The order of image dimensions can differ between libraries and frameworks.</p>
<p>For example, image tensors in PyTorch typically follow the format <code>[colour_channels, height, width]</code> whereas in TensorFlow they follow <code>[height, width, colour_channels]</code>.</p>
<p>And in PIL, the format is <code>[width, height]</code>.</p>
<p>As you can imagine, this can get confusing.</p>
<p>However, with some practice, youâ€™ll be able to decipher which is which.</p>
<p>And if your images and bounding boxes start looking strange, perhaps checking the image dimension and format can help.</p>
</div>
</div>
</section>
<section id="discussing-how-to-convert-our-annotations-into-coco-format" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="discussing-how-to-convert-our-annotations-into-coco-format"><span class="header-section-number">6.4</span> Discussing how to convert our annotations into COCO format</h3>
<p>Our <code>image_processor.processor</code> method expects input annotations in COCO format.</p>
<p>In the <a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess">documentation we can read that</a> the <code>annotations</code> parameter taks in a list of dictionaries with the following keys:</p>
<ul>
<li><code>"image_id"</code> (<code>int</code>): The image id.</li>
<li><code>"annotations"</code> (<code>List[Dict]</code>): List of annotations for an image. Each annotation should be a dictionary. An image can have no annotations, in which case the list should be empty.</li>
</ul>
<p>As for the <code>"annotations"</code> field, this should be a list of dictionaries containing individual annotations in <a href="https://cocodataset.org/#format-data">COCO format</a>:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># COCO format, see: https://cocodataset.org/#format-data  </span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>[{</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"image_id"</span>: <span class="dv">42</span>,</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"annotations"</span>: [{</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"id"</span>: <span class="dv">123456</span>,</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"category_id"</span>: <span class="dv">1</span>,</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"iscrowd"</span>: <span class="dv">0</span>,</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"segmentation"</span>: [</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">42.0</span>, <span class="fl">55.6</span>, ... <span class="fl">99.3</span>, <span class="fl">102.3</span>]</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"image_id"</span>: <span class="dv">42</span>, <span class="co"># this matches the 'image_id' field above</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">"area"</span>: <span class="fl">135381.07</span>,</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">"bbox"</span>: [<span class="fl">523.70</span>,</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>                 <span class="fl">545.09</span>,</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>                 <span class="fl">402.79</span>,</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>                 <span class="fl">336.11</span>]</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Next annotation in the same format as the previous one (one annotation per dict).</span></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For example, if an image had 4 bounding boxes, there would be a list of 4 dictionaries</span></span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># each containing a single annotation.</span></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>    ...]</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>}]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Letâ€™s breakdown each of the fields in the COCO annotation:</p>
<div id="tbl-coco-format" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-coco-format-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: COCO data format keys breakdown
</figcaption>
<div aria-describedby="tbl-coco-format-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Field</th>
<th>Requirement</th>
<th>Data Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>image_id</code> (top-level)</td>
<td>Required</td>
<td>Integer</td>
<td>ID of the target image.</td>
</tr>
<tr class="even">
<td><code>annotations</code></td>
<td>Required</td>
<td>List[Dict]</td>
<td>List of dictionaries with one box annotation per dict. Can be empty if there are no boxes.</td>
</tr>
<tr class="odd">
<td><code>id</code></td>
<td>Not required</td>
<td>Integer</td>
<td>ID of the particular annotation.</td>
</tr>
<tr class="even">
<td><code>category_id</code></td>
<td>Required</td>
<td>Integer</td>
<td>ID of the class the box relates to (e.g.&nbsp;<code>{0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash'}</code>).</td>
</tr>
<tr class="odd">
<td><code>segmentation</code></td>
<td>Not required</td>
<td>List or None</td>
<td>Segmentation mask related to an annotation instance. Focus is on boxes, not segmentation.</td>
</tr>
<tr class="even">
<td><code>image_id</code> (inside <code>annotations</code> field)</td>
<td>Required</td>
<td>Integer</td>
<td>ID of the target image the particular box relates to, should match <code>image_id</code> on the top-level field.</td>
</tr>
<tr class="odd">
<td><code>area</code></td>
<td>Not required</td>
<td>Float</td>
<td>Area of the target bounding box (e.g.&nbsp;box height * width).</td>
</tr>
<tr class="even">
<td><code>bbox</code></td>
<td>Required</td>
<td>List[Float]</td>
<td>Coordinates of the target bounding box in <code>XYWH</code> (<code>[x, y, width, height]</code>) format. <code>(x, y)</code> are the top left corner coordinates, <code>width</code> and <code>height</code> are dimensions.</td>
</tr>
<tr class="odd">
<td><code>is_crowd</code></td>
<td>Not required</td>
<td>Int</td>
<td>Boolean flag (0 or 1) to indicate whether or not an object is multiple (a crowd) of the same thing. For example, a crowd of â€œpeopleâ€ or a group of â€œapplesâ€ rather than a single apple.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>And now our annotation data comes in the format:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>{<span class="st">'image'</span>: <span class="op">&lt;</span>PIL.Image.Image image mode<span class="op">=</span>RGB size<span class="op">=</span><span class="dv">960</span><span class="er">x1280</span><span class="op">&gt;</span>,</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a> <span class="st">'image_id'</span>: <span class="dv">292</span>,</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a> <span class="st">'annotations'</span>: {<span class="st">'file_name'</span>: [<span class="st">'00347467-13f1-4cb9-94aa-4e4369457e0c.jpeg'</span>,</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>   <span class="st">'00347467-13f1-4cb9-94aa-4e4369457e0c.jpeg'</span>],</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">'image_id'</span>: [<span class="dv">292</span>, <span class="dv">292</span>],</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">'category_id'</span>: [<span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">'bbox'</span>: [[<span class="fl">523.7000122070312</span>,</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="fl">545.0999755859375</span>,</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    <span class="fl">402.79998779296875</span>,</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    <span class="fl">336.1000061035156</span>],</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">10.399999618530273</span>,</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>    <span class="fl">163.6999969482422</span>,</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    <span class="fl">943.4000244140625</span>,</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>    <span class="fl">1101.9000244140625</span>]],</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>  <span class="st">'iscrowd'</span>: [<span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>  <span class="st">'area'</span>: [<span class="fl">135381.078125</span>, <span class="fl">1039532.4375</span>]},</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a> <span class="st">'label_source'</span>: <span class="st">'manual_prodigy_label'</span>,</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a> <span class="st">'image_source'</span>: <span class="st">'manual_taken_photo'</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>How about we write some code to convert our current annotation format to COCO format?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Itâ€™s common practice to get a dataset in a certain format and then have to preprocess it into another format before you can use it with a model.</p>
<p>Weâ€™re getting hands-on and practicing here so when it comes to working on converting another dataset, youâ€™ve already had some practice.</p>
</div>
</div>
</section>
<section id="tk---creating-dataclasses-to-represent-the-coco-bounding-box-format" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="tk---creating-dataclasses-to-represent-the-coco-bounding-box-format"><span class="header-section-number">6.5</span> TK - Creating dataclasses to represent the COCO bounding box format</h3>
<p>Letâ€™s write some code to transform our existing annotation data into the format required by <code>image_processor</code>.</p>
<p>Weâ€™ll start by creating two <a href="https://docs.python.org/3/library/dataclasses.html#module-dataclasses">Python dataclasses</a> to house our desired COCO annotation format.</p>
<p>To do this weâ€™ll:</p>
<ol type="1">
<li>Create <code>SingleCOCOAnnotation</code> which contains the format structure of a single COCO annotation.</li>
<li>Create <code>ImageCOCOAnnotations</code> which contains all of the annotations for a given image in COCO format. This may be a single instance of <code>SingleCOCOAnnotation</code> or multiple.</li>
</ol>
<p>Weâ€™ll decorate both of these with the <a href="https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass"><code>@dataclass</code></a> decorator.</p>
<p>Using a <code>@dataclass</code> gives several benefits:</p>
<ul>
<li>Type hints - we can define the types of objects we want in the class definition, for example, we want <code>image_id</code> to be an <code>int</code>.</li>
<li>Helpful built-in methods - we can use methods such as <a href="https://docs.python.org/3/library/dataclasses.html#dataclasses.asdict"><code>asdict</code></a> to convert our <code>@dataclass</code> into a dictionary (COCO wants lists of dictionaries).</li>
<li>Data validation - we can use methods such as <a href="https://docs.python.org/3/library/dataclasses.html#dataclasses.__post_init__"><code>__post_init__</code></a> to run checks on our <code>@dataclass</code> as itâ€™s initialized, for example, we always want the length of <code>bbox</code> to be 4 (bounding box coordinates in <code>XYWH</code> format).</li>
</ul>
<div id="cell-60" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass, asdict</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Tuple</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create a dataclass for a single COCO annotation</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SingleCOCOAnnotation:</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""An instance of a single COCO annotation. </span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Represent a COCO-formatted (see: https://cocodataset.org/#format-data) single instance of an object </span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="co">    in an image. </span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a><span class="co">        image_id: Unique integer identifier for the image which the annotation belongs to.</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a><span class="co">        category_id: Integer identifier for the target object label/category (e.g. "0" for "bin").</span></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a><span class="co">        bbox: List of floats containing target bounding box coordinates in absolute XYWH format ([x_top_left, y_top_left, width, height]).</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a><span class="co">        area: Area of the target bounding box. Defaults to 0.0.</span></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a><span class="co">        iscrowd: Boolean flag (0 or 1) indicating whether the target is a crowd of objects, for example, a group of </span></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a><span class="co">            apples rather than a single apple. Defaults to 0.</span></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>    image_id: <span class="bu">int</span></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>    category_id: <span class="bu">int</span></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>    bbox: List[<span class="bu">float</span>] <span class="co"># bboxes in XYWH format ([x_top_left, y_top_left, width, height])</span></span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>    area: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>    iscrowd: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make sure the bbox is always a list of 4 values (XYWH format)</span></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> __post_init__(<span class="va">self</span>):</span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.bbox) <span class="op">!=</span> <span class="dv">4</span>:</span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"bbox must contain exactly 4 values, current length: </span><span class="sc">{</span><span class="bu">len</span>(<span class="va">self</span>.bbox)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Create a dataclass for a collection of COCO annotations for a single image</span></span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ImageCOCOAnnotations:</span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A collection of COCO annotations for a single image_id.</span></span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a><span class="co">        image_id: Unique integer identifier for the image which the annotations belong to.</span></span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a><span class="co">        annotations: List of SingleCOCOAnnotation instances.</span></span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a>    image_id: <span class="bu">int</span></span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a>    annotations: List[SingleCOCOAnnotation]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Beautiful!</p>
<p>Letâ€™s now inspect our <code>SingleCOCOAnnotation</code> dataclass.</p>
<p>We can use the <code>SingleCOCOAnnotation?</code> syntax to view the docstring of the class.</p>
<div id="cell-62" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># One of the benefits of using a dataclass is that we can inspect the attributes with the `?` syntax</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>SingleCOCOAnnotation?</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Init signature:
SingleCOCOAnnotation(
    image_id: int,
    category_id: int,
    bbox: List[float],
    area: float = 0.0,
    iscrowd: int = 0,
) -&gt; None
Docstring:     
An instance of a single COCO annotation. 

Represent a COCO-formatted (see: https://cocodataset.org/#format-data) single instance of an object 
in an image. 

Attributes:
    image_id: Unique integer identifier for the image which the annotation belongs to.
    category_id: Integer identifier for the target object label/category (e.g. "0" for "bin").
    bbox: List of floats containing target bounding box coordinates in absolute XYWH format ([x_top_left, y_top_left, width, height]).
    area: Area of the target bounding box. Defaults to 0.0.
    iscrowd: Boolean flag (0 or 1) indicating whether the target is a crowd of objects, for example, a group of 
        apples rather than a single apple. Defaults to 0.
Type:           type
Subclasses:     </code></pre>
</div>
</div>
<p>We can also see the error handling of our <code>__post_init__</code> method in action by trying to create an instance of <code>SingleCOCOAnnotation</code> with an incorrect number of bbox values.</p>
<div id="cell-64" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's try our SingleCOCOAnnotation dataclass (this will error since the bbox doesn't have 4 values)</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>SingleCOCOAnnotation(image_id<span class="op">=</span><span class="dv">42</span>, </span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>                     category_id<span class="op">=</span><span class="dv">0</span>, </span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>                     bbox<span class="op">=</span>[<span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>]) <span class="co"># missing a 4th value</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[45], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Let's try our SingleCOCOAnnotation dataclass (this will error since the bbox doesn't have 4 values)</span>
<span class="ansi-green-fg">----&gt; 2</span> <span class="ansi-yellow-bg">SingleCOCOAnnotation</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">image_id</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">42</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span>
<span class="ansi-green-fg ansi-bold">      3</span> <span class="ansi-yellow-bg">                     </span><span class="ansi-yellow-bg">category_id</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">0</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span>
<span class="ansi-green-fg ansi-bold">      4</span> <span class="ansi-yellow-bg">                     </span><span class="ansi-yellow-bg">bbox</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">[</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">100</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">100</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">100</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">)</span> <span style="font-style:italic;color:rgb(95,135,135)"># missing a 4th value</span>

File <span class="ansi-green-fg">&lt;string&gt;:8</span>, in <span class="ansi-cyan-fg">__init__</span><span class="ansi-blue-fg">(self, image_id, category_id, bbox, area, iscrowd)</span>

Cell <span class="ansi-green-fg">In[43], line 29</span>, in <span class="ansi-cyan-fg">SingleCOCOAnnotation.__post_init__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-fg ansi-bold">     27</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span> <span style="color:rgb(0,0,255)">__post_init__</span>(<span style="color:rgb(0,135,0)">self</span>):
<span class="ansi-green-fg ansi-bold">     28</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">len</span>(<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>bbox) <span style="color:rgb(98,98,98)">!=</span> <span style="color:rgb(98,98,98)">4</span>:
<span class="ansi-green-fg">---&gt; 29</span>         <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">ValueError</span>(<span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">bbox must contain exactly 4 values, current length: </span><span style="font-weight:bold;color:rgb(175,95,135)">{</span><span style="color:rgb(0,135,0)">len</span>(<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>bbox)<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">"</span>)

<span class="ansi-red-fg">ValueError</span>: bbox must contain exactly 4 values, current length: 3</pre>
</div>
</div>
</div>
<p>And now if we pass the correct number of values to our <code>SingleCOCOAnnotation</code>, it should work.</p>
<div id="cell-66" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>SingleCOCOAnnotation(image_id<span class="op">=</span><span class="dv">42</span>, </span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>                     category_id<span class="op">=</span><span class="dv">0</span>, </span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>                     bbox<span class="op">=</span>[<span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>SingleCOCOAnnotation(image_id=42, category_id=0, bbox=[100, 100, 100, 100], area=0.0, iscrowd=0)</code></pre>
</div>
</div>
</section>
<section id="creating-a-function-to-format-our-annotations-as-coco-format" class="level3" data-number="6.6">
<h3 data-number="6.6" class="anchored" data-anchor-id="creating-a-function-to-format-our-annotations-as-coco-format"><span class="header-section-number">6.6</span> Creating a function to format our annotations as COCO format</h3>
<p>Now weâ€™ve got the COCO data format in our <code>SingleCOCOAnnotation</code> and <code>ImageCOCOAnnotation</code> dataclasses, letâ€™s write a function to take our existing image annotations and format them in COCO style.</p>
<p>Our <code>format_image_annotations_as_coco</code> function will:</p>
<ol type="1">
<li>Take in an <code>image_id</code> to represent a unique identifier for the image as well as lists of category integers, area values and bounding box coordinates.</li>
<li>Perform a list comprehension on a zipped version of each category, area and bounding box coordinate value in the input lists creating an instance of <code>SingleCOCOAnnotation</code> as a dictionary (using the <code>asdict</code> method) each time, this will give us a list of <code>SingleCOCOAnnotation</code> formatted dictionaries.</li>
<li>Return a dictionary version of <code>ImageCOCOAnnotations</code> using <code>asdict</code> passing it the <code>image_id</code> as well as list of <code>SingleCOCOAnnotation</code> dictionaries from 2.</li>
</ol>
<p>Why does our function take in lists of categories, areas and bounding boxes?</p>
<p>Because thatâ€™s the current format our existing annotations are in (how we downloaded them from Hugging Face).</p>
<p>Letâ€™s do it!</p>
<div id="cell-68" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Take in a unique image_id as well as lists of categories, areas, and bounding boxes</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_image_annotations_as_coco(</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>        image_id: <span class="bu">int</span>,</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>        categories: List[<span class="bu">int</span>],</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>        areas: List[<span class="bu">float</span>],</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>        bboxes: List[Tuple[<span class="bu">float</span>, <span class="bu">float</span>, <span class="bu">float</span>, <span class="bu">float</span>]] <span class="co"># bboxes in XYWH format ([x_top_left, y_top_left, width, height])</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">dict</span>:</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Formats lists of image annotations into COCO format.</span></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Takes in parallel lists of categories, areas, and bounding boxes and</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="co">    then formats them into a COCO-style dictionary of annotations.</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a><span class="co">        image_id: Unique integer identifier for an image.</span></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a><span class="co">        categories: List of integer category IDs for each annotation.</span></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a><span class="co">        areas: List of float areas for each annotation.</span></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a><span class="co">        bboxes: List of tuples containing bounding box coordinates in XYWH format </span></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a><span class="co">            ([x_top_left, y_top_left, width, height]).</span></span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a><span class="co">        A dictionary of image annotations in COCO format with the following structure:</span></span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a><span class="co">        {</span></span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a><span class="co">            "image_id": int,</span></span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a><span class="co">            "annotations": [</span></span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a><span class="co">                {</span></span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a><span class="co">                    "image_id": int,</span></span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a><span class="co">                    "category_id": int,</span></span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a><span class="co">                    "bbox": List[float],</span></span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a><span class="co">                    "area": float</span></span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a><span class="co">                },</span></span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a><span class="co">                ...more annotations here</span></span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a><span class="co">            ]</span></span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a><span class="co">        }</span></span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a><span class="co">    Note:</span></span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a><span class="co">        All input lists much be the same length and in the same order.</span></span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a><span class="co">        Otherwise, there will be mismatched annotations.</span></span>
<span id="cb48-38"><a href="#cb48-38" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb48-39"><a href="#cb48-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-40"><a href="#cb48-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Turn input lists into a list of dicts in SingleCOCOAnnotation format</span></span>
<span id="cb48-41"><a href="#cb48-41" aria-hidden="true" tabindex="-1"></a>    coco_format_annotations <span class="op">=</span> [</span>
<span id="cb48-42"><a href="#cb48-42" aria-hidden="true" tabindex="-1"></a>        asdict(SingleCOCOAnnotation(</span>
<span id="cb48-43"><a href="#cb48-43" aria-hidden="true" tabindex="-1"></a>            image_id<span class="op">=</span>image_id,</span>
<span id="cb48-44"><a href="#cb48-44" aria-hidden="true" tabindex="-1"></a>            category_id<span class="op">=</span>category,</span>
<span id="cb48-45"><a href="#cb48-45" aria-hidden="true" tabindex="-1"></a>            bbox<span class="op">=</span><span class="bu">list</span>(bbox),</span>
<span id="cb48-46"><a href="#cb48-46" aria-hidden="true" tabindex="-1"></a>            area<span class="op">=</span>area,</span>
<span id="cb48-47"><a href="#cb48-47" aria-hidden="true" tabindex="-1"></a>        ))</span>
<span id="cb48-48"><a href="#cb48-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> category, area, bbox <span class="kw">in</span> <span class="bu">zip</span>(categories, areas, bboxes)</span>
<span id="cb48-49"><a href="#cb48-49" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb48-50"><a href="#cb48-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-51"><a href="#cb48-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Return a of annotations with format {"image_id": ..., "annotations": [...]} (required COCO format)</span></span>
<span id="cb48-52"><a href="#cb48-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> asdict(ImageCOCOAnnotations(image_id<span class="op">=</span>image_id,</span>
<span id="cb48-53"><a href="#cb48-53" aria-hidden="true" tabindex="-1"></a>                                       annotations<span class="op">=</span>coco_format_annotations))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nice!</p>
<p>Having those pre-built dataclasses makes everything else fall into place.</p>
<p>Now letâ€™s try our <code>format_image_annotations_as_coco</code> function on our <code>random_sample</code> from before.</p>
<p>First, weâ€™ll remind ourselves what our <code>random_sample</code> looks like.</p>
<div id="cell-70" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inpsect our random sample (in original format)</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>random_sample</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,
 'image_id': 782,
 'annotations': {'file_name': ['46cd4c32-3832-4a8d-81ed-b04abb17f41c.jpeg',
   '46cd4c32-3832-4a8d-81ed-b04abb17f41c.jpeg',
   '46cd4c32-3832-4a8d-81ed-b04abb17f41c.jpeg'],
  'image_id': [782, 782, 782],
  'category_id': [0, 5, 1],
  'bbox': [[93.30000305175781,
    407.20001220703125,
    776.4000244140625,
    670.2999877929688],
   [376.8999938964844,
    716.5999755859375,
    194.10000610351562,
    219.89999389648438],
   [498.29998779296875, 663.5, 303.20001220703125, 247.1999969482422]],
  'iscrowd': [0, 0, 0],
  'area': [520420.90625, 42682.58984375, 74951.0390625]},
 'label_source': 'manual_prodigy_label',
 'image_source': 'manual_taken_photo'}</code></pre>
</div>
</div>
<p>Ok wonderful, looks like we can extract the <code>image_id</code>, <code>category_id</code> <code>bbox</code> and <code>area</code> fields from our <code>random_sample</code> to get the required inputs to our <code>format_image_annotations_as_coco</code> function.</p>
<p>Letâ€™s try it out.</p>
<div id="cell-72" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract image_id, categories, areas, and bboxes from the random sample</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>random_sample_image_id <span class="op">=</span> random_sample[<span class="st">"image_id"</span>]</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>random_sample_categories <span class="op">=</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"category_id"</span>]</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>random_sample_areas <span class="op">=</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"area"</span>]</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>random_sample_bboxes <span class="op">=</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"bbox"</span>]</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Format the random sample annotations as COCO format</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>random_sample_coco_annotations <span class="op">=</span> format_image_annotations_as_coco(image_id<span class="op">=</span>random_sample_image_id,</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>                                                                  categories<span class="op">=</span>random_sample_categories,</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>                                                                  areas<span class="op">=</span>random_sample_areas,</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>                                                                  bboxes<span class="op">=</span>random_sample_bboxes)</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>random_sample_coco_annotations</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>{'image_id': 782,
 'annotations': [{'image_id': 782,
   'category_id': 0,
   'bbox': [93.30000305175781,
    407.20001220703125,
    776.4000244140625,
    670.2999877929688],
   'area': 520420.90625,
   'iscrowd': 0},
  {'image_id': 782,
   'category_id': 5,
   'bbox': [376.8999938964844,
    716.5999755859375,
    194.10000610351562,
    219.89999389648438],
   'area': 42682.58984375,
   'iscrowd': 0},
  {'image_id': 782,
   'category_id': 1,
   'bbox': [498.29998779296875, 663.5, 303.20001220703125, 247.1999969482422],
   'area': 74951.0390625,
   'iscrowd': 0}]}</code></pre>
</div>
</div>
<p>Woohoo!</p>
<p>Looks like we just fixed our <code>ValueError</code> from before:</p>
<blockquote class="blockquote">
<p>ValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: <code>image_id</code> and <code>annotations</code>, with the latter being a list of annotations in the COCO format.</p>
</blockquote>
<p>Our COCO formatted annotations have the <code>image_id</code> and <code>annotations</code> keys and our <code>annotations</code> are a list of annotations in COCO format.</p>
<p>Perfect!</p>
</section>
<section id="tk---preprocess-a-single-image-and-set-of-coco-format-annotations" class="level3" data-number="6.7">
<h3 data-number="6.7" class="anchored" data-anchor-id="tk---preprocess-a-single-image-and-set-of-coco-format-annotations"><span class="header-section-number">6.7</span> TK - Preprocess a single image and set of COCO format annotations</h3>
<p>Now weâ€™ve preprocessed our annotations to be in COCO format, we can use them with <a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess"><code>image_processor.preprocess</code></a>.</p>
<p>Letâ€™s pass our <code>random_sample</code> image and COCO formatted annotations to the <code>preprocess</code> method.</p>
<div id="cell-75" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess random sample image and assosciated annotations</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>random_sample_preprocessed <span class="op">=</span> image_processor.preprocess(images<span class="op">=</span>random_sample[<span class="st">"image"</span>],</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>                                                        annotations<span class="op">=</span>random_sample_coco_annotations,</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>                                                        return_tensors<span class="op">=</span><span class="st">"pt"</span> <span class="co"># can return as tensors or not, "pt" returns as PyTorch tensors</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>                                                        ) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>TK - Note: You may see a warning of</p>
<blockquote class="blockquote">
<p>The <code>max_size</code> parameter is deprecated and will be removed in v4.26. Please specify in <code>size['longest_edge'] instead</code>.</p>
</blockquote>
<p>If you are not using the <code>max_size</code> parameter and are using a version of <code>transformers</code> &gt; 4.26, you can ignore this.</p>
<div id="cell-77" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Disable warnings about `max_size` parameter being deprecated (this is okay)</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>, message<span class="op">=</span><span class="st">"The `max_size` parameter is deprecated*"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Excellent!</p>
<p>It looks like the <code>preprocess</code> method worked on our single sample.</p>
<p>Letâ€™s inspect the <code>keys()</code> in our <code>random_sample_preprocessed</code>.</p>
<div id="cell-79" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the keys of our preprocessed example</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>random_sample_preprocessed.keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>dict_keys(['pixel_values', 'pixel_mask', 'labels'])</code></pre>
</div>
</div>
<p>Wonderful, we get a preprocessed image and labels:</p>
<ul>
<li><code>pixel_values</code> = preprocessed pixels (the preprocessed image).</li>
<li><code>pixel_mask</code> = whether or not to mask the pixels (e.g.&nbsp;0 = mask, 1 = no mask, in our case, all values will be <code>1</code> since we want the model to see all pixels).</li>
<li><code>labels</code> = preprocessed labels (the preprocessed annotations).</li>
</ul>
<div id="cell-81" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect preprocessed image shape</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Preprocessed image shape: </span><span class="sc">{</span>random_sample_preprocessed[<span class="st">'pixel_values'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch_size, colour_channels, height, width]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Preprocessed image shape: torch.Size([1, 3, 640, 480]) -&gt; [batch_size, colour_channels, height, width]</code></pre>
</div>
</div>
<p>Since we only passed a single sample to <code>preprocess</code>, we get back a batch size of 1.</p>
<p>Now how do our labels look?</p>
<div id="cell-83" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>pprint(random_sample_preprocessed[<span class="st">"labels"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[{'area': tensor([130105.2266,  10670.6475,  18737.7598]),
  'boxes': tensor([[0.5016, 0.5800, 0.8088, 0.5237],
        [0.4937, 0.6457, 0.2022, 0.1718],
        [0.6770, 0.6149, 0.3158, 0.1931]]),
  'class_labels': tensor([0, 5, 1]),
  'image_id': tensor([782]),
  'iscrowd': tensor([0, 0, 0]),
  'orig_size': tensor([1280,  960]),
  'size': tensor([640, 480])}]</code></pre>
</div>
</div>
<p>UPTOHERE - breaking down whatâ€™s the output of the above cell to showcase whatâ€™s the in the formatted labels</p>
<p>TK - break it down whatâ€™s in the labels</p>
<ul>
<li><code>size</code> = image size in format [height, width]</li>
<li><code>image_id</code> = ID of image passed in</li>
<li><code>class_labels</code> = list of labels assosciated with image e.g.&nbsp;<code>tensor([5, 1, 0, 0, 4])</code> -&gt; <code>{0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash'}</code></li>
<li><code>boxes</code> = list of boxes with coordinates for where the box is on the image in format <code>CXCYWH</code> (normalized)</li>
</ul>
<div id="cell-85" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>random_sample_preprocessed[<span class="st">"labels"</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>{'size': tensor([640, 480]), 'image_id': tensor([460]), 'class_labels': tensor([], dtype=torch.int64), 'boxes': tensor([], size=(0, 4)), 'area': tensor([]), 'iscrowd': tensor([], dtype=torch.int64), 'orig_size': tensor([1280,  960])}</code></pre>
</div>
</div>
</section>
<section id="tk---creating-a-function-to-build-our-model" class="level3" data-number="6.8">
<h3 data-number="6.8" class="anchored" data-anchor-id="tk---creating-a-function-to-build-our-model"><span class="header-section-number">6.8</span> TK - Creating a function to build our model</h3>
<p>We may want to make multiple instances of our modelâ€¦ so letâ€™s functionize the instantiation of a new model</p>
<div id="cell-87" class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup the model</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model():</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Docstring here"""</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> AutoModelForObjectDetection.from_pretrained(</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>        pretrained_model_name_or_path<span class="op">=</span>MODEL_NAME,</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>        label2id<span class="op">=</span>label2id,</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>        id2label<span class="op">=</span>id2label,</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>        ignore_mismatched_sizes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>        backbone<span class="op">=</span><span class="st">"resnet50"</span>,</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_model()</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of ConditionalDetrForObjectDetection were not initialized from the model checkpoint at microsoft/conditional-detr-resnet-50 and are newly initialized because the shapes did not match:
- class_labels_classifier.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([7]) in the model instantiated
- class_labels_classifier.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>ConditionalDetrForObjectDetection(
  (model): ConditionalDetrModel(
    (backbone): ConditionalDetrConvModel(
      (conv_encoder): ConditionalDetrConvEncoder(
        (model): FeatureListNet(
          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
          (bn1): ConditionalDetrFrozenBatchNorm2d()
          (act1): ReLU(inplace=True)
          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
          (layer1): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): ConditionalDetrFrozenBatchNorm2d()
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
          )
          (layer2): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): ConditionalDetrFrozenBatchNorm2d()
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
          )
          (layer3): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): ConditionalDetrFrozenBatchNorm2d()
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (4): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (5): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
          )
          (layer4): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): ConditionalDetrFrozenBatchNorm2d()
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
          )
        )
      )
      (position_embedding): ConditionalDetrSinePositionEmbedding()
    )
    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (query_position_embeddings): Embedding(300, 256)
    (encoder): ConditionalDetrEncoder(
      (layers): ModuleList(
        (0-5): 6 x ConditionalDetrEncoderLayer(
          (self_attn): DetrAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (activation_fn): ReLU()
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (decoder): ConditionalDetrDecoder(
      (layers): ModuleList(
        (0): ConditionalDetrDecoderLayer(
          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)
          (self_attn): ConditionalDetrAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_qpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)
          (encoder_attn): ConditionalDetrAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1-5): 5 x ConditionalDetrDecoderLayer(
          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)
          (self_attn): ConditionalDetrAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_qpos_proj): None
          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)
          (encoder_attn): ConditionalDetrAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_scale): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ref_point_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
  )
  (class_labels_classifier): Linear(in_features=256, out_features=7, bias=True)
  (bbox_predictor): ConditionalDetrMLPPredictionHead(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
)</code></pre>
</div>
</div>
<p>TK - callout: many models on Hugging Face follow this pattern of â€œprocessorâ€ -&gt; â€œmodelâ€</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> TK - This may output some information about the model not being prepared for a custom dataset due to it originally being prepared for a certain number of classes (e.g.&nbsp;the model can only recognize what it was trained on). Weâ€™ve initialized it with an output head to have 4</p>
</blockquote>
<div id="cell-89" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>model.forward?</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Signature:
model.forward(
    pixel_values: torch.FloatTensor,
    pixel_mask: Optional[torch.LongTensor] = None,
    decoder_attention_mask: Optional[torch.LongTensor] = None,
    encoder_outputs: Optional[torch.FloatTensor] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
    labels: Optional[List[dict]] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -&gt; Union[Tuple[torch.FloatTensor], transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrObjectDetectionOutput]
Docstring:
The [`ConditionalDetrForObjectDetection`] forward method, overrides the `__call__` special method.

&lt;Tip&gt;

Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.

&lt;/Tip&gt;

Args:
    pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):
        Pixel values. Padding will be ignored by default should you provide it.

        Pixel values can be obtained using [`AutoImageProcessor`]. See [`ConditionalDetrImageProcessor.__call__`]
        for details.

    pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):
        Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:

        - 1 for pixels that are real (i.e. **not masked**),
        - 0 for pixels that are padding (i.e. **masked**).

        [What are attention masks?](../glossary#attention-mask)

    decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):
        Not used by default. Can be used to mask object queries.
    encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
        Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
        `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
        hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
        Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you
        can choose to directly pass a flattened representation of an image.
    decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):
        Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an
        embedded representation.
    output_attentions (`bool`, *optional*):
        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
        tensors for more detail.
    output_hidden_states (`bool`, *optional*):
        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
        more detail.
    return_dict (`bool`, *optional*):
        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.

    labels (`List[Dict]` of len `(batch_size,)`, *optional*):
        Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the
        following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch
        respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes
        in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.


    Returns:
        [`transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrObjectDetectionOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrObjectDetectionOutput`] or a tuple of
        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various
        elements depending on the configuration ([`ConditionalDetrConfig`]) and inputs.

        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)) -- Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a
          bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized
          scale-invariant IoU loss.
        - **loss_dict** (`Dict`, *optional*) -- A dictionary containing the individual losses. Useful for logging.
        - **logits** (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`) -- Classification logits (including no-object) for all queries.
        - **pred_boxes** (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`) -- Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
          values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding
          possible padding). You can use [`~ConditionalDetrImageProcessor.post_process_object_detection`] to retrieve the
          unnormalized bounding boxes.
        - **auxiliary_outputs** (`list[Dict]`, *optional*) -- Optional, only returned when auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)
          and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and
          `pred_boxes`) for each decoder layer.
        - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) -- Sequence of hidden-states at the output of the last layer of the decoder of the model.
        - **decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of
          shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the decoder at the output of each
          layer plus the initial embedding outputs.
        - **decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
          sequence_length)`. Attentions weights of the decoder, after the attention softmax, used to compute the
          weighted average in the self-attention heads.
        - **cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
          sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,
          used to compute the weighted average in the cross-attention heads.
        - **encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) -- Sequence of hidden-states at the output of the last layer of the encoder of the model.
        - **encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of
          shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each
          layer plus the initial embedding outputs.
        - **encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
          sequence_length)`. Attentions weights of the encoder, after the attention softmax, used to compute the
          weighted average in the self-attention heads.
  

    Examples:

    ```python
    &gt;&gt;&gt; from transformers import AutoImageProcessor, AutoModelForObjectDetection
    &gt;&gt;&gt; from PIL import Image
    &gt;&gt;&gt; import requests

    &gt;&gt;&gt; url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    &gt;&gt;&gt; image = Image.open(requests.get(url, stream=True).raw)

    &gt;&gt;&gt; image_processor = AutoImageProcessor.from_pretrained("microsoft/conditional-detr-resnet-50")
    &gt;&gt;&gt; model = AutoModelForObjectDetection.from_pretrained("microsoft/conditional-detr-resnet-50")

    &gt;&gt;&gt; inputs = image_processor(images=image, return_tensors="pt")

    &gt;&gt;&gt; outputs = model(**inputs)

    &gt;&gt;&gt; # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
    &gt;&gt;&gt; target_sizes = torch.tensor([image.size[::-1]])
    &gt;&gt;&gt; results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[
    ...     0
    ... ]
    &gt;&gt;&gt; for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    ...     box = [round(i, 2) for i in box.tolist()]
    ...     print(
    ...         f"Detected {model.config.id2label[label.item()]} with confidence "
    ...         f"{round(score.item(), 3)} at location {box}"
    ...     )
    Detected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]
    Detected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]
    Detected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]
    Detected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]
    Detected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]
    ```
File:      ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/conditional_detr/modeling_conditional_detr.py
Type:      method</code></pre>
</div>
</div>
<div id="cell-90" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>random_sample_preprocessed[<span class="st">"pixel_values"</span>][<span class="dv">0</span>].shape <span class="co"># [color_channels, height, width]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>torch.Size([3, 640, 480])</code></pre>
</div>
</div>
<div id="cell-91" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Do a single forward pass with the model</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>random_sample_outputs <span class="op">=</span> model(pixel_values<span class="op">=</span>random_sample_preprocessed[<span class="st">"pixel_values"</span>][<span class="dv">0</span>].unsqueeze(<span class="dv">0</span>), <span class="co"># model expects input [batch_size, color_channels, height, width]</span></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>                              pixel_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>random_sample_outputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>ConditionalDetrObjectDetectionOutput(loss=None, loss_dict=None, logits=tensor([[[ 0.0454, -0.0711, -0.4182,  ...,  0.2894,  0.0483,  0.0123],
         [-0.1012, -0.1597, -0.1998,  ..., -0.0486, -0.1782, -0.2652],
         [ 0.1434,  0.0662, -0.1789,  ...,  0.0542, -0.0454, -0.0935],
         ...,
         [-0.3237, -0.4062, -0.1989,  ...,  0.2875, -0.0910,  0.2941],
         [ 0.1114, -0.0177, -0.3141,  ..., -0.0593, -0.1495, -0.1393],
         [-0.1669, -0.1889,  0.1891,  ...,  0.1096, -0.2838, -0.0589]]],
       grad_fn=&lt;ViewBackward0&gt;), pred_boxes=tensor([[[0.8267, 0.6865, 0.3329, 0.6065],
         [0.6527, 0.1801, 0.0381, 0.0135],
         [0.8987, 0.5712, 0.2006, 0.2254],
         ...,
         [0.3474, 0.3090, 0.6915, 0.1174],
         [0.8373, 0.5285, 0.3022, 0.1941],
         [0.0810, 0.2927, 0.1605, 0.0432]]], grad_fn=&lt;SigmoidBackward0&gt;), auxiliary_outputs=None, last_hidden_state=tensor([[[ 0.2234,  0.0444,  0.9698,  ..., -1.0443, -0.1137,  0.3582],
         [ 0.2838, -0.6804,  0.3960,  ...,  0.7212,  0.3551,  0.3658],
         [ 0.5051, -0.0147,  0.5885,  ..., -1.2090, -0.0941, -0.0717],
         ...,
         [ 0.4280, -1.5612,  0.3054,  ..., -0.8336,  0.0790, -0.3486],
         [ 0.2858, -0.0132,  0.5693,  ..., -1.1525, -0.1821, -0.1940],
         [ 0.2017,  0.1479, -0.3311,  ..., -1.1814, -0.0651, -0.0979]]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.3918,  0.4741, -0.3829,  ..., -0.5659,  0.4583,  0.3095],
         [ 0.1083,  0.5762, -0.0826,  ...,  0.2379,  0.1619,  0.3629],
         [ 0.1359,  0.6453, -0.1079,  ..., -0.1028,  0.1878,  0.3184],
         ...,
         [ 0.1694,  0.8391, -0.1381,  ...,  0.1942,  0.0713,  0.2323],
         [ 0.1709,  0.6931, -0.0919,  ...,  0.2428,  0.0508,  0.1932],
         [-0.1842,  0.4742, -0.1434,  ..., -0.1434,  0.2518,  0.2516]]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;), encoder_hidden_states=None, encoder_attentions=None)</code></pre>
</div>
</div>
<div id="cell-92" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the keys of the output</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>random_sample_outputs.keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>odict_keys(['logits', 'pred_boxes', 'last_hidden_state', 'encoder_last_hidden_state'])</code></pre>
</div>
</div>
<div id="cell-93" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We get 300 total boxes with shape the same as our number of labels</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>random_sample_outputs.logits.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>torch.Size([1, 300, 7])</code></pre>
</div>
</div>
<div id="cell-94" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>random_sample_outputs.pred_boxes.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>torch.Size([1, 300, 4])</code></pre>
</div>
</div>
<p>TK - note: see <code>forward()</code> method for output format of boxes -&gt; https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward</p>
<p>From the docs:</p>
<blockquote class="blockquote">
<p>Returns â€¦ pred_boxes (torch.FloatTensor of shape (batch_size, num_queries, 4)) â€” Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding). You can use post_process_object_detection() to retrieve the unnormalized bounding boxes.</p>
</blockquote>
<div id="cell-96" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example pred box output</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Box output comes in the form CXCYWH normalized (e.g. [center_X, center_Y, width, height]) to be between 0 and 1, this is in the docs</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>random_sample_outputs.pred_boxes[:, <span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>tensor([[0.8267, 0.6865, 0.3329, 0.6065]], grad_fn=&lt;SelectBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-97" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model outputs one logit per category value (e.g. 6 categories = 6 logits)</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(categories.names)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>7</code></pre>
</div>
</div>
<div id="cell-98" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For example, one value for each of the following:</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>label2id</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>{'bin': 0,
 'hand': 1,
 'not_bin': 2,
 'not_hand': 3,
 'not_trash': 4,
 'trash': 5,
 'trash_arm': 6}</code></pre>
</div>
</div>
<div id="cell-99" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>random_sample</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,
 'image_id': 384,
 'annotations': {'file_name': ['3e85a851-513d-40b8-8b16-240b365132d8.jpeg',
   '3e85a851-513d-40b8-8b16-240b365132d8.jpeg',
   '3e85a851-513d-40b8-8b16-240b365132d8.jpeg',
   '3e85a851-513d-40b8-8b16-240b365132d8.jpeg'],
  'image_id': [384, 384, 384, 384],
  'category_id': [5, 1, 0, 0],
  'bbox': [[452.70001220703125,
    485.3999938964844,
    265.29998779296875,
    174.1999969482422],
   [625.5, 459.5, 180.1999969482422, 238.10000610351562],
   [221.3000030517578,
    371.8999938964844,
    447.8999938964844,
    496.3999938964844],
   [7.699999809265137, 328.0, 301.3999938964844, 440.5]],
  'iscrowd': [0, 0, 0, 0],
  'area': [46215.26171875, 42905.62109375, 222337.5625, 132766.703125]},
 'label_source': 'manual_prodigy_label',
 'image_source': 'manual_taken_photo'}</code></pre>
</div>
</div>
</section>
<section id="post-process-a-single-output" class="level3" data-number="6.9">
<h3 data-number="6.9" class="anchored" data-anchor-id="post-process-a-single-output"><span class="header-section-number">6.9</span> Post process a single output</h3>
<p>Always a good step to get your model working end-to-end on a single sample and then upgrading it.</p>
<p>Box formats:</p>
<ul>
<li>Starting data (the input data) -&gt; [x_top_left, y_top_left, width, height] -&gt; <code>XYWH</code> (absolute)</li>
<li>Out of <code>image_processor.preprocess()</code> -&gt; [center_x, center_y, width, height] -&gt; <code>CXCYWH</code> (normalized) -&gt; into model
<ul>
<li>See docs: https://huggingface.co/docs/transformers.js/en/custom_usage</li>
</ul></li>
<li>Out of <code>model</code> -&gt; [center_x, center_y, width, height] -&gt; <code>CXCYWH</code> (normalized)
<ul>
<li>See docs for <code>forward()</code> and output <code>pred_boxes</code>: https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward</li>
</ul></li>
<li>Out of <code>image_processor.post_process_object_detection()</code> -&gt; [x_top_left, y_top_left, x_bottom_right, y_bottom_right] -&gt; <code>XYXY</code>
<ul>
<li>This is PASCL VOC format - (xmin, ymin, xmax, ymax)</li>
<li>See docs: https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.post_process_object_detection</li>
</ul></li>
</ul>
<div id="cell-101" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the keys of the labels for the image</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>random_sample_preprocessed[<span class="st">"labels"</span>][<span class="dv">0</span>].keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>dict_keys(['size', 'image_id', 'class_labels', 'boxes', 'area', 'iscrowd', 'orig_size'])</code></pre>
</div>
</div>
<div id="cell-102" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Image original size: </span><span class="sc">{</span>random_sample_preprocessed<span class="sc">.</span>labels[<span class="dv">0</span>]<span class="sc">.</span>orig_size<span class="sc">}</span><span class="ss"> (height, width)"</span>)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Image size after preprocessing: </span><span class="sc">{</span>random_sample_preprocessed<span class="sc">.</span>labels[<span class="dv">0</span>]<span class="sc">.</span>size<span class="sc">}</span><span class="ss"> (height, width)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Image original size: tensor([1280,  960]) (height, width)
[INFO] Image size after preprocessing: tensor([640, 480]) (height, width)</code></pre>
</div>
</div>
<div id="cell-103" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Output logits will be post-processed to turn into prediction probabilities as well as boxes</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Get pred probs from logits, this will be used for our threshold parameter in post_process_object_detection </span></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>torch.softmax(random_sample_outputs.logits, dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>tensor([[[0.1471, 0.1309, 0.0925,  ..., 0.1878, 0.1475, 0.1423],
         [0.1330, 0.1255, 0.1205,  ..., 0.1402, 0.1232, 0.1129],
         [0.1611, 0.1492, 0.1167,  ..., 0.1474, 0.1334, 0.1272],
         ...,
         [0.0988, 0.0910, 0.1119,  ..., 0.1821, 0.1247, 0.1833],
         [0.1683, 0.1479, 0.1100,  ..., 0.1419, 0.1297, 0.1310],
         [0.1238, 0.1211, 0.1767,  ..., 0.1632, 0.1101, 0.1379]]],
       grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-104" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>random_sample_outputs_post_processed <span class="op">=</span> image_processor.post_process_object_detection(</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>random_sample_outputs,</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span><span class="fl">0.3</span>, <span class="co"># prediction probability threshold for boxes (note: boxes from an untrained model will likely be bad)</span></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>    target_sizes<span class="op">=</span>[random_sample_preprocessed[<span class="st">"labels"</span>][<span class="dv">0</span>][<span class="st">"orig_size"</span>]] <span class="co"># original input image size (or whichever target size you'd like), required to be same number of input items in a list</span></span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>random_sample_outputs_post_processed</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>[{'scores': tensor([0.6839, 0.6737, 0.6616, 0.6614, 0.6574, 0.6541, 0.6478, 0.6476, 0.6475,
          0.6475, 0.6472, 0.6472, 0.6448, 0.6444, 0.6436, 0.6434, 0.6426, 0.6419,
          0.6416, 0.6408, 0.6404, 0.6383, 0.6382, 0.6374, 0.6372, 0.6359, 0.6354,
          0.6352, 0.6346, 0.6338, 0.6310, 0.6308, 0.6302, 0.6280, 0.6277, 0.6273,
          0.6272, 0.6272, 0.6271, 0.6265, 0.6265, 0.6265, 0.6259, 0.6255, 0.6248,
          0.6243, 0.6242, 0.6241, 0.6237, 0.6229, 0.6223, 0.6221, 0.6215, 0.6213,
          0.6207, 0.6207, 0.6203, 0.6199, 0.6196, 0.6195, 0.6185, 0.6184, 0.6183,
          0.6177, 0.6163, 0.6160, 0.6150, 0.6144, 0.6144, 0.6139, 0.6139, 0.6137,
          0.6135, 0.6129, 0.6125, 0.6124, 0.6108, 0.6106, 0.6104, 0.6101, 0.6100,
          0.6099, 0.6097, 0.6092, 0.6089, 0.6089, 0.6085, 0.6085, 0.6079, 0.6076,
          0.6070, 0.6070, 0.6068, 0.6063, 0.6057, 0.6057, 0.6056, 0.6055, 0.6053,
          0.6051], grad_fn=&lt;IndexBackward0&gt;),
  'labels': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4,
          3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 4, 4, 3, 6, 3, 4, 3, 3, 3, 3,
          3, 3, 3, 3, 3, 3, 4, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 3, 4, 4, 3, 3, 4, 4,
          3, 3, 6, 3]),
  'boxes': tensor([[ 1.5021e+02,  2.2275e+02,  9.2612e+02,  3.1100e+02],
          [ 3.0236e+02,  2.3473e+02,  8.8168e+02,  3.0422e+02],
          [ 5.0880e+02,  2.1115e+02,  5.5853e+02,  2.3331e+02],
          [ 4.7810e+02,  1.9805e+02,  5.2191e+02,  2.2018e+02],
          [ 4.3730e+02,  2.4722e+02,  9.2742e+02,  3.1818e+02],
          [ 9.0321e+02,  3.0775e+02,  9.6117e+02,  3.3864e+02],
          [ 7.1403e+02,  2.3229e+02,  7.5484e+02,  2.4963e+02],
          [ 6.4448e+02,  2.3185e+02,  7.0163e+02,  2.4743e+02],
          [ 9.7196e+01,  2.4972e+02,  8.0122e+02,  3.4492e+02],
          [ 6.9588e+02,  2.2955e+02,  7.2308e+02,  2.4626e+02],
          [ 6.8868e+02,  2.3618e+02,  7.3616e+02,  2.5229e+02],
          [ 6.1711e+02,  2.2698e+02,  6.7091e+02,  2.4342e+02],
          [ 1.1697e+02,  2.0081e+02,  5.3524e+02,  2.5107e+02],
          [ 4.8558e+02,  2.0865e+02,  5.4637e+02,  2.3481e+02],
          [ 7.4454e+02,  2.2994e+02,  7.8603e+02,  2.5308e+02],
          [ 6.8199e+02,  2.3242e+02,  7.2622e+02,  2.4876e+02],
          [ 5.3925e+02,  2.2372e+02,  5.9300e+02,  2.4156e+02],
          [ 4.4184e+02,  2.0462e+02,  4.7448e+02,  2.2387e+02],
          [ 6.6260e+02,  2.3085e+02,  7.0588e+02,  2.4724e+02],
          [ 7.2336e+02,  2.3609e+02,  7.7372e+02,  2.5295e+02],
          [ 5.9377e+02,  2.3010e+02,  6.5768e+02,  2.4654e+02],
          [ 6.5659e+02,  2.3656e+02,  7.1577e+02,  2.5289e+02],
          [ 2.1040e+02,  1.9925e+02,  3.2352e+02,  2.3685e+02],
          [ 1.8045e+02,  1.5578e+02,  3.1142e+02,  1.9401e+02],
          [ 4.2432e+02,  2.0136e+02,  4.5325e+02,  2.1855e+02],
          [ 7.8032e+02,  2.2787e+02,  8.3353e+02,  2.5188e+02],
          [ 6.4112e+02,  2.1902e+02,  6.7090e+02,  2.3924e+02],
          [ 3.7899e+02,  2.4951e+02,  7.8740e+02,  3.0072e+02],
          [ 1.6532e+02,  2.3143e+02,  3.2209e+02,  2.6452e+02],
          [ 1.0642e+02,  2.3275e+02,  2.2752e+02,  2.5950e+02],
          [-2.7386e+00,  2.8250e+02,  9.5569e+02,  8.3506e+02],
          [ 5.6764e+02,  2.2579e+02,  6.0133e+02,  2.4452e+02],
          [ 7.2771e+02,  2.3997e+02,  7.9102e+02,  2.5931e+02],
          [ 1.3404e+02,  4.4818e+02,  9.4260e+02,  8.4707e+02],
          [ 4.5860e+02,  4.7699e+02,  5.8809e+02,  5.3134e+02],
          [ 1.6226e+02,  2.1322e+02,  3.1575e+02,  2.5415e+02],
          [ 5.8245e+02,  2.3491e+02,  7.3097e+02,  2.5595e+02],
          [ 9.5602e+02,  2.3172e+02,  9.5996e+02,  2.8035e+02],
          [ 7.6245e+02,  2.3498e+02,  8.0948e+02,  2.5390e+02],
          [ 8.5605e+02,  3.1974e+02,  9.6036e+02,  3.4395e+02],
          [ 1.4028e+01,  2.9742e+02,  6.6831e+02,  7.8667e+02],
          [ 2.1273e+00,  8.9889e+02,  9.6114e+02,  1.2628e+03],
          [ 3.9504e+02,  2.1397e+02,  4.3043e+02,  2.2851e+02],
          [ 3.4446e-01,  1.8749e+02,  7.8433e+01,  2.2499e+02],
          [ 6.7294e+02,  2.5800e+02,  7.6941e+02,  2.8344e+02],
          [ 6.0833e+02,  2.2191e+02,  6.4493e+02,  2.3915e+02],
          [-4.1992e+00,  3.3792e+02,  9.5432e+02,  1.1725e+03],
          [ 3.5863e+02,  2.0646e+02,  3.8990e+02,  2.3022e+02],
          [ 4.6378e+02,  2.2039e+02,  5.2133e+02,  2.3814e+02],
          [-5.5702e+00,  2.8453e+02,  8.7009e+02,  7.9807e+02],
          [ 7.9385e+02,  2.3517e+02,  8.4780e+02,  2.5737e+02],
          [ 8.8628e+02,  2.7585e+02,  9.6165e+02,  3.1819e+02],
          [-1.4085e+01,  7.4535e+02,  9.4306e+02,  1.2644e+03],
          [ 7.6982e-01,  4.5878e+02,  9.5951e+02,  8.9642e+02],
          [ 3.8868e+02,  1.9604e+02,  4.2339e+02,  2.1367e+02],
          [ 1.0205e+02,  2.3049e+02,  1.9162e+02,  2.5180e+02],
          [ 1.5999e+02,  1.4399e+02,  2.6803e+02,  1.9518e+02],
          [ 8.3596e+02,  2.8685e+02,  9.6574e+02,  3.4110e+02],
          [ 4.0079e+02,  2.0236e+02,  4.3893e+02,  2.2204e+02],
          [ 2.2829e-02,  1.7629e+02,  7.5778e+00,  2.1321e+02],
          [ 4.0679e+02,  2.0669e+02,  4.4338e+02,  2.2396e+02],
          [-1.5052e+00,  3.2000e+02,  6.8636e+02,  8.1026e+02],
          [-1.0880e+01,  4.2899e+02,  9.4746e+02,  8.7154e+02],
          [ 8.5131e+00,  5.3675e+02,  6.7816e+02,  8.2705e+02],
          [ 5.8866e+02,  2.2367e+02,  6.3574e+02,  2.4067e+02],
          [ 1.4028e+01,  2.9742e+02,  6.6831e+02,  7.8667e+02],
          [ 7.6864e+02,  3.0461e+02,  8.5804e+02,  3.3697e+02],
          [ 3.2264e+02,  7.8797e+02,  3.9313e+02,  8.3845e+02],
          [ 1.2273e+02,  3.3624e+02,  2.9674e+02,  3.8669e+02],
          [ 4.9169e+02,  6.0451e+02,  5.7784e+02,  6.6474e+02],
          [ 7.3638e+01,  1.6488e+02,  1.4301e+02,  2.1829e+02],
          [ 8.8692e+02,  2.2889e+02,  9.5606e+02,  2.5846e+02],
          [ 2.5381e+02,  1.2226e+03,  9.6300e+02,  1.2849e+03],
          [-1.0880e+01,  4.2899e+02,  9.4746e+02,  8.7154e+02],
          [-2.6988e+00,  2.1334e+02,  5.3616e+01,  2.4198e+02],
          [ 8.3390e+02,  2.2830e+02,  8.8685e+02,  2.5244e+02],
          [ 9.3645e+00,  2.3032e+02,  1.1699e+02,  2.6169e+02],
          [ 5.2715e+02,  2.8053e+02,  5.8582e+02,  3.0946e+02],
          [ 4.9539e+02,  2.8255e+02,  5.5250e+02,  3.0303e+02],
          [ 8.7392e+02,  3.0168e+02,  9.6059e+02,  3.3615e+02],
          [ 2.7833e+02,  4.6456e+02,  9.4896e+02,  9.1551e+02],
          [ 4.8947e+02,  4.1488e+02,  6.1218e+02,  4.9252e+02],
          [ 7.5197e+02,  2.5210e+02,  8.2422e+02,  2.7998e+02],
          [ 2.5285e+02,  4.3588e+02,  9.3822e+02,  8.7275e+02],
          [ 8.7054e+02,  2.2011e+02,  9.3370e+02,  2.4845e+02],
          [ 9.0750e+02,  3.1742e+02,  9.6316e+02,  3.5016e+02],
          [-4.3446e-01,  2.1810e+02,  2.3509e+01,  2.4937e+02],
          [ 1.1742e+02,  2.1277e+02,  3.1611e+02,  2.5940e+02],
          [ 9.6207e+01,  3.5631e+02,  9.5135e+02,  9.5458e+02],
          [ 2.7833e+02,  4.6456e+02,  9.4896e+02,  9.1551e+02],
          [ 4.0679e+02,  2.0669e+02,  4.4338e+02,  2.2396e+02],
          [-1.1057e+01,  2.5194e+02,  3.7717e+02,  7.1443e+02],
          [ 3.7007e+02,  1.9513e+02,  4.0063e+02,  2.1357e+02],
          [ 4.0936e+02,  5.2738e+02,  6.3709e+02,  6.3685e+02],
          [ 6.5659e+02,  2.3656e+02,  7.1577e+02,  2.5289e+02],
          [ 7.2771e+02,  2.3997e+02,  7.9102e+02,  2.5931e+02],
          [ 2.6593e+02,  1.8549e+02,  3.8638e+02,  2.2928e+02],
          [ 4.9952e+02,  1.2543e+03,  8.9540e+02,  1.2808e+03],
          [-1.1057e+01,  2.5194e+02,  3.7717e+02,  7.1443e+02],
          [-4.4207e+00,  2.8687e+02,  9.5472e+02,  1.0455e+03]],
         grad_fn=&lt;IndexBackward0&gt;)}]</code></pre>
</div>
</div>
<p>TK - letâ€™s visualize, visualize, visualize!</p>
<div id="cell-106" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract scores, labels and boxes</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>random_sample_pred_scores <span class="op">=</span> random_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"scores"</span>]</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>random_sample_pred_labels <span class="op">=</span> random_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"labels"</span>]</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>random_sample_pred_boxes <span class="op">=</span> random_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"boxes"</span>]</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of labels to plot on the boxes </span></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>random_sample_labels_to_plot <span class="op">=</span> [<span class="ss">f"Pred: </span><span class="sc">{</span>id2label[label_pred.item()]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score_pred.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">)"</span> </span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>                  <span class="cf">for</span> label_pred, score_pred <span class="kw">in</span> <span class="bu">zip</span>(random_sample_pred_labels, random_sample_pred_scores)]</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Labels with scores: </span><span class="sc">{</span>random_sample_labels_to_plot<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the random sample image with randomly predicted boxes (these will be very poor since the model is not trained on our data yet)</span></span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>to_pil_image(</span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>random_sample[<span class="st">"image"</span>]),</span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>random_sample_pred_boxes,</span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>random_sample_labels_to_plot,</span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">3</span></span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Labels with scores: ['Pred: not_hand (0.6839)', 'Pred: not_hand (0.6737)', 'Pred: not_hand (0.6616)', 'Pred: not_hand (0.6614)', 'Pred: not_hand (0.6574)', 'Pred: not_hand (0.6541)', 'Pred: not_hand (0.6478)', 'Pred: not_hand (0.6476)', 'Pred: not_hand (0.6475)', 'Pred: not_hand (0.6475)', 'Pred: not_hand (0.6472)', 'Pred: not_hand (0.6472)', 'Pred: not_hand (0.6448)', 'Pred: not_hand (0.6444)', 'Pred: not_hand (0.6436)', 'Pred: not_hand (0.6434)', 'Pred: not_hand (0.6426)', 'Pred: not_hand (0.6419)', 'Pred: not_hand (0.6416)', 'Pred: not_hand (0.6408)', 'Pred: not_hand (0.6404)', 'Pred: not_hand (0.6383)', 'Pred: not_hand (0.6382)', 'Pred: not_hand (0.6374)', 'Pred: not_hand (0.6372)', 'Pred: not_hand (0.6359)', 'Pred: not_hand (0.6354)', 'Pred: not_hand (0.6352)', 'Pred: not_hand (0.6346)', 'Pred: not_hand (0.6338)', 'Pred: not_hand (0.631)', 'Pred: not_hand (0.6308)', 'Pred: not_hand (0.6302)', 'Pred: not_hand (0.628)', 'Pred: not_hand (0.6277)', 'Pred: not_hand (0.6273)', 'Pred: not_hand (0.6272)', 'Pred: not_hand (0.6272)', 'Pred: not_hand (0.6271)', 'Pred: not_hand (0.6265)', 'Pred: not_trash (0.6265)', 'Pred: not_hand (0.6265)', 'Pred: not_trash (0.6259)', 'Pred: not_hand (0.6255)', 'Pred: not_hand (0.6248)', 'Pred: not_hand (0.6243)', 'Pred: not_hand (0.6242)', 'Pred: not_trash (0.6241)', 'Pred: not_hand (0.6237)', 'Pred: not_trash (0.6229)', 'Pred: not_hand (0.6223)', 'Pred: not_hand (0.6221)', 'Pred: not_hand (0.6215)', 'Pred: not_hand (0.6213)', 'Pred: not_hand (0.6207)', 'Pred: not_hand (0.6207)', 'Pred: not_hand (0.6203)', 'Pred: not_hand (0.6199)', 'Pred: not_hand (0.6196)', 'Pred: not_hand (0.6195)', 'Pred: not_hand (0.6185)', 'Pred: trash_arm (0.6184)', 'Pred: not_trash (0.6183)', 'Pred: not_trash (0.6177)', 'Pred: not_hand (0.6163)', 'Pred: trash_arm (0.616)', 'Pred: not_hand (0.615)', 'Pred: not_trash (0.6144)', 'Pred: not_hand (0.6144)', 'Pred: not_hand (0.6139)', 'Pred: not_hand (0.6139)', 'Pred: not_hand (0.6137)', 'Pred: not_hand (0.6135)', 'Pred: not_hand (0.6129)', 'Pred: not_hand (0.6125)', 'Pred: not_hand (0.6124)', 'Pred: not_hand (0.6108)', 'Pred: not_hand (0.6106)', 'Pred: not_trash (0.6104)', 'Pred: not_hand (0.6101)', 'Pred: not_trash (0.61)', 'Pred: not_hand (0.6099)', 'Pred: not_hand (0.6097)', 'Pred: not_trash (0.6092)', 'Pred: not_hand (0.6089)', 'Pred: not_hand (0.6089)', 'Pred: not_hand (0.6085)', 'Pred: not_hand (0.6085)', 'Pred: not_trash (0.6079)', 'Pred: not_hand (0.6076)', 'Pred: not_trash (0.607)', 'Pred: not_trash (0.607)', 'Pred: not_hand (0.6068)', 'Pred: not_hand (0.6063)', 'Pred: not_trash (0.6057)', 'Pred: not_trash (0.6057)', 'Pred: not_hand (0.6056)', 'Pred: not_hand (0.6055)', 'Pred: trash_arm (0.6053)', 'Pred: not_hand (0.6051)']</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="51">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-49-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Our predictions are poor since our model hasnâ€™t been specifically trained on our data.</p>
<p>But we can improve them by fine-tuning the model to our dataset.</p>
</section>
</section>
<section id="tk---fine-tune-the-model-to-our-dataset" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="tk---fine-tune-the-model-to-our-dataset"><span class="header-section-number">7</span> TK - Fine-tune the model to our dataset</h2>
<p>Steps: - preprocess dataset (no augmentation) - get it ready for a model to train on - train model - inspect the results of the trained model</p>
<section id="tk---preprocess-dataset-for-model" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="tk---preprocess-dataset-for-model"><span class="header-section-number">7.1</span> TK - Preprocess dataset for model</h3>
<ul>
<li>Weâ€™ve preprocessed and tried one sample, now we can do the same for batches of data.</li>
</ul>
<p>UPTOHERE</p>
<p>Next: - TK - write a function to transform batches of images (no augmentation, later can add augmentation) - TK - e.g.&nbsp;call it â€œpreprocess_batch_of_examplesâ€ - TK - preprocess datasets using .with_transform (only need one function to batchify data, can add transforms later) - TK - create a collate function</p>
<div id="cell-110" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess_batch(examples,</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>                    <span class="co">#  transforms, # Note: Could optionally add transforms (e.g. data augmentation) here </span></span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>                     image_processor):</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Function to preprocess batches of data.</span></span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Can optionally apply a transform later on.</span></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> []</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a>    coco_annotations <span class="op">=</span> [] </span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> image, image_id, annotations_dict <span class="kw">in</span> <span class="bu">zip</span>(examples[<span class="st">"image"</span>], examples[<span class="st">"image_id"</span>], examples[<span class="st">"annotations"</span>]):</span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note: may need to open image if it is an image path rather than PIL.Image</span></span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a>        bbox_list <span class="op">=</span> annotations_dict[<span class="st">"bbox"</span>]</span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a>        category_list <span class="op">=</span> annotations_dict[<span class="st">"category_id"</span>]</span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a>        area_list <span class="op">=</span> annotations_dict[<span class="st">"area"</span>]</span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note: Could optionally apply a transform here.</span></span>
<span id="cb96-19"><a href="#cb96-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">###</span></span>
<span id="cb96-20"><a href="#cb96-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-21"><a href="#cb96-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Format the annotations into COCO format</span></span>
<span id="cb96-22"><a href="#cb96-22" aria-hidden="true" tabindex="-1"></a>        cooc_format_annotations <span class="op">=</span> format_image_annotations_as_coco(image_id<span class="op">=</span>image_id,</span>
<span id="cb96-23"><a href="#cb96-23" aria-hidden="true" tabindex="-1"></a>                                                                   categories<span class="op">=</span>category_list,</span>
<span id="cb96-24"><a href="#cb96-24" aria-hidden="true" tabindex="-1"></a>                                                                   areas<span class="op">=</span>area_list,</span>
<span id="cb96-25"><a href="#cb96-25" aria-hidden="true" tabindex="-1"></a>                                                                   bboxes<span class="op">=</span>bbox_list)</span>
<span id="cb96-26"><a href="#cb96-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb96-27"><a href="#cb96-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add images/annotations to their respective lists</span></span>
<span id="cb96-28"><a href="#cb96-28" aria-hidden="true" tabindex="-1"></a>        images.append(image)</span>
<span id="cb96-29"><a href="#cb96-29" aria-hidden="true" tabindex="-1"></a>        coco_annotations.append(cooc_format_annotations)</span>
<span id="cb96-30"><a href="#cb96-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-31"><a href="#cb96-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-32"><a href="#cb96-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply the image processor to lists of images and annotations</span></span>
<span id="cb96-33"><a href="#cb96-33" aria-hidden="true" tabindex="-1"></a>    preprocessed_batch <span class="op">=</span> image_processor.preprocess(images<span class="op">=</span>images,</span>
<span id="cb96-34"><a href="#cb96-34" aria-hidden="true" tabindex="-1"></a>                                                    annotations<span class="op">=</span>coco_annotations,</span>
<span id="cb96-35"><a href="#cb96-35" aria-hidden="true" tabindex="-1"></a>                                                    return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb96-36"><a href="#cb96-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-37"><a href="#cb96-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> preprocessed_batch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-111" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a partial function for preprocessing</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Could create separate </span></span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>preprocess_batch_partial <span class="op">=</span> partial(preprocess_batch,</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>                                   image_processor<span class="op">=</span>image_processor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="tk---split-the-data" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="tk---split-the-data"><span class="header-section-number">7.2</span> TK - Split the data</h3>
<div id="cell-113" class="cell">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>dataset_split <span class="op">=</span> dataset[<span class="st">"train"</span>].train_test_split(test_size<span class="op">=</span><span class="fl">0.3</span>, seed<span class="op">=</span><span class="dv">42</span>) <span class="co"># split the dataset into 70/30 train/test</span></span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>dataset_test_val_split <span class="op">=</span> dataset_split[<span class="st">"test"</span>].train_test_split(test_size<span class="op">=</span><span class="fl">0.6</span>, seed<span class="op">=</span><span class="dv">42</span>) <span class="co"># split the test set into 40/60 validation/test</span></span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create splits</span></span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"train"</span>] <span class="op">=</span> dataset_split[<span class="st">"train"</span>]</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"validation"</span>] <span class="op">=</span> dataset_test_val_split[<span class="st">"train"</span>]</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"test"</span>] <span class="op">=</span> dataset_test_val_split[<span class="st">"test"</span>]</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>TK - apply processing function to each split</p>
<div id="cell-115" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the preprocessing function to the datasets (the preprocessing will happen on the fly, e.g. when the dataset is called rather than in-place)</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>processed_dataset <span class="op">=</span> dataset.copy()</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"train"</span>] <span class="op">=</span> dataset[<span class="st">"train"</span>].with_transform(transform<span class="op">=</span>preprocess_batch_partial)</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"validation"</span>] <span class="op">=</span> dataset[<span class="st">"validation"</span>].with_transform(transform<span class="op">=</span>preprocess_batch_partial)</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"test"</span>] <span class="op">=</span> dataset[<span class="st">"test"</span>].with_transform(transform<span class="op">=</span>preprocess_batch_partial)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-116" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"validation"</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>{'pixel_values': tensor([[[ 0.1254,  0.1254,  0.1597,  ..., -2.0837, -1.9809, -1.9295],
          [ 0.1426,  0.1254,  0.1597,  ..., -2.0494, -1.9638, -1.9467],
          [ 0.1426,  0.1426,  0.1597,  ..., -1.9467, -1.9295, -1.9467],
          ...,
          [ 1.2899,  1.0502,  1.1358,  ...,  0.7248,  0.7933,  0.7762],
          [ 1.4098,  1.1872,  1.0331,  ...,  0.7077,  0.7419,  0.7419],
          [ 1.2728,  0.9646,  0.9303,  ...,  0.7077,  0.7591,  0.7248]],
 
         [[ 1.2206,  1.1856,  1.1506,  ..., -1.9832, -1.8782, -1.7731],
          [ 1.2381,  1.1856,  1.1506,  ..., -1.9657, -1.8606, -1.8256],
          [ 1.2381,  1.2031,  1.1681,  ..., -1.8606, -1.8256, -1.8431],
          ...,
          [ 1.2906,  1.0630,  1.1506,  ...,  0.3803,  0.4503,  0.4328],
          [ 1.4307,  1.2031,  1.0280,  ...,  0.3627,  0.3978,  0.3978],
          [ 1.2906,  0.9755,  0.9230,  ...,  0.3627,  0.4153,  0.3803]],
 
         [[ 2.1346,  2.2217,  2.1868,  ..., -1.7173, -1.6127, -1.5604],
          [ 2.1520,  2.2217,  2.1868,  ..., -1.6999, -1.5953, -1.5779],
          [ 2.1694,  2.2217,  2.1868,  ..., -1.5953, -1.5430, -1.5604],
          ...,
          [ 1.2108,  0.9842,  1.0539,  ...,  0.3568,  0.4265,  0.4091],
          [ 1.3154,  1.0888,  0.9494,  ...,  0.3393,  0.3742,  0.3742],
          [ 1.1759,  0.8622,  0.8448,  ...,  0.3393,  0.3916,  0.3568]]]),
 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]]),
 'labels': {'size': tensor([640, 480]), 'image_id': tensor([719]), 'class_labels': tensor([4, 4, 1, 5, 0, 0]), 'boxes': tensor([[0.1898, 0.1767, 0.2161, 0.1620],
         [0.5669, 0.1938, 0.0742, 0.0805],
         [0.7672, 0.7768, 0.4526, 0.4327],
         [0.4715, 0.6213, 0.2235, 0.1502],
         [0.3973, 0.5639, 0.7729, 0.6337],
         [0.6906, 0.4581, 0.5110, 0.4600]]), 'area': tensor([ 10753.6875,   1833.4000,  60167.3867,  10316.8945, 150459.0469,
          72216.3203]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}}</code></pre>
</div>
</div>
<div id="cell-117" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Now when we call one or more of our samples, the preprocessing will take place</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"train"</span>][<span class="dv">0</span>:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>{'pixel_values': tensor([[[[-1.5870, -1.5870, -1.6042,  ..., -1.2617, -1.2617, -1.2788],
          [-1.5870, -1.5870, -1.5870,  ..., -0.9363, -0.9192, -0.9192],
          [-1.6042, -1.5870, -1.5870,  ..., -0.8164, -0.8335, -0.8164],
          ...,
          [-1.2959, -1.4329, -0.5938,  ..., -0.5596, -0.2856, -0.4054],
          [-1.2103, -0.9192, -0.3541,  ..., -0.5596,  0.1426,  0.1768],
          [-0.5938, -0.6109, -0.7137,  ..., -0.4226,  0.4337,  0.6906]],

         [[-1.9482, -1.9482, -1.9657,  ..., -1.0903, -1.0903, -1.1078],
          [-1.9482, -1.9482, -1.9482,  ..., -0.7227, -0.6877, -0.7052],
          [-1.9657, -1.9482, -1.9482,  ..., -0.5476, -0.5826, -0.5651],
          ...,
          [-0.9503, -1.0728, -0.1975,  ..., -0.1625,  0.0826, -0.0924],
          [-0.8803, -0.5476,  0.0476,  ..., -0.1625,  0.5028,  0.4678],
          [-0.2150, -0.1975, -0.2850,  ..., -0.0399,  0.7654,  0.9755]],

         [[-1.7347, -1.7347, -1.7522,  ..., -0.8807, -0.8807, -0.8981],
          [-1.7347, -1.7347, -1.7347,  ..., -0.5321, -0.4973, -0.5147],
          [-1.7522, -1.7347, -1.7347,  ..., -0.3753, -0.3927, -0.3753],
          ...,
          [-1.4210, -1.4907, -0.8110,  ..., -1.0550, -0.7761, -0.9330],
          [-1.3861, -1.2467, -0.8110,  ..., -1.0898, -0.3927, -0.4275],
          [-1.0376, -1.1247, -1.3687,  ..., -0.9853, -0.1835,  0.0256]]],


        [[[-1.7412, -1.8268, -1.7754,  ..., -1.5870, -1.2788, -1.4329],
          [-1.6555, -1.6213, -1.7583,  ..., -1.3815, -1.4158, -1.7240],
          [-1.7583, -1.7583, -1.3987,  ..., -1.6042, -1.8782, -1.9124],
          ...,
          [ 0.2624,  1.4440,  1.3584,  ...,  0.3823,  0.8276,  1.0502],
          [ 0.4851,  1.4783,  0.3823,  ...,  1.2557,  0.9988,  0.7419],
          [-0.0801, -0.0116, -0.1828,  ...,  0.9988,  0.8276,  0.8447]],

         [[-1.5280, -1.6155, -1.5455,  ..., -1.4930, -1.1604, -1.3179],
          [-1.4755, -1.3704, -1.5105,  ..., -1.2654, -1.3004, -1.5980],
          [-1.5980, -1.5455, -1.1078,  ..., -1.4755, -1.7731, -1.8081],
          ...,
          [ 0.3978,  1.6057,  1.5182,  ...,  0.4853,  0.9230,  1.1155],
          [ 0.6254,  1.6408,  0.5203,  ...,  1.3782,  1.1155,  0.8354],
          [ 0.0476,  0.1176, -0.0749,  ...,  1.1331,  0.9405,  0.9230]],

         [[-1.7173, -1.6824, -1.6127,  ..., -1.3164, -1.0550, -1.2293],
          [-1.5430, -1.5779, -1.6650,  ..., -1.1073, -1.2119, -1.5256],
          [-1.5953, -1.6476, -1.4733,  ..., -1.3513, -1.6650, -1.7347],
          ...,
          [ 0.4439,  1.6640,  1.5942,  ...,  0.4265,  0.8099,  0.9842],
          [ 0.6531,  1.6814,  0.6008,  ...,  1.2631,  0.9668,  0.6356],
          [ 0.0605,  0.1651,  0.0256,  ...,  0.9842,  0.7576,  0.7054]]],


        [[[-0.9363, -0.7479, -1.0390,  ..., -2.1008, -2.1008, -2.0665],
          [-1.3302, -0.9363, -0.7822,  ..., -2.1008, -2.1008, -2.0665],
          [-1.5014, -1.2617, -0.9705,  ..., -2.1008, -2.1008, -2.1008],
          ...,
          [ 1.8550,  1.8379,  1.7523,  ...,  1.2899,  1.2899,  0.8789],
          [ 1.8208,  1.7523,  1.6838,  ...,  1.1015,  1.3927,  0.9474],
          [ 1.7009,  1.6153,  1.6324,  ...,  1.1187,  1.4783,  1.1187]],

         [[-0.7577, -0.5651, -0.8627,  ..., -1.9832, -1.9832, -1.9482],
          [-1.1604, -0.7577, -0.6001,  ..., -1.9832, -1.9657, -1.9307],
          [-1.3354, -1.0903, -0.7927,  ..., -1.9832, -1.9832, -1.9657],
          ...,
          [ 1.1681,  1.1506,  1.0630,  ...,  1.5007,  1.4482,  0.9755],
          [ 1.1331,  1.0455,  0.9930,  ...,  1.2906,  1.5357,  1.0455],
          [ 1.0280,  0.9580,  0.9755,  ...,  1.3256,  1.6408,  1.2206]],

         [[-1.0376, -0.8110, -1.0724,  ..., -1.5779, -1.5779, -1.5604],
          [-1.4036, -0.9678, -0.7761,  ..., -1.5779, -1.5604, -1.5604],
          [-1.5779, -1.2816, -0.9678,  ..., -1.5779, -1.5604, -1.5953],
          ...,
          [ 0.8622,  0.8448,  0.7576,  ...,  1.5768,  1.4897,  0.9842],
          [ 0.8274,  0.7576,  0.7228,  ...,  1.3851,  1.5768,  1.0539],
          [ 0.7402,  0.6705,  0.7054,  ...,  1.4025,  1.6814,  1.2282]]],


        ...,


        [[[-1.2103, -1.1760, -1.1075,  ..., -0.7822, -0.9877, -1.0904],
          [-0.9192, -0.9705, -1.0219,  ..., -0.7993, -1.1247, -1.0219],
          [-0.5424, -0.8678, -1.0733,  ..., -1.0219, -1.2103, -0.9192],
          ...,
          [ 1.2385,  0.7591,  0.2624,  ...,  1.2214,  0.9132,  0.8618],
          [ 1.2385,  0.9474,  1.0502,  ...,  0.9646, -0.0801,  0.1083],
          [ 1.1187,  1.1872,  0.9474,  ...,  0.6906,  0.2967,  0.3652]],

         [[-1.0728, -1.0378, -0.9678,  ..., -0.6001, -0.8102, -0.9153],
          [-0.7752, -0.8277, -0.8803,  ..., -0.6352, -0.9503, -0.8452],
          [-0.3901, -0.7227, -0.9328,  ..., -0.8627, -1.0553, -0.7752],
          ...,
          [ 1.0980,  0.6429,  0.1527,  ...,  1.3081,  0.9755,  0.9405],
          [ 1.0980,  0.8704,  0.9755,  ...,  1.0280, -0.0574,  0.1352],
          [ 0.9930,  1.1155,  0.8704,  ...,  0.7479,  0.3102,  0.3803]],

         [[-1.2641, -1.2293, -1.1247,  ..., -0.9504, -1.1596, -1.2293],
          [-0.9330, -1.0027, -1.0376,  ..., -0.9678, -1.2641, -1.1421],
          [-0.5321, -0.8633, -1.0898,  ..., -1.1421, -1.2990, -0.9853],
          ...,
          [ 0.9319,  0.5659,  0.1651,  ...,  1.1934,  0.9145,  0.8797],
          [ 1.0539,  0.7751,  0.8797,  ...,  0.8971, -0.1312,  0.0605],
          [ 1.0365,  1.0017,  0.6356,  ...,  0.5834,  0.1999,  0.2871]]],


        [[[-1.4843, -1.3473, -1.4329,  ..., -0.9020, -0.8678, -0.8507],
          [-1.6898, -1.6555, -1.4843,  ..., -0.8507, -0.8507, -0.8507],
          [-1.4500, -1.6898, -1.3987,  ..., -0.8507, -0.8678, -0.8849],
          ...,
          [-0.8849, -0.7308, -0.4911,  ...,  1.8208,  1.8722,  1.8722],
          [-1.2274, -1.0219, -0.6109,  ...,  1.8550,  1.9064,  1.9064],
          [-1.7069, -1.4843, -1.1418,  ...,  1.8379,  1.9235,  1.9578]],

         [[-1.2829, -1.1779, -1.3004,  ...,  0.2752,  0.2577,  0.2402],
          [-1.4755, -1.4930, -1.3529,  ...,  0.2577,  0.2752,  0.2752],
          [-1.2829, -1.5280, -1.2654,  ...,  0.2927,  0.2927,  0.2927],
          ...,
          [-0.7752, -0.6176, -0.3550,  ...,  1.1681,  1.2381,  1.2556],
          [-1.1429, -0.9328, -0.4951,  ...,  1.2031,  1.2906,  1.3081],
          [-1.6331, -1.4055, -1.0378,  ...,  1.2031,  1.3081,  1.3606]],

         [[-1.3164, -1.1944, -1.2641,  ...,  1.7511,  1.7511,  1.7685],
          [-1.5256, -1.5256, -1.3513,  ...,  1.7685,  1.7337,  1.6814],
          [-1.2990, -1.5430, -1.2641,  ...,  1.7337,  1.7511,  1.7511],
          ...,
          [-0.6890, -0.5321, -0.2881,  ...,  0.8971,  0.9668,  0.9668],
          [-1.0027, -0.8110, -0.4101,  ...,  0.9319,  1.0017,  1.0191],
          [-1.4733, -1.2816, -0.9504,  ...,  0.9145,  1.0191,  1.0714]]],


        [[[-1.6042, -1.6213, -1.5870,  ..., -0.1486, -0.1314,  0.0056],
          [-1.5699, -1.5528, -1.5699,  ..., -0.1314, -0.1143,  0.0569],
          [-1.5870, -1.5185, -1.4843,  ..., -0.1143, -0.0629,  0.1597],
          ...,
          [ 0.9132,  1.1187,  1.3413,  ..., -0.7822, -0.7822, -0.7650],
          [ 1.4440,  1.0844,  1.3242,  ..., -0.7993, -0.7650, -0.7479],
          [ 1.3755,  0.8961,  1.3927,  ..., -0.8335, -0.7993, -0.7993]],

         [[-1.5980, -1.6506, -1.6506,  ..., -0.0224, -0.0049,  0.1176],
          [-1.5980, -1.6155, -1.6331,  ..., -0.0224, -0.0049,  0.1527],
          [-1.6506, -1.5980, -1.5805,  ..., -0.0399,  0.0126,  0.2402],
          ...,
          [ 0.4853,  0.7129,  0.9580,  ..., -0.7577, -0.7402, -0.7227],
          [ 1.0280,  0.6604,  0.9230,  ..., -0.7752, -0.7402, -0.7052],
          [ 0.9405,  0.4503,  1.0105,  ..., -0.8102, -0.7752, -0.7577]],

         [[-1.5256, -1.5604, -1.5430,  ...,  0.0779,  0.1128,  0.2522],
          [-1.4733, -1.4733, -1.4907,  ...,  0.0779,  0.1302,  0.2871],
          [-1.4733, -1.4210, -1.3861,  ...,  0.0779,  0.1476,  0.3742],
          ...,
          [ 0.1476,  0.3393,  0.5834,  ..., -0.6541, -0.6715, -0.6541],
          [ 0.7054,  0.3393,  0.6008,  ..., -0.6715, -0.6367, -0.6367],
          [ 0.6705,  0.1651,  0.7228,  ..., -0.6890, -0.6541, -0.6890]]]]), 'pixel_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]],

        [[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]],

        [[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]],

        ...,

        [[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]],

        [[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]],

        [[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]]]), 'labels': [{'size': tensor([640, 480]), 'image_id': tensor([69]), 'class_labels': tensor([5, 0, 1, 4, 4, 4, 4, 4]), 'boxes': tensor([[0.4675, 0.5152, 0.1846, 0.2045],
        [0.5092, 0.5843, 0.3970, 0.3951],
        [0.2719, 0.5861, 0.3738, 0.2471],
        [0.1023, 0.6896, 0.2019, 0.1655],
        [0.3902, 0.0924, 0.1530, 0.0898],
        [0.5345, 0.0871, 0.0252, 0.0556],
        [0.6370, 0.0877, 0.1357, 0.0899],
        [0.9383, 0.0634, 0.0789, 0.0627]]), 'area': tensor([11597.7402, 48180.5664, 28372.1094, 10266.5547,  4223.3750,   430.7600,
         3749.3826,  1517.7850]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1027]), 'class_labels': tensor([5, 4, 1, 0, 0]), 'boxes': tensor([[0.4669, 0.5782, 0.1456, 0.1290],
        [0.5031, 0.6013, 0.0410, 0.0237],
        [0.5269, 0.6380, 0.1138, 0.1280],
        [0.3863, 0.5047, 0.4801, 0.3840],
        [0.1074, 0.4195, 0.2101, 0.3353]]), 'area': tensor([ 5770.2451,   298.4550,  4471.7402, 56633.0859, 21642.4102]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1092]), 'class_labels': tensor([2, 5, 1, 0]), 'boxes': tensor([[0.1943, 0.1126, 0.1849, 0.0794],
        [0.5387, 0.5818, 0.3646, 0.2689],
        [0.3515, 0.7725, 0.3171, 0.2903],
        [0.5404, 0.4307, 0.6236, 0.4566]]), 'area': tensor([ 4508.5000, 30117.5000, 28278.7598, 87485.0391]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([228]), 'class_labels': tensor([0]), 'boxes': tensor([[0.5187, 0.5418, 0.4982, 0.5698]]), 'area': tensor([87218.0078]), 'iscrowd': tensor([0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([511]), 'class_labels': tensor([5, 1]), 'boxes': tensor([[0.5284, 0.5886, 0.2903, 0.3347],
        [0.7784, 0.7873, 0.4400, 0.4222]]), 'area': tensor([29848.7695, 57066.2383]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([338]), 'class_labels': tensor([5, 0, 1]), 'boxes': tensor([[0.4990, 0.5424, 0.2227, 0.1716],
        [0.5455, 0.5335, 0.3754, 0.3595],
        [0.7111, 0.6979, 0.3313, 0.2838]]), 'area': tensor([11742.9648, 41455.0117, 28882.3496]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([405]), 'class_labels': tensor([0, 1, 5]), 'boxes': tensor([[0.4952, 0.6559, 0.6088, 0.4872],
        [0.2074, 0.7760, 0.4117, 0.4459],
        [0.4132, 0.5714, 0.0663, 0.0580]]), 'area': tensor([91107.9609, 56385.1602,  1179.7800]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([3]), 'class_labels': tensor([0, 5, 1, 4, 4, 4]), 'boxes': tensor([[0.5020, 0.4466, 0.6579, 0.5829],
        [0.5148, 0.5684, 0.2288, 0.1367],
        [0.7040, 0.7836, 0.4468, 0.4219],
        [0.3160, 0.8416, 0.3991, 0.2993],
        [0.4095, 0.0661, 0.0888, 0.0666],
        [0.7489, 0.1356, 0.3843, 0.2637]]), 'area': tensor([117809.1875,   9607.5000,  57901.5000,  36691.4023,   1814.7600,
         31125.9375]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([182]), 'class_labels': tensor([0, 1, 5]), 'boxes': tensor([[0.5786, 0.5016, 0.5992, 0.4539],
        [0.6307, 0.7197, 0.4165, 0.3323],
        [0.4415, 0.6429, 0.1546, 0.2070]]), 'area': tensor([83547.7969, 42508.7344,  9827.7900]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([640]), 'class_labels': tensor([5, 1, 0]), 'boxes': tensor([[0.5314, 0.6391, 0.2920, 0.4553],
        [0.7088, 0.7733, 0.5596, 0.4422],
        [0.5282, 0.5060, 0.5678, 0.4612]]), 'area': tensor([40839.7109, 76013.7969, 80443.1328]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}]}</code></pre>
</div>
</div>
<div id="cell-118" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Images are reshaped to be the IMAGE_SIZE value that we set</span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"train"</span>][<span class="dv">0</span>][<span class="st">"pixel_values"</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>torch.Size([3, 640, 480])</code></pre>
</div>
</div>
</section>
<section id="tk---create-a-collation-function" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="tk---create-a-collation-function"><span class="header-section-number">7.3</span> TK - Create a collation function</h3>
<p>Notes: * The input to the <code>data_collator</code> function will be the output of <code>image_processor</code>, see below for format. * The output of the <code>data_collator</code> will be passed to our modelâ€™s <code>forward()</code> method. * <code>data_collator</code> for <code>transformers.Trainer</code> - https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.data_collator * â€œThe function to use to form a batch from a list of elements of <code>train_dataset</code>.</p>
<p>Input to <code>data_collator</code> is the output of <code>image_processor</code>:</p>
<pre><code>{'pixel_values': tensor([[[ 2.2318,  2.2318,  2.2318,  ...,  0.3309,  0.2282,  0.1254],
          [ 2.2318,  2.2318,  2.2318,  ...,  0.3138,  0.2111,  0.1426],
          [ 2.2318,  2.2318,  2.2489,  ...,  0.2967,  0.2111,  0.1426],
          ...,
          [-0.8164, -0.8164, -0.7993,  ...,  0.5878,  0.5707,  0.5878],
          [-0.9363, -0.8849, -0.8164,  ...,  0.5193,  0.5364,  0.5707],
          [-0.9877, -0.9363, -0.9192,  ...,  0.5707,  0.5707,  0.5878]],
 
         [[ 2.4286,  2.4286,  2.4286,  ...,  0.4853,  0.4153,  0.3277],
          [ 2.4286,  2.4286,  2.4286,  ...,  0.4853,  0.3978,  0.3277],
          [ 2.4286,  2.4286,  2.4286,  ...,  0.4678,  0.3803,  0.3102],
          ...,
          [-1.1253, -1.1253, -1.1078,  ...,  0.2052,  0.1877,  0.2052],
          [-1.2129, -1.1604, -1.1253,  ...,  0.1352,  0.1527,  0.1877],
          [-1.2479, -1.2129, -1.2304,  ...,  0.1877,  0.1877,  0.2052]],
 
         [[ 2.6051,  2.6051,  2.6051,  ...,  0.6531,  0.6008,  0.5311],
          [ 2.6051,  2.6051,  2.6051,  ...,  0.6531,  0.5659,  0.5136],
          [ 2.6051,  2.6051,  2.6051,  ...,  0.6356,  0.5485,  0.4788],
          ...,
          [-1.3861, -1.3687, -1.3339,  ..., -0.2358, -0.2532, -0.2358],
          [-1.4907, -1.4210, -1.3513,  ..., -0.3055, -0.2881, -0.2532],
          [-1.5256, -1.4733, -1.4559,  ..., -0.2532, -0.2532, -0.2358]]]),
 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
...
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]]),
 'labels': {'size': tensor([1066,  800]), 'image_id': tensor([0]), 'class_labels': tensor([1, 0]), 'boxes': tensor([[0.7553, 0.5571, 0.4196, 0.2626],
         [0.5022, 0.5583, 0.9827, 0.8609]]), 'area': tensor([ 93955.8828, 721446.3750]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}}</code></pre>
<p>The <code>data_collator</code> function will turn collections of these into batches (e.g.&nbsp;stack together the <code>pixel_values</code>, <code>pixel_mask</code>, <code>labels</code> etc).</p>
<div id="cell-120" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create data_collate_function to collect samples into batches</span></span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a><span class="co"># TK - want to get a dictionary of {"pixel_mask": [batch_of_samples], "labels": [batch_of_samples], "pixel_mask": [batch_of_samples]}</span></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> data_collate_function(batch):</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>    collated_data <span class="op">=</span> {} </span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stack together a collection of pixel_values tensors</span></span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a>    collated_data[<span class="st">"pixel_values"</span>] <span class="op">=</span> torch.stack([sample[<span class="st">"pixel_values"</span>] <span class="cf">for</span> sample <span class="kw">in</span> batch])</span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the labels (these are dictionaries so no need to use torch.stack)</span></span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a>    collated_data[<span class="st">"labels"</span>] <span class="op">=</span> [sample[<span class="st">"labels"</span>] <span class="cf">for</span> sample <span class="kw">in</span> batch]</span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If there is a pixel_mask key, return the pixel_mask's as well</span></span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"pixel_mask"</span> <span class="kw">in</span> batch[<span class="dv">0</span>]:</span>
<span id="cb107-14"><a href="#cb107-14" aria-hidden="true" tabindex="-1"></a>        collated_data[<span class="st">"pixel_mask"</span>] <span class="op">=</span> torch.stack([sample[<span class="st">"pixel_mask"</span>] <span class="cf">for</span> sample <span class="kw">in</span> batch])</span>
<span id="cb107-15"><a href="#cb107-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-16"><a href="#cb107-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> collated_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-121" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Try data_collate_function </span></span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>example_collated_data_batch <span class="op">=</span> data_collate_function(processed_dataset[<span class="st">"train"</span>].select(<span class="bu">range</span>(<span class="dv">32</span>)))</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>example_collated_data_batch[<span class="st">"pixel_values"</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 2.01 s, sys: 131 ms, total: 2.14 s
Wall time: 1.45 s</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>torch.Size([32, 3, 640, 480])</code></pre>
</div>
</div>
<div id="cell-122" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>example_collated_data_batch.keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>dict_keys(['pixel_values', 'labels', 'pixel_mask'])</code></pre>
</div>
</div>
<div id="cell-123" class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 32 samples (because that's our batch size)</span></span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(example_collated_data_batch[<span class="st">"pixel_values"</span>]), <span class="bu">len</span>(example_collated_data_batch[<span class="st">"labels"</span>]), <span class="bu">len</span>(example_collated_data_batch[<span class="st">"pixel_mask"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>(32, 32, 32)</code></pre>
</div>
</div>
<p>TK - We get a batch of 32 samples with size 640, 480, these are all preprocessed as well and will be fed to our model.</p>
<div id="cell-125" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time </span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Try pass a batch through our model (note: this will be slow if our model is on the CPU)</span></span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>example_batch_outputs <span class="op">=</span> model(example_collated_data_batch[<span class="st">"pixel_values"</span>])</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>example_batch_outputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 1min 4s, sys: 12.5 s, total: 1min 17s
Wall time: 5.21 s</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>ConditionalDetrObjectDetectionOutput(loss=None, loss_dict=None, logits=tensor([[[ 0.1756,  0.0112, -0.1084,  ...,  0.1422,  0.0683,  0.1605],
         [-0.2120, -0.2104, -0.1722,  ...,  0.3864, -0.1778,  0.2019],
         [ 0.1066,  0.1096,  0.2123,  ...,  0.1867, -0.0547,  0.2594],
         ...,
         [-0.3185,  0.3699, -0.2245,  ...,  0.1371,  0.2279,  0.2639],
         [ 0.0702,  0.0533,  0.1279,  ...,  0.2358, -0.1269,  0.2406],
         [-0.1309, -0.3195,  0.1867,  ...,  0.4492, -0.0839,  0.4281]],

        [[ 0.1036,  0.0428, -0.2660,  ...,  0.0152,  0.0188,  0.0505],
         [-0.1730, -0.3609, -0.0393,  ...,  0.2778, -0.2219,  0.1670],
         [ 0.0929,  0.2278,  0.2457,  ...,  0.0409, -0.1385,  0.1913],
         ...,
         [-0.0265,  0.0631,  0.0627,  ...,  0.0372, -0.1568,  0.0072],
         [ 0.0708,  0.1320,  0.1984,  ...,  0.1450, -0.0370,  0.1971],
         [-0.2185, -0.3554,  0.0250,  ...,  0.1523, -0.1766, -0.2412]],

        [[-0.0034, -0.1252, -0.4586,  ...,  0.0920, -0.0194,  0.0565],
         [-0.1779, -0.3050, -0.0245,  ...,  0.1755, -0.2620,  0.3097],
         [-0.0193,  0.0550, -0.0951,  ..., -0.0771,  0.0046,  0.0384],
         ...,
         [-0.2811, -0.0509, -0.0340,  ...,  0.4088, -0.0885,  0.1977],
         [-0.1411, -0.2114, -0.0364,  ...,  0.1844, -0.2052, -0.1303],
         [-0.0397, -0.3287,  0.0959,  ...,  0.3857, -0.2455,  0.3551]],

        ...,

        [[-0.2905, -0.1199, -0.5113,  ...,  0.0797,  0.0761, -0.1454],
         [-0.3391, -0.4398,  0.1613,  ...,  0.3521, -0.2897,  0.4688],
         [ 0.0515,  0.1871,  0.2654,  ...,  0.0055,  0.0177, -0.2444],
         ...,
         [-0.5897,  0.2452, -0.1715,  ...,  0.1403,  0.2739,  0.2423],
         [ 0.0082,  0.3222,  0.1669,  ...,  0.0938,  0.1326, -0.1318],
         [-0.1835, -0.0591,  0.1662,  ...,  0.1506, -0.1369, -0.0960]],

        [[ 0.0088,  0.0562, -0.1568,  ...,  0.0956,  0.1420, -0.0164],
         [-0.1252, -0.3315, -0.0670,  ...,  0.3029, -0.3670,  0.2253],
         [ 0.1418,  0.0832,  0.1878,  ...,  0.2082, -0.2881,  0.0064],
         ...,
         [-0.3357,  0.0241, -0.2351,  ...,  0.1009,  0.2384,  0.1972],
         [ 0.1632,  0.0212,  0.1528,  ...,  0.2441, -0.2813, -0.1012],
         [-0.2424, -0.3850,  0.1242,  ...,  0.2214, -0.4294, -0.2708]],

        [[ 0.0734, -0.0391, -0.4524,  ...,  0.0742, -0.0376, -0.1117],
         [ 0.0506, -0.0210,  0.0115,  ...,  0.0043, -0.1665, -0.0796],
         [ 0.0133, -0.2106, -0.0142,  ...,  0.5130, -0.2083,  0.1878],
         ...,
         [-0.2460, -0.1284, -0.1073,  ...,  0.2888, -0.2080,  0.0897],
         [-0.1026, -0.2328, -0.1268,  ...,  0.4177, -0.3034,  0.1005],
         [-0.2828, -0.4220,  0.1543,  ...,  0.3707, -0.5253, -0.1016]]],
       grad_fn=&lt;ViewBackward0&gt;), pred_boxes=tensor([[[0.9500, 0.6381, 0.1323, 0.6838],
         [0.6333, 0.0871, 0.1233, 0.0674],
         [0.9906, 0.3960, 0.0203, 0.1109],
         ...,
         [0.3539, 0.4133, 0.7001, 0.7790],
         [0.9606, 0.3789, 0.0489, 0.0365],
         [0.0161, 0.1030, 0.0344, 0.0579]],

        [[0.7669, 0.9339, 0.5311, 0.1394],
         [0.6464, 0.0556, 0.1167, 0.1030],
         [0.9931, 0.5555, 0.0139, 0.1452],
         ...,
         [0.3557, 0.4036, 0.2504, 0.1227],
         [0.9974, 0.1280, 0.0060, 0.2545],
         [0.0663, 0.3102, 0.1351, 0.1014]],

        [[0.7914, 0.7499, 0.3953, 0.4964],
         [0.6263, 0.0586, 0.2221, 0.0999],
         [0.8788, 0.5819, 0.2370, 0.4087],
         ...,
         [0.5177, 0.3094, 0.5866, 0.2528],
         [0.8649, 0.4862, 0.2535, 0.2284],
         [0.0075, 0.1096, 0.0162, 0.0404]],

        ...,

        [[0.6732, 0.8154, 0.6100, 0.3546],
         [0.6279, 0.0335, 0.0599, 0.0650],
         [0.9707, 0.6607, 0.0617, 0.3039],
         ...,
         [0.4202, 0.3927, 0.8342, 0.4869],
         [0.9947, 0.7069, 0.0112, 0.4913],
         [0.0305, 0.3756, 0.0607, 0.2126]],

        [[0.8158, 0.7939, 0.3493, 0.3947],
         [0.6333, 0.0566, 0.1713, 0.1139],
         [0.9268, 0.5027, 0.1466, 0.1161],
         ...,
         [0.3697, 0.3443, 0.7456, 0.7034],
         [0.8904, 0.4677, 0.2055, 0.1397],
         [0.0306, 0.3043, 0.0608, 0.0623]],

        [[0.7667, 0.7876, 0.4440, 0.4146],
         [0.6509, 0.2361, 0.0934, 0.0602],
         [0.9449, 0.3689, 0.0946, 0.0297],
         ...,
         [0.4404, 0.3215, 0.4841, 0.1150],
         [0.8951, 0.3655, 0.1890, 0.0374],
         [0.0411, 0.2678, 0.0848, 0.0570]]], grad_fn=&lt;SigmoidBackward0&gt;), auxiliary_outputs=None, last_hidden_state=tensor([[[-1.4020e-01, -1.5893e-01,  4.4403e-01,  ...,  1.5252e-01,
           2.8576e-01,  2.6249e-01],
         [ 6.8369e-02, -2.7463e-01, -4.5402e-01,  ..., -9.0982e-01,
          -4.7036e-01,  7.2642e-01],
         [-1.7512e-01,  3.1511e-01,  2.2512e-01,  ..., -1.4200e-01,
           2.5577e-01,  4.1778e-01],
         ...,
         [ 1.5953e-01, -4.0302e-01,  2.2796e-01,  ..., -8.1157e-01,
          -3.6345e-01, -8.9928e-02],
         [ 4.0930e-02,  6.6010e-04,  1.2503e-01,  ..., -5.6554e-02,
           3.2782e-01,  3.9761e-01],
         [-1.5904e-02,  5.8626e-01, -1.3788e-01,  ..., -7.4208e-01,
          -1.3682e-01,  1.0417e-01]],

        [[ 1.8487e-01, -2.6388e-01,  7.6519e-01,  ..., -4.4617e-01,
           1.6003e-01,  5.6029e-01],
         [ 5.1641e-01, -5.4275e-02,  1.0399e+00,  ..., -8.5620e-01,
          -2.2614e-01, -2.9099e-01],
         [-2.0582e-01,  2.9136e-01,  2.8441e-01,  ..., -4.6227e-02,
           2.9668e-01,  7.5241e-01],
         ...,
         [ 3.8438e-01,  6.9957e-01, -5.8716e-01,  ..., -9.2270e-01,
          -4.5221e-02, -1.3225e-01],
         [-6.4926e-02,  1.9942e-01,  4.3592e-01,  ...,  3.0664e-02,
           5.1831e-01,  3.6161e-01],
         [ 4.8070e-01, -5.2024e-01,  2.0143e-01,  ..., -1.5431e+00,
          -3.6578e-01, -2.4390e-01]],

        [[ 2.7832e-01,  7.0842e-02,  1.2050e+00,  ..., -7.3184e-01,
           1.7189e-01,  3.8562e-02],
         [ 7.5524e-01,  1.0498e-01,  5.4896e-01,  ..., -4.7316e-01,
           7.8752e-03,  2.6307e-01],
         [-3.7225e-01,  5.2872e-02,  5.6387e-01,  ..., -1.3147e+00,
           2.3460e-01,  4.7530e-01],
         ...,
         [ 3.1748e-01, -1.0066e+00,  3.5116e-01,  ..., -8.5966e-01,
          -1.8258e-01,  2.6463e-01],
         [-3.0530e-02, -1.0162e+00,  4.3357e-01,  ..., -1.1250e+00,
          -1.9363e-01, -7.9971e-02],
         [ 3.0213e-01,  1.3661e-01, -6.4669e-01,  ..., -5.1888e-01,
          -6.2747e-02,  6.2570e-01]],

        ...,

        [[ 3.9975e-01, -9.5206e-01,  8.8087e-01,  ..., -8.3797e-01,
          -3.4231e-02,  1.5127e-02],
         [ 3.7874e-01,  4.6002e-01,  5.5632e-01,  ..., -8.4079e-01,
           3.5074e-01, -1.0479e-01],
         [-2.1702e-01, -6.3238e-01,  3.0843e-01,  ..., -4.9595e-01,
           3.9976e-01,  7.5963e-01],
         ...,
         [-9.0952e-02, -1.8212e+00, -7.9186e-02,  ..., -1.0548e+00,
          -7.6392e-02,  3.0424e-01],
         [-5.6228e-02, -5.4257e-01,  3.7607e-01,  ..., -1.8365e-01,
           7.9351e-01,  1.0800e+00],
         [ 8.0718e-02, -3.2467e-01,  3.0199e-02,  ..., -1.0819e+00,
           1.6267e-01,  4.1212e-01]],

        [[ 4.9446e-01, -3.8678e-01,  9.7415e-01,  ..., -9.0278e-01,
           9.9647e-03,  4.2870e-02],
         [ 7.2289e-01,  2.6472e-01,  6.9674e-01,  ..., -8.4964e-01,
          -3.5554e-01, -4.0242e-01],
         [ 2.0905e-01,  1.7493e-01,  7.1425e-01,  ..., -6.0879e-01,
          -2.6598e-01,  5.8427e-01],
         ...,
         [ 3.1929e-01, -1.3318e+00,  1.0949e+00,  ..., -1.0937e+00,
          -4.9580e-01, -4.8511e-01],
         [ 2.8816e-01,  1.6738e-04,  1.1606e+00,  ..., -7.3686e-01,
          -2.4679e-01,  1.9954e-01],
         [ 1.8261e-01, -1.2720e-02, -3.0613e-01,  ..., -6.9232e-01,
          -2.6717e-01,  1.7242e-01]],

        [[ 1.1384e-01,  1.4387e-01,  3.6687e-02,  ..., -7.7477e-01,
           1.0376e-01, -2.5709e-01],
         [ 3.4558e-01, -4.4018e-01,  3.6415e-01,  ...,  1.7454e-01,
           2.4093e-01, -4.9051e-02],
         [ 1.7516e-01, -2.2057e-01, -1.2419e-01,  ..., -1.5287e-01,
           6.2450e-02,  4.9240e-02],
         ...,
         [ 6.6910e-01, -3.4297e-01, -2.0511e-01,  ..., -1.0155e+00,
           7.9812e-03,  3.0636e-01],
         [ 4.0032e-01, -3.4343e-01,  1.5294e-01,  ..., -3.3256e-01,
          -2.5672e-01, -1.9711e-01],
         [-1.1014e-01, -4.8125e-01,  1.0338e-01,  ..., -7.0084e-01,
           4.9208e-03,  2.7278e-01]]], grad_fn=&lt;NativeLayerNormBackward0&gt;), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.3462,  0.1944, -0.1375,  ..., -0.4447,  0.4016,  0.4290],
         [ 0.0648,  0.2144,  0.0340,  ...,  0.2365,  0.1294,  0.3575],
         [ 0.1515,  0.5005, -0.0685,  ..., -0.0157,  0.1598,  0.3866],
         ...,
         [ 0.1488,  0.8020, -0.2199,  ...,  0.2656,  0.0879,  0.2309],
         [ 0.1548,  0.6870, -0.1847,  ...,  0.3029,  0.0465,  0.2264],
         [-0.1375,  0.4506, -0.2336,  ..., -0.0616,  0.1774,  0.2659]],

        [[-0.3369,  0.3608, -0.2942,  ..., -0.4818,  0.4762,  0.3779],
         [ 0.0714,  0.3084,  0.0148,  ...,  0.0797,  0.2380,  0.3244],
         [ 0.0873,  0.4330, -0.0352,  ..., -0.2179,  0.2011,  0.2788],
         ...,
         [-0.0706,  0.0146,  0.1921,  ..., -0.1177, -0.1456,  0.0187],
         [ 0.1120,  0.2591,  0.0263,  ...,  0.1479, -0.0880,  0.0873],
         [-0.1774,  0.3163, -0.0410,  ...,  0.0425,  0.1321,  0.2753]],

        [[-0.2815,  0.3443, -0.2270,  ..., -0.5475,  0.2527,  0.3086],
         [ 0.1719,  0.4588, -0.0811,  ...,  0.0694,  0.0811,  0.3715],
         [ 0.2389,  0.2392, -0.1076,  ..., -0.1341, -0.2286,  0.2902],
         ...,
         [ 0.2274,  0.4766,  0.0128,  ...,  0.2001,  0.2571,  0.2773],
         [ 0.2339,  0.5257,  0.0034,  ...,  0.2795,  0.2356,  0.2127],
         [-0.0985,  0.3517, -0.0659,  ..., -0.0961,  0.3029,  0.1836]],

        ...,

        [[-0.3820,  0.4122, -0.4279,  ..., -0.4390,  0.4537,  0.3619],
         [ 0.0776,  0.4093, -0.1319,  ...,  0.3167,  0.1865,  0.4449],
         [ 0.0644,  0.5139, -0.1786,  ...,  0.1034,  0.1915,  0.3504],
         ...,
         [-0.0715,  0.1232,  0.0057,  ...,  0.2714,  0.0190,  0.1771],
         [ 0.1267,  0.3740,  0.0213,  ..., -0.0367,  0.0245,  0.2749],
         [-0.1652,  0.1528,  0.1033,  ..., -0.1985,  0.0891,  0.3079]],

        [[-0.2655,  0.2723, -0.2191,  ..., -0.3646,  0.3872,  0.2680],
         [ 0.1672,  0.2333, -0.0337,  ...,  0.2537,  0.2663,  0.3487],
         [ 0.1631,  0.3007, -0.1148,  ...,  0.1061,  0.1698,  0.2983],
         ...,
         [ 0.1221,  0.1708,  0.0071,  ...,  0.4499, -0.0821,  0.0854],
         [ 0.1202,  0.0732, -0.0148,  ...,  0.6552, -0.2320,  0.0461],
         [-0.0094,  0.2407,  0.1013,  ..., -0.1772, -0.1296,  0.0011]],

        [[-0.3589,  0.4908, -0.3906,  ..., -0.5620,  0.4539,  0.2588],
         [ 0.1310,  0.5131, -0.0584,  ...,  0.1296,  0.1215,  0.2423],
         [ 0.1021,  0.6150, -0.0859,  ..., -0.0818,  0.1724,  0.2820],
         ...,
         [ 0.2026,  0.4986,  0.1082,  ...,  0.1570,  0.1229,  0.1716],
         [ 0.1716,  0.3375,  0.1374,  ...,  0.4551,  0.0419,  0.0987],
         [-0.0736,  0.2892,  0.0910,  ..., -0.2655,  0.1247,  0.0657]]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;), encoder_hidden_states=None, encoder_attentions=None)</code></pre>
</div>
</div>
<div id="cell-126" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>example_batch_outputs.keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre><code>odict_keys(['logits', 'pred_boxes', 'last_hidden_state', 'encoder_last_hidden_state'])</code></pre>
</div>
</div>
<div id="cell-127" class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We get 300 predictions per image in our batch, each with a logit value for each of the classes in our dataset </span></span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>example_batch_outputs.logits.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>torch.Size([32, 300, 7])</code></pre>
</div>
</div>
<p>This is what will happen during training, our model will continually go over batches over data and try to match its own predictions with the ground truth labels.</p>
</section>
</section>
<section id="tk---setup-trainingarguments-trainer" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="tk---setup-trainingarguments-trainer"><span class="header-section-number">8</span> TK - Setup TrainingArguments + Trainer</h2>
<p>UPTOHERE - creating TrainingArguments + Trainer + Training a model</p>
<ul>
<li>TK - for hyperparameters, see example in RT-DETR paper: https://arxiv.org/pdf/2304.08069</li>
<li>As well as DETR - https://arxiv.org/pdf/2005.12872 (see Appendix A.4)</li>
<li>Try training for 25 epochs and see what happens</li>
</ul>
<div id="cell-130" class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"validation"</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>{'pixel_values': tensor([[[ 0.1254,  0.1254,  0.1597,  ..., -2.0837, -1.9809, -1.9295],
          [ 0.1426,  0.1254,  0.1597,  ..., -2.0494, -1.9638, -1.9467],
          [ 0.1426,  0.1426,  0.1597,  ..., -1.9467, -1.9295, -1.9467],
          ...,
          [ 1.2899,  1.0502,  1.1358,  ...,  0.7248,  0.7933,  0.7762],
          [ 1.4098,  1.1872,  1.0331,  ...,  0.7077,  0.7419,  0.7419],
          [ 1.2728,  0.9646,  0.9303,  ...,  0.7077,  0.7591,  0.7248]],
 
         [[ 1.2206,  1.1856,  1.1506,  ..., -1.9832, -1.8782, -1.7731],
          [ 1.2381,  1.1856,  1.1506,  ..., -1.9657, -1.8606, -1.8256],
          [ 1.2381,  1.2031,  1.1681,  ..., -1.8606, -1.8256, -1.8431],
          ...,
          [ 1.2906,  1.0630,  1.1506,  ...,  0.3803,  0.4503,  0.4328],
          [ 1.4307,  1.2031,  1.0280,  ...,  0.3627,  0.3978,  0.3978],
          [ 1.2906,  0.9755,  0.9230,  ...,  0.3627,  0.4153,  0.3803]],
 
         [[ 2.1346,  2.2217,  2.1868,  ..., -1.7173, -1.6127, -1.5604],
          [ 2.1520,  2.2217,  2.1868,  ..., -1.6999, -1.5953, -1.5779],
          [ 2.1694,  2.2217,  2.1868,  ..., -1.5953, -1.5430, -1.5604],
          ...,
          [ 1.2108,  0.9842,  1.0539,  ...,  0.3568,  0.4265,  0.4091],
          [ 1.3154,  1.0888,  0.9494,  ...,  0.3393,  0.3742,  0.3742],
          [ 1.1759,  0.8622,  0.8448,  ...,  0.3393,  0.3916,  0.3568]]]),
 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]]),
 'labels': {'size': tensor([640, 480]), 'image_id': tensor([719]), 'class_labels': tensor([4, 4, 1, 5, 0, 0]), 'boxes': tensor([[0.1898, 0.1767, 0.2161, 0.1620],
         [0.5669, 0.1938, 0.0742, 0.0805],
         [0.7672, 0.7768, 0.4526, 0.4327],
         [0.4715, 0.6213, 0.2235, 0.1502],
         [0.3973, 0.5639, 0.7729, 0.6337],
         [0.6906, 0.4581, 0.5110, 0.4600]]), 'area': tensor([ 10753.6875,   1833.4000,  60167.3867,  10316.8945, 150459.0469,
          72216.3203]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}}</code></pre>
</div>
</div>
<div id="cell-131" class="cell">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Depending on the size/speed of your GPU, this may take a while</span></span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TrainingArguments, Trainer</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the batch size according to the memory you have available on your GPU</span></span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a><span class="co"># e.g. on my NVIDIA RTX 4090 with 24GB of VRAM, I can use a batch size of 32 without running out of memory</span></span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb124-7"><a href="#cb124-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-8"><a href="#cb124-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: AdamW Optimizer is used by default</span></span>
<span id="cb124-9"><a href="#cb124-9" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb124-10"><a href="#cb124-10" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"detr_finetuned_trashify_box_detector"</span>,</span>
<span id="cb124-11"><a href="#cb124-11" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">25</span>,</span>
<span id="cb124-12"><a href="#cb124-12" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb124-13"><a href="#cb124-13" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb124-14"><a href="#cb124-14" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb124-15"><a href="#cb124-15" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">1e-4</span>,</span>
<span id="cb124-16"><a href="#cb124-16" aria-hidden="true" tabindex="-1"></a>    lr_scheduler_type<span class="op">=</span><span class="st">"linear"</span>,</span>
<span id="cb124-17"><a href="#cb124-17" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">1e-4</span>,</span>
<span id="cb124-18"><a href="#cb124-18" aria-hidden="true" tabindex="-1"></a>    max_grad_norm<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb124-19"><a href="#cb124-19" aria-hidden="true" tabindex="-1"></a>    metric_for_best_model<span class="op">=</span><span class="st">"eval_loss"</span>,</span>
<span id="cb124-20"><a href="#cb124-20" aria-hidden="true" tabindex="-1"></a>    greater_is_better<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb124-21"><a href="#cb124-21" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb124-22"><a href="#cb124-22" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb124-23"><a href="#cb124-23" aria-hidden="true" tabindex="-1"></a>    logging_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb124-24"><a href="#cb124-24" aria-hidden="true" tabindex="-1"></a>    save_total_limit<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb124-25"><a href="#cb124-25" aria-hidden="true" tabindex="-1"></a>    remove_unused_columns<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb124-26"><a href="#cb124-26" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>, <span class="co"># don't save experiments to a third party service</span></span>
<span id="cb124-27"><a href="#cb124-27" aria-hidden="true" tabindex="-1"></a>    dataloader_num_workers<span class="op">=</span><span class="dv">4</span>, <span class="co"># note: if you're on Google Colab, you may have to lower this to os.cpu_count() or to 0 </span></span>
<span id="cb124-28"><a href="#cb124-28" aria-hidden="true" tabindex="-1"></a>    warmup_ratio<span class="op">=</span><span class="fl">0.05</span>, <span class="co"># learning rate warmup</span></span>
<span id="cb124-29"><a href="#cb124-29" aria-hidden="true" tabindex="-1"></a>    push_to_hub<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb124-30"><a href="#cb124-30" aria-hidden="true" tabindex="-1"></a>    eval_do_concat_batches<span class="op">=</span><span class="va">False</span></span>
<span id="cb124-31"><a href="#cb124-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb124-32"><a href="#cb124-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-33"><a href="#cb124-33" aria-hidden="true" tabindex="-1"></a>model_v1_trainer <span class="op">=</span> Trainer(</span>
<span id="cb124-34"><a href="#cb124-34" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb124-35"><a href="#cb124-35" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb124-36"><a href="#cb124-36" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>processed_dataset[<span class="st">"train"</span>],</span>
<span id="cb124-37"><a href="#cb124-37" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>processed_dataset[<span class="st">"validation"</span>],</span>
<span id="cb124-38"><a href="#cb124-38" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>image_processor,</span>
<span id="cb124-39"><a href="#cb124-39" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collate_function,</span>
<span id="cb124-40"><a href="#cb124-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute_metrics=None # </span><span class="al">TODO</span><span class="co">: TK - can add a metrics function, just see if model trains first, see here for an example: https://github.com/huggingface/transformers/blob/336dc69d63d56f232a183a3e7f52790429b871ef/examples/pytorch/object-detection/run_object_detection.py#L160 </span></span>
<span id="cb124-41"><a href="#cb124-41" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb124-42"><a href="#cb124-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-43"><a href="#cb124-43" aria-hidden="true" tabindex="-1"></a>model_v1_results <span class="op">=</span> model_v1_trainer.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/daniel/miniconda3/envs/ai/lib/python3.11/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)</code></pre>
</div>
<div class="cell-output cell-output-display">

    <div>
      
      <progress value="1250" max="1250" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [1250/1250 04:25, Epoch 25/25]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>101.878300</td>
<td>7.513162</td>
</tr>
<tr class="even">
<td>2</td>
<td>4.145500</td>
<td>3.055572</td>
</tr>
<tr class="odd">
<td>3</td>
<td>2.596400</td>
<td>2.273679</td>
</tr>
<tr class="even">
<td>4</td>
<td>2.277300</td>
<td>2.069138</td>
</tr>
<tr class="odd">
<td>5</td>
<td>2.081800</td>
<td>1.849403</td>
</tr>
<tr class="even">
<td>6</td>
<td>1.925300</td>
<td>1.687234</td>
</tr>
<tr class="odd">
<td>7</td>
<td>1.780200</td>
<td>1.603322</td>
</tr>
<tr class="even">
<td>8</td>
<td>1.675000</td>
<td>1.451112</td>
</tr>
<tr class="odd">
<td>9</td>
<td>1.526300</td>
<td>1.409718</td>
</tr>
<tr class="even">
<td>10</td>
<td>1.432200</td>
<td>1.339651</td>
</tr>
<tr class="odd">
<td>11</td>
<td>1.386000</td>
<td>1.289711</td>
</tr>
<tr class="even">
<td>12</td>
<td>1.309800</td>
<td>1.281332</td>
</tr>
<tr class="odd">
<td>13</td>
<td>1.248000</td>
<td>1.209565</td>
</tr>
<tr class="even">
<td>14</td>
<td>1.209000</td>
<td>1.220024</td>
</tr>
<tr class="odd">
<td>15</td>
<td>1.175700</td>
<td>1.198685</td>
</tr>
<tr class="even">
<td>16</td>
<td>1.144000</td>
<td>1.175700</td>
</tr>
<tr class="odd">
<td>17</td>
<td>1.073200</td>
<td>1.193522</td>
</tr>
<tr class="even">
<td>18</td>
<td>1.050100</td>
<td>1.153087</td>
</tr>
<tr class="odd">
<td>19</td>
<td>0.986400</td>
<td>1.157631</td>
</tr>
<tr class="even">
<td>20</td>
<td>0.994100</td>
<td>1.151300</td>
</tr>
<tr class="odd">
<td>21</td>
<td>0.958900</td>
<td>1.144987</td>
</tr>
<tr class="even">
<td>22</td>
<td>0.927900</td>
<td>1.135496</td>
</tr>
<tr class="odd">
<td>23</td>
<td>0.907100</td>
<td>1.123257</td>
</tr>
<tr class="even">
<td>24</td>
<td>0.885100</td>
<td>1.133819</td>
</tr>
<tr class="odd">
<td>25</td>
<td>0.870900</td>
<td>1.130216</td>
</tr>
</tbody>
</table>
<p>
</p></div>
</div>
<p>TK - Note: May get an error at the beginning where a box is predicted a negative output. This will break training as boxes are expected to be positive floats.</p>
</section>
<section id="tk---make-predictions-on-the-test-dataset" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="tk---make-predictions-on-the-test-dataset"><span class="header-section-number">9</span> TK - Make predictions on the test dataset</h2>
<div id="cell-134" class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"test"</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="94">
<pre><code>{'pixel_values': tensor([[[-0.9705, -0.7308, -0.9705,  ..., -1.8953, -1.8268, -1.3130],
          [-1.2959, -0.9363, -0.3883,  ..., -1.8953, -1.7240, -0.5596],
          [-1.4843, -1.1418, -0.1999,  ..., -1.8782, -1.2788, -0.5424],
          ...,
          [ 1.3242,  1.3242,  1.3413,  ..., -0.6452, -0.2856, -0.9877],
          [ 1.3070,  1.3584,  1.4098,  ..., -0.8678,  0.0398, -0.4911],
          [ 1.2728,  1.3413,  1.4098,  ..., -0.9705,  0.1768, -0.1657]],
 
         [[-0.5476, -0.3550, -0.6527,  ..., -1.7031, -1.6155, -1.0903],
          [-0.8803, -0.5476, -0.0399,  ..., -1.6856, -1.5280, -0.3200],
          [-1.0728, -0.7402,  0.1527,  ..., -1.6506, -1.0553, -0.3025],
          ...,
          [-1.7031, -1.7031, -1.6856,  ..., -0.3901,  0.0301, -0.7227],
          [-1.7206, -1.6681, -1.6155,  ..., -0.6176,  0.3803, -0.1800],
          [-1.7556, -1.6856, -1.6155,  ..., -0.7052,  0.5203,  0.1527]],
 
         [[-1.0550, -0.7064, -0.8284,  ..., -1.6824, -1.5953, -1.0201],
          [-1.3861, -0.9504, -0.2881,  ..., -1.6999, -1.4559, -0.2532],
          [-1.6476, -1.1944, -0.1661,  ..., -1.6650, -1.0376, -0.2881],
          ...,
          [-1.2641, -1.2641, -1.2467,  ..., -0.9504, -0.8284, -1.2293],
          [-1.2816, -1.2293, -1.1770,  ..., -1.1596, -0.5321, -1.0027],
          [-1.3164, -1.2467, -1.1770,  ..., -1.3513, -0.4973, -0.8981]]]),
 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]]),
 'labels': {'size': tensor([640, 480]), 'image_id': tensor([61]), 'class_labels': tensor([4, 5, 1, 0]), 'boxes': tensor([[0.2104, 0.8563, 0.2855, 0.2720],
         [0.4194, 0.4927, 0.2398, 0.1785],
         [0.3610, 0.6227, 0.2706, 0.2330],
         [0.4974, 0.4785, 0.3829, 0.3820]]), 'area': tensor([23860.4043, 13150.1748, 19368.0898, 44929.9102]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}}</code></pre>
</div>
</div>
<div id="cell-135" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions with trainer containing trained model</span></span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>test_dataset_preds <span class="op">=</span> model_v1_trainer.predict(test_dataset<span class="op">=</span>processed_dataset[<span class="st">"test"</span>])</span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a><span class="co"># test_dataset_preds</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[39], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Make predictions with trainer containing trained model</span>
<span class="ansi-green-fg">----&gt; 2</span> test_dataset_preds <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">model_v1_trainer</span><span style="color:rgb(98,98,98)">.</span>predict(test_dataset<span style="color:rgb(98,98,98)">=</span>processed_dataset[<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">test</span><span style="color:rgb(175,0,0)">"</span>])
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># test_dataset_preds</span>

<span class="ansi-red-fg">NameError</span>: name 'model_v1_trainer' is not defined</pre>
</div>
</div>
</div>
<div id="cell-136" class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the logits</span></span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>test_pred_logits <span class="op">=</span> test_dataset_preds.predictions[<span class="dv">0</span>][<span class="dv">1</span>]</span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the boxes</span></span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a>test_pred_boxes <span class="op">=</span> test_dataset_preds.predictions[<span class="dv">0</span>][<span class="dv">2</span>]</span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-7"><a href="#cb129-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the label IDs</span></span>
<span id="cb129-8"><a href="#cb129-8" aria-hidden="true" tabindex="-1"></a>test_pred_label_ids <span class="op">=</span> test_dataset_preds.label_ids</span>
<span id="cb129-9"><a href="#cb129-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-10"><a href="#cb129-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Check shapes</span></span>
<span id="cb129-11"><a href="#cb129-11" aria-hidden="true" tabindex="-1"></a>test_pred_logits.shape, test_pred_boxes.shape, <span class="bu">len</span>(test_pred_label_ids)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="96">
<pre><code>((16, 300, 7), (16, 300, 4), 13)</code></pre>
</div>
</div>
<div id="cell-137" class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a random sample from the test preds</span></span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a>random_test_pred_index <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(processed_dataset[<span class="st">"test"</span>]))</span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Making predictions on test item with index: </span><span class="sc">{</span>random_test_pred_index<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-7"><a href="#cb131-7" aria-hidden="true" tabindex="-1"></a>random_test_sample <span class="op">=</span> processed_dataset[<span class="st">"test"</span>][random_test_pred_index]</span>
<span id="cb131-8"><a href="#cb131-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-9"><a href="#cb131-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Do a single forward pass with the model</span></span>
<span id="cb131-10"><a href="#cb131-10" aria-hidden="true" tabindex="-1"></a>random_test_sample_outputs <span class="op">=</span> model(pixel_values<span class="op">=</span>random_test_sample[<span class="st">"pixel_values"</span>].unsqueeze(<span class="dv">0</span>).to(<span class="st">"cuda"</span>), <span class="co"># model expects input [batch_size, color_channels, height, width]</span></span>
<span id="cb131-11"><a href="#cb131-11" aria-hidden="true" tabindex="-1"></a>                                   pixel_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb131-12"><a href="#cb131-12" aria-hidden="true" tabindex="-1"></a><span class="co"># random_test_sample_outputs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Making predictions on test item with index: 163
CPU times: user 51.5 ms, sys: 10.3 ms, total: 61.8 ms
Wall time: 63.1 ms</code></pre>
</div>
</div>
<div id="cell-138" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="co"># image_processor.preprocess?</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-139" class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a random sample from the test preds</span></span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>random_test_pred_index <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(processed_dataset[<span class="st">"test"</span>]))</span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Making predictions on test item with index: </span><span class="sc">{</span>random_test_pred_index<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a>random_test_sample <span class="op">=</span> processed_dataset[<span class="st">"test"</span>][random_test_pred_index]</span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-7"><a href="#cb134-7" aria-hidden="true" tabindex="-1"></a><span class="co"># # Do a single forward pass with the model</span></span>
<span id="cb134-8"><a href="#cb134-8" aria-hidden="true" tabindex="-1"></a>random_test_sample_outputs <span class="op">=</span> model(pixel_values<span class="op">=</span>random_test_sample[<span class="st">"pixel_values"</span>].unsqueeze(<span class="dv">0</span>).to(<span class="st">"cuda"</span>), <span class="co"># model expects input [batch_size, color_channels, height, width]</span></span>
<span id="cb134-9"><a href="#cb134-9" aria-hidden="true" tabindex="-1"></a>                                   pixel_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb134-10"><a href="#cb134-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-11"><a href="#cb134-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Post process a random item from test preds</span></span>
<span id="cb134-12"><a href="#cb134-12" aria-hidden="true" tabindex="-1"></a>random_test_sample_outputs_post_processed <span class="op">=</span> image_processor.post_process_object_detection(</span>
<span id="cb134-13"><a href="#cb134-13" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>random_test_sample_outputs,</span>
<span id="cb134-14"><a href="#cb134-14" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span><span class="fl">0.25</span>, <span class="co"># prediction probability threshold for boxes (note: boxes from an untrained model will likely be bad)</span></span>
<span id="cb134-15"><a href="#cb134-15" aria-hidden="true" tabindex="-1"></a>    target_sizes<span class="op">=</span>[random_test_sample[<span class="st">"labels"</span>][<span class="st">"orig_size"</span>]] <span class="co"># original input image size (or whichever target size you'd like), required to be same number of input items in a list</span></span>
<span id="cb134-16"><a href="#cb134-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb134-17"><a href="#cb134-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-18"><a href="#cb134-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the random sample test preds</span></span>
<span id="cb134-19"><a href="#cb134-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract scores, labels and boxes</span></span>
<span id="cb134-20"><a href="#cb134-20" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_scores <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"scores"</span>]</span>
<span id="cb134-21"><a href="#cb134-21" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_labels <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"labels"</span>]</span>
<span id="cb134-22"><a href="#cb134-22" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_boxes <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"boxes"</span>]</span>
<span id="cb134-23"><a href="#cb134-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-24"><a href="#cb134-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of labels to plot on the boxes </span></span>
<span id="cb134-25"><a href="#cb134-25" aria-hidden="true" tabindex="-1"></a>random_test_sample_labels_to_plot <span class="op">=</span> [<span class="ss">f"Pred: </span><span class="sc">{</span>id2label[label_pred.item()]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score_pred.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">)"</span> </span>
<span id="cb134-26"><a href="#cb134-26" aria-hidden="true" tabindex="-1"></a>                  <span class="cf">for</span> label_pred, score_pred <span class="kw">in</span> <span class="bu">zip</span>(random_test_sample_pred_labels, random_test_sample_pred_scores)]</span>
<span id="cb134-27"><a href="#cb134-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-28"><a href="#cb134-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Labels with scores: </span><span class="sc">{</span>random_test_sample_labels_to_plot<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb134-29"><a href="#cb134-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-30"><a href="#cb134-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted boxes on the random test image </span></span>
<span id="cb134-31"><a href="#cb134-31" aria-hidden="true" tabindex="-1"></a>to_pil_image(</span>
<span id="cb134-32"><a href="#cb134-32" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb134-33"><a href="#cb134-33" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>dataset[<span class="st">"test"</span>][random_test_pred_index][<span class="st">"image"</span>]),</span>
<span id="cb134-34"><a href="#cb134-34" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>random_test_sample_pred_boxes,</span>
<span id="cb134-35"><a href="#cb134-35" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>random_test_sample_labels_to_plot,</span>
<span id="cb134-36"><a href="#cb134-36" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">3</span></span>
<span id="cb134-37"><a href="#cb134-37" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb134-38"><a href="#cb134-38" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Making predictions on test item with index: 28
[INFO] Labels with scores: ['Pred: hand (0.4208)', 'Pred: trash (0.3352)']</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="99">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-71-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>TK - nice!!! these boxes look far better than our randomly predicted boxes with an untrained modelâ€¦</p>
<section id="tk---predict-on-image-from-filepath" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="tk---predict-on-image-from-filepath"><span class="header-section-number">9.1</span> TK - Predict on image from filepath</h3>
<div id="cell-142" class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pred on image from pathname</span></span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a>path_to_test_image_folder <span class="op">=</span> Path(<span class="st">"data/trashify_test_images"</span>)</span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a>test_image_filepaths <span class="op">=</span> <span class="bu">list</span>(path_to_test_image_folder.rglob(<span class="st">"*.jp*g"</span>))</span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true" tabindex="-1"></a>test_image_targ_filepath <span class="op">=</span> random.choice(test_image_filepaths)</span>
<span id="cb136-7"><a href="#cb136-7" aria-hidden="true" tabindex="-1"></a><span class="co"># test_image_targ_filepath = "data/trashify_test_images/IMG_6692.jpeg"</span></span>
<span id="cb136-8"><a href="#cb136-8" aria-hidden="true" tabindex="-1"></a>test_image_pil <span class="op">=</span> Image.<span class="bu">open</span>(test_image_targ_filepath)</span>
<span id="cb136-9"><a href="#cb136-9" aria-hidden="true" tabindex="-1"></a>test_image_preprocessed <span class="op">=</span> image_processor.preprocess(images<span class="op">=</span>test_image_pil,</span>
<span id="cb136-10"><a href="#cb136-10" aria-hidden="true" tabindex="-1"></a>                                                     return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb136-11"><a href="#cb136-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-12"><a href="#cb136-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_image_dimensions_from_pil(image: Image.Image) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb136-13"><a href="#cb136-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb136-14"><a href="#cb136-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Convert the dimensions of a PIL image to a PyTorch tensor in the order (height, width).</span></span>
<span id="cb136-15"><a href="#cb136-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-16"><a href="#cb136-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb136-17"><a href="#cb136-17" aria-hidden="true" tabindex="-1"></a><span class="co">        image (Image.Image): The input PIL image.</span></span>
<span id="cb136-18"><a href="#cb136-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-19"><a href="#cb136-19" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb136-20"><a href="#cb136-20" aria-hidden="true" tabindex="-1"></a><span class="co">        torch.Tensor: A tensor containing the height and width of the image.</span></span>
<span id="cb136-21"><a href="#cb136-21" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb136-22"><a href="#cb136-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get (width, height) of image (PIL.Image.size returns width, height)</span></span>
<span id="cb136-23"><a href="#cb136-23" aria-hidden="true" tabindex="-1"></a>    width, height <span class="op">=</span> image.size</span>
<span id="cb136-24"><a href="#cb136-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-25"><a href="#cb136-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to a tensor in the order (height, width)</span></span>
<span id="cb136-26"><a href="#cb136-26" aria-hidden="true" tabindex="-1"></a>    image_dimensions_tensor <span class="op">=</span> torch.tensor([height, width])</span>
<span id="cb136-27"><a href="#cb136-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-28"><a href="#cb136-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image_dimensions_tensor</span>
<span id="cb136-29"><a href="#cb136-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-30"><a href="#cb136-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Get image original size</span></span>
<span id="cb136-31"><a href="#cb136-31" aria-hidden="true" tabindex="-1"></a>test_image_size <span class="op">=</span> get_image_dimensions_from_pil(image<span class="op">=</span>test_image_pil)</span>
<span id="cb136-32"><a href="#cb136-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-33"><a href="#cb136-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the preprocessed image</span></span>
<span id="cb136-34"><a href="#cb136-34" aria-hidden="true" tabindex="-1"></a>random_test_sample_outputs <span class="op">=</span> model(pixel_values<span class="op">=</span>test_image_preprocessed[<span class="st">"pixel_values"</span>].to(<span class="st">"cuda"</span>), <span class="co"># model expects input [batch_size, color_channels, height, width]</span></span>
<span id="cb136-35"><a href="#cb136-35" aria-hidden="true" tabindex="-1"></a>                                   pixel_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb136-36"><a href="#cb136-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-37"><a href="#cb136-37" aria-hidden="true" tabindex="-1"></a>THRESHOLD <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb136-38"><a href="#cb136-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-39"><a href="#cb136-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Post process the predictions</span></span>
<span id="cb136-40"><a href="#cb136-40" aria-hidden="true" tabindex="-1"></a>random_test_sample_outputs_post_processed <span class="op">=</span> image_processor.post_process_object_detection(</span>
<span id="cb136-41"><a href="#cb136-41" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>random_test_sample_outputs,</span>
<span id="cb136-42"><a href="#cb136-42" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span>THRESHOLD,</span>
<span id="cb136-43"><a href="#cb136-43" aria-hidden="true" tabindex="-1"></a>    target_sizes<span class="op">=</span>[test_image_size] <span class="co"># needs to be same length as batch dimension of the logits (e.g. [[height, width]])</span></span>
<span id="cb136-44"><a href="#cb136-44" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb136-45"><a href="#cb136-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-46"><a href="#cb136-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract scores, labels and boxes</span></span>
<span id="cb136-47"><a href="#cb136-47" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_scores <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"scores"</span>]</span>
<span id="cb136-48"><a href="#cb136-48" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_labels <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"labels"</span>]</span>
<span id="cb136-49"><a href="#cb136-49" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_boxes <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"boxes"</span>]</span>
<span id="cb136-50"><a href="#cb136-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-51"><a href="#cb136-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a lsit of labels to plot on the boxes </span></span>
<span id="cb136-52"><a href="#cb136-52" aria-hidden="true" tabindex="-1"></a>random_test_sample_labels_to_plot <span class="op">=</span> [<span class="ss">f"Pred: </span><span class="sc">{</span>id2label[label_pred.item()]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score_pred.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb136-53"><a href="#cb136-53" aria-hidden="true" tabindex="-1"></a>                                     <span class="cf">for</span> label_pred, score_pred <span class="kw">in</span> <span class="bu">zip</span>(random_test_sample_pred_labels, random_test_sample_pred_scores)]</span>
<span id="cb136-54"><a href="#cb136-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-55"><a href="#cb136-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"[INFO] Labels with scores:"</span>)</span>
<span id="cb136-56"><a href="#cb136-56" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> item <span class="kw">in</span> random_test_sample_labels_to_plot:</span>
<span id="cb136-57"><a href="#cb136-57" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(item)</span>
<span id="cb136-58"><a href="#cb136-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-59"><a href="#cb136-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted boxes on the random test image </span></span>
<span id="cb136-60"><a href="#cb136-60" aria-hidden="true" tabindex="-1"></a>to_pil_image(</span>
<span id="cb136-61"><a href="#cb136-61" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb136-62"><a href="#cb136-62" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>test_image_pil),                    </span>
<span id="cb136-63"><a href="#cb136-63" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>random_test_sample_pred_boxes,</span>
<span id="cb136-64"><a href="#cb136-64" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>random_test_sample_labels_to_plot,</span>
<span id="cb136-65"><a href="#cb136-65" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">3</span></span>
<span id="cb136-66"><a href="#cb136-66" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb136-67"><a href="#cb136-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb136-68"><a href="#cb136-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-69"><a href="#cb136-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-70"><a href="#cb136-70" aria-hidden="true" tabindex="-1"></a><span class="co"># # Plot the random sample image with randomly predicted boxes (these will be very poor since the model is not trained on our data yet)</span></span>
<span id="cb136-71"><a href="#cb136-71" aria-hidden="true" tabindex="-1"></a><span class="co"># to_pil_image(</span></span>
<span id="cb136-72"><a href="#cb136-72" aria-hidden="true" tabindex="-1"></a><span class="co">#     pic=draw_bounding_boxes(</span></span>
<span id="cb136-73"><a href="#cb136-73" aria-hidden="true" tabindex="-1"></a><span class="co">#         image=pil_to_tensor(pic=dataset["test"][random_test_pred_index]["image"]),</span></span>
<span id="cb136-74"><a href="#cb136-74" aria-hidden="true" tabindex="-1"></a><span class="co">#         boxes=random_test_sample_pred_boxes,</span></span>
<span id="cb136-75"><a href="#cb136-75" aria-hidden="true" tabindex="-1"></a><span class="co">#         labels=random_test_sample_labels_to_plot,</span></span>
<span id="cb136-76"><a href="#cb136-76" aria-hidden="true" tabindex="-1"></a><span class="co">#         width=3</span></span>
<span id="cb136-77"><a href="#cb136-77" aria-hidden="true" tabindex="-1"></a><span class="co">#     )</span></span>
<span id="cb136-78"><a href="#cb136-78" aria-hidden="true" tabindex="-1"></a><span class="co"># )</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Labels with scores:
Pred: trash (0.7138)
Pred: bin (0.699)
Pred: hand (0.6244)
Pred: bin (0.6231)
Pred: not_trash (0.4189)
Pred: bin (0.2655)
Pred: hand (0.2617)
Pred: not_trash (0.2392)
Pred: not_trash (0.2335)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="100">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-72-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="tk---upload-our-trained-model-to-hugging-face-hub" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="tk---upload-our-trained-model-to-hugging-face-hub"><span class="header-section-number">10</span> TK - Upload our trained model to Hugging Face Hub</h2>
<p>TK - Letâ€™s make our model available for others to use.</p>
<div id="cell-144" class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UPTOHERE</span></span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Make extensions to make the model better... (e.g. data augmentation = harder training set = better overall validation loss)</span></span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Model with data augmentation</span></span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Model with longer training (e.g. 100 epochs) </span></span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Research eval_do_concat_batches=False/True &amp; see what the results do...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-145" class="cell" data-execution_count="102">
<div class="sourceCode cell-code" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime</span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-4"><a href="#cb139-4" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co">: update this save path so we know when the model was saved and what its parameters were</span></span>
<span id="cb139-5"><a href="#cb139-5" aria-hidden="true" tabindex="-1"></a>training_epochs_ <span class="op">=</span> training_args.num_train_epochs</span>
<span id="cb139-6"><a href="#cb139-6" aria-hidden="true" tabindex="-1"></a>learning_rate_ <span class="op">=</span> <span class="st">"</span><span class="sc">{:.0e}</span><span class="st">"</span>.<span class="bu">format</span>(training_args.learning_rate)</span>
<span id="cb139-7"><a href="#cb139-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-8"><a href="#cb139-8" aria-hidden="true" tabindex="-1"></a>model_save_path <span class="op">=</span> <span class="ss">f"models/learn_hf_microsoft_detr_finetuned_trashify_box_dataset_only_manual_data_no_aug_</span><span class="sc">{</span>training_epochs_<span class="sc">}</span><span class="ss">_epochs_lr_</span><span class="sc">{</span>learning_rate_<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb139-9"><a href="#cb139-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Saving model to: </span><span class="sc">{</span>model_save_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb139-10"><a href="#cb139-10" aria-hidden="true" tabindex="-1"></a>model_v1_trainer.save_model(model_save_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Saving model to: models/learn_hf_microsoft_detr_finetuned_trashify_box_dataset_only_manual_data_no_aug_25_epochs_lr_1e-04</code></pre>
</div>
</div>
<div id="cell-146" class="cell" data-execution_count="103">
<div class="sourceCode cell-code" id="cb141"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Push the model to the hub</span></span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: this will require you to have your Hugging Face account setup </span></span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>model_v1_trainer.push_to_hub(commit_message<span class="op">=</span><span class="st">"upload trashify object detection model"</span>,</span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># token=None # Optional to add a token manually</span></span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a>                    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a7fff4958f1046088387ea08ef67f7ae","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3697686114864827ae609ce445e1b916","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"15fb1db183b9482c97190244ab07ccd3","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="103">
<pre><code>CommitInfo(commit_url='https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector/commit/ab273cec67e5124ac047dc1e068c379c718e6c37', commit_message='upload trashify object detection model', commit_description='', oid='ab273cec67e5124ac047dc1e068c379c718e6c37', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector', endpoint='https://huggingface.co', repo_type='model', repo_id='mrdbourke/detr_finetuned_trashify_box_detector'), pr_revision=None, pr_num=None)</code></pre>
</div>
</div>
</section>
<section id="creating-a-demo-of-our-model-with-gradio" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="creating-a-demo-of-our-model-with-gradio"><span class="header-section-number">11</span> Creating a demo of our model with Gradio</h2>
<div id="cell-148" class="cell" data-execution_count="104">
<div class="sourceCode cell-code" id="cb143"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile demos<span class="op">/</span>trashify_object_detector<span class="op">/</span>README.md</span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a><span class="op">---</span></span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a>title: Trashify Demo V1 ðŸš®</span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a>emoji: ðŸ—‘ï¸</span>
<span id="cb143-5"><a href="#cb143-5" aria-hidden="true" tabindex="-1"></a>colorFrom: purple</span>
<span id="cb143-6"><a href="#cb143-6" aria-hidden="true" tabindex="-1"></a>colorTo: blue</span>
<span id="cb143-7"><a href="#cb143-7" aria-hidden="true" tabindex="-1"></a>sdk: gradio</span>
<span id="cb143-8"><a href="#cb143-8" aria-hidden="true" tabindex="-1"></a>sdk_version: <span class="fl">4.40.0</span></span>
<span id="cb143-9"><a href="#cb143-9" aria-hidden="true" tabindex="-1"></a>app_file: app.py</span>
<span id="cb143-10"><a href="#cb143-10" aria-hidden="true" tabindex="-1"></a>pinned: false</span>
<span id="cb143-11"><a href="#cb143-11" aria-hidden="true" tabindex="-1"></a>license: apache<span class="op">-</span><span class="fl">2.0</span></span>
<span id="cb143-12"><a href="#cb143-12" aria-hidden="true" tabindex="-1"></a><span class="op">---</span></span>
<span id="cb143-13"><a href="#cb143-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-14"><a href="#cb143-14" aria-hidden="true" tabindex="-1"></a><span class="co"># ðŸš® Trashify Object Detector V1 </span></span>
<span id="cb143-15"><a href="#cb143-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-16"><a href="#cb143-16" aria-hidden="true" tabindex="-1"></a>Object detection demo to detect `trash`, `bin`, `hand`, `trash_arm`, `not_trash`, `not_bin`, `not_hand`. </span>
<span id="cb143-17"><a href="#cb143-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-18"><a href="#cb143-18" aria-hidden="true" tabindex="-1"></a>Used <span class="im">as</span> example <span class="cf">for</span> encouraging people to cleanup their local area.</span>
<span id="cb143-19"><a href="#cb143-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-20"><a href="#cb143-20" aria-hidden="true" tabindex="-1"></a>If `trash`, `hand`, `bin` <span class="bu">all</span> detected <span class="op">=</span> <span class="op">+</span><span class="dv">1</span> point.</span>
<span id="cb143-21"><a href="#cb143-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-22"><a href="#cb143-22" aria-hidden="true" tabindex="-1"></a><span class="co">## Dataset</span></span>
<span id="cb143-23"><a href="#cb143-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-24"><a href="#cb143-24" aria-hidden="true" tabindex="-1"></a>All Trashify models are trained on a custom hand<span class="op">-</span>labelled dataset of people picking up trash <span class="kw">and</span> placing it <span class="kw">in</span> a <span class="bu">bin</span>.</span>
<span id="cb143-25"><a href="#cb143-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-26"><a href="#cb143-26" aria-hidden="true" tabindex="-1"></a>The dataset can be found on Hugging Face <span class="im">as</span> [`mrdbourke<span class="op">/</span>trashify_manual_labelled_images`](https:<span class="op">//</span>huggingface.co<span class="op">/</span>datasets<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_manual_labelled_images).</span>
<span id="cb143-27"><a href="#cb143-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-28"><a href="#cb143-28" aria-hidden="true" tabindex="-1"></a><span class="co">## Demos</span></span>
<span id="cb143-29"><a href="#cb143-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-30"><a href="#cb143-30" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> [V1](https:<span class="op">//</span>huggingface.co<span class="op">/</span>spaces<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_demo_v1) <span class="op">=</span> Fine<span class="op">-</span>tuned DETR model trained <span class="op">*</span>without<span class="op">*</span> data augmentation.</span>
<span id="cb143-31"><a href="#cb143-31" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> [V2](https:<span class="op">//</span>huggingface.co<span class="op">/</span>spaces<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_demo_v2) <span class="op">=</span> Fine<span class="op">-</span>tuned DETR model trained <span class="op">*</span><span class="cf">with</span><span class="op">*</span> data augmentation.</span>
<span id="cb143-32"><a href="#cb143-32" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> [V3](https:<span class="op">//</span>huggingface.co<span class="op">/</span>spaces<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_demo_v3) <span class="op">=</span> Fine<span class="op">-</span>tuned DETR model trained <span class="op">*</span><span class="cf">with</span><span class="op">*</span> data augmentation (same <span class="im">as</span> V2) <span class="cf">with</span> an NMS (Non Maximum Suppression) post<span class="op">-</span>processing step.</span>
<span id="cb143-33"><a href="#cb143-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-34"><a href="#cb143-34" aria-hidden="true" tabindex="-1"></a>TK <span class="op">-</span> add links to resources to learn more</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting demos/trashify_object_detector/README.md</code></pre>
</div>
</div>
<div id="cell-149" class="cell" data-execution_count="105">
<div class="sourceCode cell-code" id="cb145"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile demos<span class="op">/</span>trashify_object_detector<span class="op">/</span>requirements.txt</span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a>timm</span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a>gradio</span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a>torch</span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a>transformers</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting demos/trashify_object_detector/requirements.txt</code></pre>
</div>
</div>
<div id="cell-150" class="cell" data-execution_count="152">
<div class="sourceCode cell-code" id="cb147"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile demos<span class="op">/</span>trashify_object_detector<span class="op">/</span>app.py</span>
<span id="cb147-2"><a href="#cb147-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb147-3"><a href="#cb147-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb147-4"><a href="#cb147-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image, ImageDraw, ImageFont</span>
<span id="cb147-5"><a href="#cb147-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-6"><a href="#cb147-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoImageProcessor</span>
<span id="cb147-7"><a href="#cb147-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForObjectDetection</span>
<span id="cb147-8"><a href="#cb147-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-9"><a href="#cb147-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Can load from Hugging Face or can load from local </span></span>
<span id="cb147-10"><a href="#cb147-10" aria-hidden="true" tabindex="-1"></a>model_save_path <span class="op">=</span> <span class="st">"mrdbourke/detr_finetuned_trashify_box_detector"</span></span>
<span id="cb147-11"><a href="#cb147-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-12"><a href="#cb147-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model and preprocessor</span></span>
<span id="cb147-13"><a href="#cb147-13" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoImageProcessor.from_pretrained(model_save_path)</span>
<span id="cb147-14"><a href="#cb147-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForObjectDetection.from_pretrained(model_save_path)</span>
<span id="cb147-15"><a href="#cb147-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-16"><a href="#cb147-16" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb147-17"><a href="#cb147-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb147-18"><a href="#cb147-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-19"><a href="#cb147-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the id2label dictionary from the model</span></span>
<span id="cb147-20"><a href="#cb147-20" aria-hidden="true" tabindex="-1"></a>id2label <span class="op">=</span> model.config.id2label</span>
<span id="cb147-21"><a href="#cb147-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-22"><a href="#cb147-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up a colour dictionary for plotting boxes with different colours</span></span>
<span id="cb147-23"><a href="#cb147-23" aria-hidden="true" tabindex="-1"></a>color_dict <span class="op">=</span> {   </span>
<span id="cb147-24"><a href="#cb147-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">"bin"</span>: <span class="st">"green"</span>,</span>
<span id="cb147-25"><a href="#cb147-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">"trash"</span>: <span class="st">"blue"</span>,</span>
<span id="cb147-26"><a href="#cb147-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">"hand"</span>: <span class="st">"purple"</span>,</span>
<span id="cb147-27"><a href="#cb147-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">"trash_arm"</span>: <span class="st">"yellow"</span>,</span>
<span id="cb147-28"><a href="#cb147-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not_trash"</span>: <span class="st">"red"</span>,</span>
<span id="cb147-29"><a href="#cb147-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not_bin"</span>: <span class="st">"red"</span>,</span>
<span id="cb147-30"><a href="#cb147-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not_hand"</span>: <span class="st">"red"</span>,</span>
<span id="cb147-31"><a href="#cb147-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb147-32"><a href="#cb147-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-33"><a href="#cb147-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Create helper functions for seeing if items from one list are in another </span></span>
<span id="cb147-34"><a href="#cb147-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> any_in_list(list_a, list_b):</span>
<span id="cb147-35"><a href="#cb147-35" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Returns True if any item from list_a is in list_b, otherwise False."</span></span>
<span id="cb147-36"><a href="#cb147-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">any</span>(item <span class="kw">in</span> list_b <span class="cf">for</span> item <span class="kw">in</span> list_a)</span>
<span id="cb147-37"><a href="#cb147-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-38"><a href="#cb147-38" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> all_in_list(list_a, list_b):</span>
<span id="cb147-39"><a href="#cb147-39" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Returns True if all items from list_a are in list_b, otherwise False."</span></span>
<span id="cb147-40"><a href="#cb147-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">all</span>(item <span class="kw">in</span> list_b <span class="cf">for</span> item <span class="kw">in</span> list_a)</span>
<span id="cb147-41"><a href="#cb147-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-42"><a href="#cb147-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_on_image(image, conf_threshold):</span>
<span id="cb147-43"><a href="#cb147-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb147-44"><a href="#cb147-44" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> image_processor(images<span class="op">=</span>[image], return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb147-45"><a href="#cb147-45" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>inputs.to(device))</span>
<span id="cb147-46"><a href="#cb147-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-47"><a href="#cb147-47" aria-hidden="true" tabindex="-1"></a>        target_sizes <span class="op">=</span> torch.tensor([[image.size[<span class="dv">1</span>], image.size[<span class="dv">0</span>]]]) <span class="co"># height, width </span></span>
<span id="cb147-48"><a href="#cb147-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-49"><a href="#cb147-49" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> image_processor.post_process_object_detection(outputs,</span>
<span id="cb147-50"><a href="#cb147-50" aria-hidden="true" tabindex="-1"></a>                                                                threshold<span class="op">=</span>conf_threshold,</span>
<span id="cb147-51"><a href="#cb147-51" aria-hidden="true" tabindex="-1"></a>                                                                target_sizes<span class="op">=</span>target_sizes)[<span class="dv">0</span>]</span>
<span id="cb147-52"><a href="#cb147-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return all items in results to CPU</span></span>
<span id="cb147-53"><a href="#cb147-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> key, value <span class="kw">in</span> results.items():</span>
<span id="cb147-54"><a href="#cb147-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb147-55"><a href="#cb147-55" aria-hidden="true" tabindex="-1"></a>            results[key] <span class="op">=</span> value.item().cpu() <span class="co"># can't get scalar as .item() so add try/except block</span></span>
<span id="cb147-56"><a href="#cb147-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span>:</span>
<span id="cb147-57"><a href="#cb147-57" aria-hidden="true" tabindex="-1"></a>            results[key] <span class="op">=</span> value.cpu()</span>
<span id="cb147-58"><a href="#cb147-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-59"><a href="#cb147-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Can return results as plotted on a PIL image (then display the image)</span></span>
<span id="cb147-60"><a href="#cb147-60" aria-hidden="true" tabindex="-1"></a>    draw <span class="op">=</span> ImageDraw.Draw(image)</span>
<span id="cb147-61"><a href="#cb147-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-62"><a href="#cb147-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get a font from ImageFont</span></span>
<span id="cb147-63"><a href="#cb147-63" aria-hidden="true" tabindex="-1"></a>    font <span class="op">=</span> ImageFont.load_default(size<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb147-64"><a href="#cb147-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-65"><a href="#cb147-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get class names as text for print out</span></span>
<span id="cb147-66"><a href="#cb147-66" aria-hidden="true" tabindex="-1"></a>    class_name_text_labels <span class="op">=</span> []</span>
<span id="cb147-67"><a href="#cb147-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-68"><a href="#cb147-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> box, score, label <span class="kw">in</span> <span class="bu">zip</span>(results[<span class="st">"boxes"</span>], results[<span class="st">"scores"</span>], results[<span class="st">"labels"</span>]):</span>
<span id="cb147-69"><a href="#cb147-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create coordinates</span></span>
<span id="cb147-70"><a href="#cb147-70" aria-hidden="true" tabindex="-1"></a>        x, y, x2, y2 <span class="op">=</span> <span class="bu">tuple</span>(box.tolist())</span>
<span id="cb147-71"><a href="#cb147-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-72"><a href="#cb147-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get label_name</span></span>
<span id="cb147-73"><a href="#cb147-73" aria-hidden="true" tabindex="-1"></a>        label_name <span class="op">=</span> id2label[label.item()]</span>
<span id="cb147-74"><a href="#cb147-74" aria-hidden="true" tabindex="-1"></a>        targ_color <span class="op">=</span> color_dict[label_name]</span>
<span id="cb147-75"><a href="#cb147-75" aria-hidden="true" tabindex="-1"></a>        class_name_text_labels.append(label_name)</span>
<span id="cb147-76"><a href="#cb147-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-77"><a href="#cb147-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw the rectangle</span></span>
<span id="cb147-78"><a href="#cb147-78" aria-hidden="true" tabindex="-1"></a>        draw.rectangle(xy<span class="op">=</span>(x, y, x2, y2), </span>
<span id="cb147-79"><a href="#cb147-79" aria-hidden="true" tabindex="-1"></a>                       outline<span class="op">=</span>targ_color,</span>
<span id="cb147-80"><a href="#cb147-80" aria-hidden="true" tabindex="-1"></a>                       width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb147-81"><a href="#cb147-81" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb147-82"><a href="#cb147-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a text string to display</span></span>
<span id="cb147-83"><a href="#cb147-83" aria-hidden="true" tabindex="-1"></a>        text_string_to_show <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>label_name<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score.item(), <span class="dv">3</span>)<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb147-84"><a href="#cb147-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-85"><a href="#cb147-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw the text on the image</span></span>
<span id="cb147-86"><a href="#cb147-86" aria-hidden="true" tabindex="-1"></a>        draw.text(xy<span class="op">=</span>(x, y),</span>
<span id="cb147-87"><a href="#cb147-87" aria-hidden="true" tabindex="-1"></a>                  text<span class="op">=</span>text_string_to_show,</span>
<span id="cb147-88"><a href="#cb147-88" aria-hidden="true" tabindex="-1"></a>                  fill<span class="op">=</span><span class="st">"white"</span>,</span>
<span id="cb147-89"><a href="#cb147-89" aria-hidden="true" tabindex="-1"></a>                  font<span class="op">=</span>font)</span>
<span id="cb147-90"><a href="#cb147-90" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb147-91"><a href="#cb147-91" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove the draw each time</span></span>
<span id="cb147-92"><a href="#cb147-92" aria-hidden="true" tabindex="-1"></a>    <span class="kw">del</span> draw</span>
<span id="cb147-93"><a href="#cb147-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-94"><a href="#cb147-94" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setup blank string to print out</span></span>
<span id="cb147-95"><a href="#cb147-95" aria-hidden="true" tabindex="-1"></a>    return_string <span class="op">=</span> <span class="st">""</span></span>
<span id="cb147-96"><a href="#cb147-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-97"><a href="#cb147-97" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setup list of target items to discover</span></span>
<span id="cb147-98"><a href="#cb147-98" aria-hidden="true" tabindex="-1"></a>    target_items <span class="op">=</span> [<span class="st">"trash"</span>, <span class="st">"bin"</span>, <span class="st">"hand"</span>]</span>
<span id="cb147-99"><a href="#cb147-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-100"><a href="#cb147-100" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If no items detected or trash, bin, hand not in list, return notification </span></span>
<span id="cb147-101"><a href="#cb147-101" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="bu">len</span>(class_name_text_labels) <span class="op">==</span> <span class="dv">0</span>) <span class="kw">or</span> <span class="kw">not</span> (any_in_list(list_a<span class="op">=</span>target_items, list_b<span class="op">=</span>class_name_text_labels)):</span>
<span id="cb147-102"><a href="#cb147-102" aria-hidden="true" tabindex="-1"></a>        return_string <span class="op">=</span> <span class="ss">f"No trash, bin or hand detected at confidence threshold </span><span class="sc">{</span>conf_threshold<span class="sc">}</span><span class="ss">. Try another image or lowering the confidence threshold."</span></span>
<span id="cb147-103"><a href="#cb147-103" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, return_string</span>
<span id="cb147-104"><a href="#cb147-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-105"><a href="#cb147-105" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If there are some missing, print the ones which are missing</span></span>
<span id="cb147-106"><a href="#cb147-106" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="kw">not</span> all_in_list(list_a<span class="op">=</span>target_items, list_b<span class="op">=</span>class_name_text_labels):</span>
<span id="cb147-107"><a href="#cb147-107" aria-hidden="true" tabindex="-1"></a>        missing_items <span class="op">=</span> []</span>
<span id="cb147-108"><a href="#cb147-108" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> item <span class="kw">in</span> target_items:</span>
<span id="cb147-109"><a href="#cb147-109" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> item <span class="kw">not</span> <span class="kw">in</span> class_name_text_labels:</span>
<span id="cb147-110"><a href="#cb147-110" aria-hidden="true" tabindex="-1"></a>                missing_items.append(item)</span>
<span id="cb147-111"><a href="#cb147-111" aria-hidden="true" tabindex="-1"></a>        return_string <span class="op">=</span> <span class="ss">f"Detected the following items: </span><span class="sc">{</span>class_name_text_labels<span class="sc">}</span><span class="ss">. But missing the following in order to get +1: </span><span class="sc">{</span>missing_items<span class="sc">}</span><span class="ss">. If this is an error, try another image or altering the confidence threshold. Otherwise, the model may need to be updated with better data."</span></span>
<span id="cb147-112"><a href="#cb147-112" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb147-113"><a href="#cb147-113" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If all 3 trash, bin, hand occur = + 1</span></span>
<span id="cb147-114"><a href="#cb147-114" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> all_in_list(list_a<span class="op">=</span>target_items, list_b<span class="op">=</span>class_name_text_labels):</span>
<span id="cb147-115"><a href="#cb147-115" aria-hidden="true" tabindex="-1"></a>        return_string <span class="op">=</span> <span class="ss">f"+1! Found the following items: </span><span class="sc">{</span>class_name_text_labels<span class="sc">}</span><span class="ss">, thank you for cleaning up the area!"</span></span>
<span id="cb147-116"><a href="#cb147-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-117"><a href="#cb147-117" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(return_string)</span>
<span id="cb147-118"><a href="#cb147-118" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb147-119"><a href="#cb147-119" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image, return_string</span>
<span id="cb147-120"><a href="#cb147-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-121"><a href="#cb147-121" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the interface</span></span>
<span id="cb147-122"><a href="#cb147-122" aria-hidden="true" tabindex="-1"></a>demo <span class="op">=</span> gr.Interface(</span>
<span id="cb147-123"><a href="#cb147-123" aria-hidden="true" tabindex="-1"></a>    fn<span class="op">=</span>predict_on_image,</span>
<span id="cb147-124"><a href="#cb147-124" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>[</span>
<span id="cb147-125"><a href="#cb147-125" aria-hidden="true" tabindex="-1"></a>        gr.Image(<span class="bu">type</span><span class="op">=</span><span class="st">"pil"</span>, label<span class="op">=</span><span class="st">"Target Image"</span>),</span>
<span id="cb147-126"><a href="#cb147-126" aria-hidden="true" tabindex="-1"></a>        gr.Slider(minimum<span class="op">=</span><span class="dv">0</span>, maximum<span class="op">=</span><span class="dv">1</span>, value<span class="op">=</span><span class="fl">0.25</span>, label<span class="op">=</span><span class="st">"Confidence Threshold"</span>)</span>
<span id="cb147-127"><a href="#cb147-127" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb147-128"><a href="#cb147-128" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>[</span>
<span id="cb147-129"><a href="#cb147-129" aria-hidden="true" tabindex="-1"></a>        gr.Image(<span class="bu">type</span><span class="op">=</span><span class="st">"pil"</span>, label<span class="op">=</span><span class="st">"Image Output"</span>),</span>
<span id="cb147-130"><a href="#cb147-130" aria-hidden="true" tabindex="-1"></a>        gr.Text(label<span class="op">=</span><span class="st">"Text Output"</span>)</span>
<span id="cb147-131"><a href="#cb147-131" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb147-132"><a href="#cb147-132" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"ðŸš® Trashify Object Detection Demo V1"</span>,</span>
<span id="cb147-133"><a href="#cb147-133" aria-hidden="true" tabindex="-1"></a>    description<span class="op">=</span><span class="st">"Help clean up your local area! Upload an image and get +1 if there is all of the following items detected: trash, bin, hand."</span>,</span>
<span id="cb147-134"><a href="#cb147-134" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Examples come in the form of a list of lists, where each inner list contains elements to prefill the `inputs` parameter with</span></span>
<span id="cb147-135"><a href="#cb147-135" aria-hidden="true" tabindex="-1"></a>    examples<span class="op">=</span>[</span>
<span id="cb147-136"><a href="#cb147-136" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"examples/trashify_example_1.jpeg"</span>, <span class="fl">0.25</span>],</span>
<span id="cb147-137"><a href="#cb147-137" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"examples/trashify_example_2.jpeg"</span>, <span class="fl">0.25</span>],</span>
<span id="cb147-138"><a href="#cb147-138" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"examples/trashify_example_3.jpeg"</span>, <span class="fl">0.25</span>],</span>
<span id="cb147-139"><a href="#cb147-139" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb147-140"><a href="#cb147-140" aria-hidden="true" tabindex="-1"></a>    cache_examples<span class="op">=</span><span class="va">True</span></span>
<span id="cb147-141"><a href="#cb147-141" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb147-142"><a href="#cb147-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-143"><a href="#cb147-143" aria-hidden="true" tabindex="-1"></a><span class="co"># Launch the demo</span></span>
<span id="cb147-144"><a href="#cb147-144" aria-hidden="true" tabindex="-1"></a>demo.launch()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting demos/trashify_object_detector/app.py</code></pre>
</div>
</div>
<section id="tk---upload-demo-to-hugging-face-spaces-to-get-it-live" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="tk---upload-demo-to-hugging-face-spaces-to-get-it-live"><span class="header-section-number">11.1</span> TK - Upload demo to Hugging Face Spaces to get it live</h3>
<div id="cell-152" class="cell" data-execution_count="153">
<div class="sourceCode cell-code" id="cb149"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Import the required methods for uploading to the Hugging Face Hub</span></span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> (</span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a>    create_repo,</span>
<span id="cb149-4"><a href="#cb149-4" aria-hidden="true" tabindex="-1"></a>    get_full_repo_name,</span>
<span id="cb149-5"><a href="#cb149-5" aria-hidden="true" tabindex="-1"></a>    upload_file, <span class="co"># for uploading a single file (if necessary)</span></span>
<span id="cb149-6"><a href="#cb149-6" aria-hidden="true" tabindex="-1"></a>    upload_folder <span class="co"># for uploading multiple files (in a folder)</span></span>
<span id="cb149-7"><a href="#cb149-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb149-8"><a href="#cb149-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-9"><a href="#cb149-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Define the parameters we'd like to use for the upload</span></span>
<span id="cb149-10"><a href="#cb149-10" aria-hidden="true" tabindex="-1"></a>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD <span class="op">=</span> <span class="st">"demos/trashify_object_detector"</span> <span class="co"># TK - update this path </span></span>
<span id="cb149-11"><a href="#cb149-11" aria-hidden="true" tabindex="-1"></a>HF_TARGET_SPACE_NAME <span class="op">=</span> <span class="st">"trashify_demo_v1"</span></span>
<span id="cb149-12"><a href="#cb149-12" aria-hidden="true" tabindex="-1"></a>HF_REPO_TYPE <span class="op">=</span> <span class="st">"space"</span> <span class="co"># we're creating a Hugging Face Space</span></span>
<span id="cb149-13"><a href="#cb149-13" aria-hidden="true" tabindex="-1"></a>HF_SPACE_SDK <span class="op">=</span> <span class="st">"gradio"</span></span>
<span id="cb149-14"><a href="#cb149-14" aria-hidden="true" tabindex="-1"></a>HF_TOKEN <span class="op">=</span> <span class="st">""</span> <span class="co"># optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)</span></span>
<span id="cb149-15"><a href="#cb149-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-16"><a href="#cb149-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Create a Space repository on Hugging Face Hub </span></span>
<span id="cb149-17"><a href="#cb149-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Creating repo on Hugging Face Hub with name: </span><span class="sc">{</span>HF_TARGET_SPACE_NAME<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb149-18"><a href="#cb149-18" aria-hidden="true" tabindex="-1"></a>create_repo(</span>
<span id="cb149-19"><a href="#cb149-19" aria-hidden="true" tabindex="-1"></a>    repo_id<span class="op">=</span>HF_TARGET_SPACE_NAME,</span>
<span id="cb149-20"><a href="#cb149-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)</span></span>
<span id="cb149-21"><a href="#cb149-21" aria-hidden="true" tabindex="-1"></a>    repo_type<span class="op">=</span>HF_REPO_TYPE,</span>
<span id="cb149-22"><a href="#cb149-22" aria-hidden="true" tabindex="-1"></a>    private<span class="op">=</span><span class="va">False</span>, <span class="co"># set to True if you don't want your Space to be accessible to others</span></span>
<span id="cb149-23"><a href="#cb149-23" aria-hidden="true" tabindex="-1"></a>    space_sdk<span class="op">=</span>HF_SPACE_SDK,</span>
<span id="cb149-24"><a href="#cb149-24" aria-hidden="true" tabindex="-1"></a>    exist_ok<span class="op">=</span><span class="va">True</span>, <span class="co"># set to False if you want an error to raise if the repo_id already exists </span></span>
<span id="cb149-25"><a href="#cb149-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb149-26"><a href="#cb149-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-27"><a href="#cb149-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})</span></span>
<span id="cb149-28"><a href="#cb149-28" aria-hidden="true" tabindex="-1"></a>full_hf_repo_name <span class="op">=</span> get_full_repo_name(model_id<span class="op">=</span>HF_TARGET_SPACE_NAME)</span>
<span id="cb149-29"><a href="#cb149-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Full Hugging Face Hub repo name: </span><span class="sc">{</span>full_hf_repo_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb149-30"><a href="#cb149-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-31"><a href="#cb149-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Upload our demo folder</span></span>
<span id="cb149-32"><a href="#cb149-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Uploading </span><span class="sc">{</span>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD<span class="sc">}</span><span class="ss"> to repo: </span><span class="sc">{</span>full_hf_repo_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb149-33"><a href="#cb149-33" aria-hidden="true" tabindex="-1"></a>folder_upload_url <span class="op">=</span> upload_folder(</span>
<span id="cb149-34"><a href="#cb149-34" aria-hidden="true" tabindex="-1"></a>    repo_id<span class="op">=</span>full_hf_repo_name,</span>
<span id="cb149-35"><a href="#cb149-35" aria-hidden="true" tabindex="-1"></a>    folder_path<span class="op">=</span>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,</span>
<span id="cb149-36"><a href="#cb149-36" aria-hidden="true" tabindex="-1"></a>    path_in_repo<span class="op">=</span><span class="st">"."</span>, <span class="co"># upload our folder to the root directory ("." means "base" or "root", this is the default)</span></span>
<span id="cb149-37"><a href="#cb149-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># token=HF_TOKEN, # optional: set token manually</span></span>
<span id="cb149-38"><a href="#cb149-38" aria-hidden="true" tabindex="-1"></a>    repo_type<span class="op">=</span>HF_REPO_TYPE,</span>
<span id="cb149-39"><a href="#cb149-39" aria-hidden="true" tabindex="-1"></a>    commit_message<span class="op">=</span><span class="st">"Uploading Trashify box detection model app.py"</span></span>
<span id="cb149-40"><a href="#cb149-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb149-41"><a href="#cb149-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Demo folder successfully uploaded with commit URL: </span><span class="sc">{</span>folder_upload_url<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Creating repo on Hugging Face Hub with name: trashify_demo_v1
[INFO] Full Hugging Face Hub repo name: mrdbourke/trashify_demo_v1
[INFO] Uploading demos/trashify_object_detector to repo: mrdbourke/trashify_demo_v1
[INFO] Demo folder successfully uploaded with commit URL: https://huggingface.co/spaces/mrdbourke/trashify_demo_v1/tree/main/.</code></pre>
</div>
</div>
<p>TK - see the demo here: https://huggingface.co/spaces/mrdbourke/trashify_demo_v1</p>
</section>
<section id="tk---testing-the-hosted-demo" class="level3" data-number="11.2">
<h3 data-number="11.2" class="anchored" data-anchor-id="tk---testing-the-hosted-demo"><span class="header-section-number">11.2</span> TK - Testing the hosted demo</h3>
<div id="cell-155" class="cell" data-execution_count="108">
<div class="sourceCode cell-code" id="cb151"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb151-2"><a href="#cb151-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-3"><a href="#cb151-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-4"><a href="#cb151-4" aria-hidden="true" tabindex="-1"></a><span class="co"># You can get embeddable HTML code for your demo by clicking the "Embed" button on the demo page</span></span>
<span id="cb151-5"><a href="#cb151-5" aria-hidden="true" tabindex="-1"></a>HTML(data<span class="op">=</span><span class="st">'''</span></span>
<span id="cb151-6"><a href="#cb151-6" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;iframe</span></span>
<span id="cb151-7"><a href="#cb151-7" aria-hidden="true" tabindex="-1"></a><span class="st">    src="https://mrdbourke-trashify-demo-v1.hf.space"</span></span>
<span id="cb151-8"><a href="#cb151-8" aria-hidden="true" tabindex="-1"></a><span class="st">    frameborder="0"</span></span>
<span id="cb151-9"><a href="#cb151-9" aria-hidden="true" tabindex="-1"></a><span class="st">    width="850"</span></span>
<span id="cb151-10"><a href="#cb151-10" aria-hidden="true" tabindex="-1"></a><span class="st">    height="1000"</span></span>
<span id="cb151-11"><a href="#cb151-11" aria-hidden="true" tabindex="-1"></a><span class="st">&gt;&lt;/iframe&gt;     </span></span>
<span id="cb151-12"><a href="#cb151-12" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="108">

<iframe src="https://mrdbourke-trashify-demo-v1.hf.space" frameborder="0" width="850" height="1000"></iframe>     
</div>
</div>
</section>
</section>
<section id="tk---improve-our-model-with-data-augmentation" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="tk---improve-our-model-with-data-augmentation"><span class="header-section-number">12</span> TK - Improve our model with data augmentation</h2>
<p>UPTOHERE - Read for object detection augmentation (keep it simple) - Check out the papers for detection augmentation - Train a model with data augmentation - Compare the modelâ€™s metrics between data augmentation and no data augmentation</p>
<section id="load-dataset" class="level3" data-number="12.1">
<h3 data-number="12.1" class="anchored" data-anchor-id="load-dataset"><span class="header-section-number">12.1</span> Load dataset</h3>
<div id="cell-158" class="cell" data-execution_count="109">
<div class="sourceCode cell-code" id="cb152"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-3"><a href="#cb152-3" aria-hidden="true" tabindex="-1"></a><span class="co"># load_dataset?</span></span>
<span id="cb152-4"><a href="#cb152-4" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(path<span class="op">=</span><span class="st">"mrdbourke/trashify_manual_labelled_images"</span>)</span>
<span id="cb152-5"><a href="#cb152-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-6"><a href="#cb152-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Length of original dataset: </span><span class="sc">{</span><span class="bu">len</span>(dataset[<span class="st">'train'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb152-7"><a href="#cb152-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-8"><a href="#cb152-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data</span></span>
<span id="cb152-9"><a href="#cb152-9" aria-hidden="true" tabindex="-1"></a>dataset_split <span class="op">=</span> dataset[<span class="st">"train"</span>].train_test_split(test_size<span class="op">=</span><span class="fl">0.3</span>, seed<span class="op">=</span><span class="dv">42</span>) <span class="co"># split the dataset into 70/30 train/test</span></span>
<span id="cb152-10"><a href="#cb152-10" aria-hidden="true" tabindex="-1"></a>dataset_test_val_split <span class="op">=</span> dataset_split[<span class="st">"test"</span>].train_test_split(test_size<span class="op">=</span><span class="fl">0.6</span>, seed<span class="op">=</span><span class="dv">42</span>) <span class="co"># split the test set into 40/60 validation/test</span></span>
<span id="cb152-11"><a href="#cb152-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-12"><a href="#cb152-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create splits</span></span>
<span id="cb152-13"><a href="#cb152-13" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"train"</span>] <span class="op">=</span> dataset_split[<span class="st">"train"</span>]</span>
<span id="cb152-14"><a href="#cb152-14" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"validation"</span>] <span class="op">=</span> dataset_test_val_split[<span class="st">"train"</span>]</span>
<span id="cb152-15"><a href="#cb152-15" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"test"</span>] <span class="op">=</span> dataset_test_val_split[<span class="st">"test"</span>]</span>
<span id="cb152-16"><a href="#cb152-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-17"><a href="#cb152-17" aria-hidden="true" tabindex="-1"></a>dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Length of original dataset: 1128</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="109">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],
        num_rows: 789
    })
    validation: Dataset({
        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],
        num_rows: 135
    })
    test: Dataset({
        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],
        num_rows: 204
    })
})</code></pre>
</div>
</div>
<div id="cell-159" class="cell" data-execution_count="110">
<div class="sourceCode cell-code" id="cb155"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the categories from the dataset</span></span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: this requires the dataset to have been uploaded with this feature setup</span></span>
<span id="cb155-3"><a href="#cb155-3" aria-hidden="true" tabindex="-1"></a>categories <span class="op">=</span> dataset[<span class="st">"train"</span>].features[<span class="st">"annotations"</span>].feature[<span class="st">"category_id"</span>]</span>
<span id="cb155-4"><a href="#cb155-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-5"><a href="#cb155-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the names attribute</span></span>
<span id="cb155-6"><a href="#cb155-6" aria-hidden="true" tabindex="-1"></a>categories.names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="110">
<pre><code>['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']</code></pre>
</div>
</div>
<div id="cell-160" class="cell" data-execution_count="111">
<div class="sourceCode cell-code" id="cb157"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a>id2label <span class="op">=</span> {i: class_name <span class="cf">for</span> i, class_name <span class="kw">in</span> <span class="bu">enumerate</span>(categories.names)}</span>
<span id="cb157-2"><a href="#cb157-2" aria-hidden="true" tabindex="-1"></a>label2id <span class="op">=</span> {value: key <span class="cf">for</span> key, value <span class="kw">in</span> id2label.items()}</span>
<span id="cb157-3"><a href="#cb157-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-4"><a href="#cb157-4" aria-hidden="true" tabindex="-1"></a>id2label, label2id</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="111">
<pre><code>({0: 'bin',
  1: 'hand',
  2: 'not_bin',
  3: 'not_hand',
  4: 'not_trash',
  5: 'trash',
  6: 'trash_arm'},
 {'bin': 0,
  'hand': 1,
  'not_bin': 2,
  'not_hand': 3,
  'not_trash': 4,
  'trash': 5,
  'trash_arm': 6})</code></pre>
</div>
</div>
<div id="cell-161" class="cell" data-execution_count="112">
<div class="sourceCode cell-code" id="cb159"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View a random sample</span></span>
<span id="cb159-2"><a href="#cb159-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb159-3"><a href="#cb159-3" aria-hidden="true" tabindex="-1"></a>random_idx <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(dataset[<span class="st">"train"</span>]))</span>
<span id="cb159-4"><a href="#cb159-4" aria-hidden="true" tabindex="-1"></a>random_sample <span class="op">=</span> dataset[<span class="st">"train"</span>][random_idx]</span>
<span id="cb159-5"><a href="#cb159-5" aria-hidden="true" tabindex="-1"></a>random_sample</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="112">
<pre><code>{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,
 'image_id': 955,
 'annotations': {'file_name': ['ed8cb1ab-2882-4ab7-a839-c53fa2908a72.jpeg',
   'ed8cb1ab-2882-4ab7-a839-c53fa2908a72.jpeg',
   'ed8cb1ab-2882-4ab7-a839-c53fa2908a72.jpeg',
   'ed8cb1ab-2882-4ab7-a839-c53fa2908a72.jpeg'],
  'image_id': [955, 955, 955, 955],
  'category_id': [5, 1, 0, 4],
  'bbox': [[464.79998779296875, 625.5999755859375, 68.30000305175781, 92.5],
   [483.0, 686.2000122070312, 173.0, 247.3000030517578],
   [102.80000305175781, 361.70001220703125, 813.5, 734.0],
   [325.29998779296875,
    716.5999755859375,
    189.60000610351562,
    215.3000030517578]],
  'iscrowd': [0, 0, 0, 0],
  'area': [6317.75, 42782.8984375, 597109.0, 40820.87890625]},
 'label_source': 'manual_prodigy_label',
 'image_source': 'manual_taken_photo'}</code></pre>
</div>
</div>
</section>
<section id="setup-model" class="level3" data-number="12.2">
<h3 data-number="12.2" class="anchored" data-anchor-id="setup-model"><span class="header-section-number">12.2</span> Setup model</h3>
<div id="cell-163" class="cell" data-execution_count="113">
<div class="sourceCode cell-code" id="cb161"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForObjectDetection, AutoImageProcessor</span>
<span id="cb161-2"><a href="#cb161-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-3"><a href="#cb161-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Model config - https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig </span></span>
<span id="cb161-4"><a href="#cb161-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Model docs - https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrModel </span></span>
<span id="cb161-5"><a href="#cb161-5" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"microsoft/conditional-detr-resnet-50"</span></span>
<span id="cb161-6"><a href="#cb161-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-7"><a href="#cb161-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set image size</span></span>
<span id="cb161-8"><a href="#cb161-8" aria-hidden="true" tabindex="-1"></a>IMAGE_SIZE <span class="op">=</span> <span class="dv">640</span> <span class="co"># other common image sizes include: 300x300, 480x480, 512x512, 640x640, 800x800 (best to experiment and see which works best)</span></span>
<span id="cb161-9"><a href="#cb161-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-10"><a href="#cb161-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the image processor (this is required for prepraring images)</span></span>
<span id="cb161-11"><a href="#cb161-11" aria-hidden="true" tabindex="-1"></a><span class="co"># See docs: https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess</span></span>
<span id="cb161-12"><a href="#cb161-12" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoImageProcessor.from_pretrained(</span>
<span id="cb161-13"><a href="#cb161-13" aria-hidden="true" tabindex="-1"></a>    pretrained_model_name_or_path<span class="op">=</span>MODEL_NAME,</span>
<span id="cb161-14"><a href="#cb161-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"coco_detection"</span>, <span class="co"># this is the default</span></span>
<span id="cb161-15"><a href="#cb161-15" aria-hidden="true" tabindex="-1"></a>    do_convert_annotations<span class="op">=</span><span class="va">True</span>, <span class="co"># defaults to True, converts boxes to (center_x, center_y, width, height)</span></span>
<span id="cb161-16"><a href="#cb161-16" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span>{<span class="st">"shortest_edge"</span>: IMAGE_SIZE, <span class="st">"longest_edge"</span>: IMAGE_SIZE},</span>
<span id="cb161-17"><a href="#cb161-17" aria-hidden="true" tabindex="-1"></a>    max_size<span class="op">=</span><span class="va">None</span> <span class="co"># Note: this parameter is deprecated and will produce a warning if used during processing.</span></span>
<span id="cb161-18"><a href="#cb161-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb161-19"><a href="#cb161-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-20"><a href="#cb161-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Check out the image processor</span></span>
<span id="cb161-21"><a href="#cb161-21" aria-hidden="true" tabindex="-1"></a>image_processor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="113">
<pre><code>ConditionalDetrImageProcessor {
  "do_convert_annotations": true,
  "do_normalize": true,
  "do_pad": true,
  "do_rescale": true,
  "do_resize": true,
  "format": "coco_detection",
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "ConditionalDetrImageProcessor",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "pad_size": null,
  "resample": 2,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 640,
    "shortest_edge": 640
  }
}</code></pre>
</div>
</div>
<div id="cell-164" class="cell" data-execution_count="114">
<div class="sourceCode cell-code" id="cb163"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="co"># First create a couple of dataclasses to store our data format</span></span>
<span id="cb163-2"><a href="#cb163-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass, asdict</span>
<span id="cb163-3"><a href="#cb163-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Tuple</span>
<span id="cb163-4"><a href="#cb163-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-5"><a href="#cb163-5" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb163-6"><a href="#cb163-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SingleCOCOAnnotation:</span>
<span id="cb163-7"><a href="#cb163-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"An instance of a single COCO annotation. See COCO format: https://cocodataset.org/#format-data"</span></span>
<span id="cb163-8"><a href="#cb163-8" aria-hidden="true" tabindex="-1"></a>    image_id: <span class="bu">int</span></span>
<span id="cb163-9"><a href="#cb163-9" aria-hidden="true" tabindex="-1"></a>    category_id: <span class="bu">int</span></span>
<span id="cb163-10"><a href="#cb163-10" aria-hidden="true" tabindex="-1"></a>    bbox: List[<span class="bu">float</span>] <span class="co"># bboxes in format [x_top_left, y_top_left, width, height]</span></span>
<span id="cb163-11"><a href="#cb163-11" aria-hidden="true" tabindex="-1"></a>    area: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb163-12"><a href="#cb163-12" aria-hidden="true" tabindex="-1"></a>    iscrowd: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb163-13"><a href="#cb163-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-14"><a href="#cb163-14" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb163-15"><a href="#cb163-15" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ImageCOCOAnnotations:</span>
<span id="cb163-16"><a href="#cb163-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"A collection of COCO annotations for a given image_id."</span></span>
<span id="cb163-17"><a href="#cb163-17" aria-hidden="true" tabindex="-1"></a>    image_id: <span class="bu">int</span></span>
<span id="cb163-18"><a href="#cb163-18" aria-hidden="true" tabindex="-1"></a>    annotations: List[SingleCOCOAnnotation]</span>
<span id="cb163-19"><a href="#cb163-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-20"><a href="#cb163-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_image_annotations_as_coco(</span>
<span id="cb163-21"><a href="#cb163-21" aria-hidden="true" tabindex="-1"></a>        image_id: <span class="bu">int</span>,</span>
<span id="cb163-22"><a href="#cb163-22" aria-hidden="true" tabindex="-1"></a>        categories: List[<span class="bu">int</span>],</span>
<span id="cb163-23"><a href="#cb163-23" aria-hidden="true" tabindex="-1"></a>        areas: List[<span class="bu">float</span>],</span>
<span id="cb163-24"><a href="#cb163-24" aria-hidden="true" tabindex="-1"></a>        bboxes: List[Tuple[<span class="bu">float</span>, <span class="bu">float</span>, <span class="bu">float</span>, <span class="bu">float</span>]] <span class="co"># bboxes in format </span></span>
<span id="cb163-25"><a href="#cb163-25" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">dict</span>:</span>
<span id="cb163-26"><a href="#cb163-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Turn input lists into a list of dicts</span></span>
<span id="cb163-27"><a href="#cb163-27" aria-hidden="true" tabindex="-1"></a>    coco_format_annotations <span class="op">=</span> [</span>
<span id="cb163-28"><a href="#cb163-28" aria-hidden="true" tabindex="-1"></a>        asdict(SingleCOCOAnnotation(</span>
<span id="cb163-29"><a href="#cb163-29" aria-hidden="true" tabindex="-1"></a>            image_id<span class="op">=</span>image_id,</span>
<span id="cb163-30"><a href="#cb163-30" aria-hidden="true" tabindex="-1"></a>            category_id<span class="op">=</span>category,</span>
<span id="cb163-31"><a href="#cb163-31" aria-hidden="true" tabindex="-1"></a>            bbox<span class="op">=</span><span class="bu">list</span>(bbox),</span>
<span id="cb163-32"><a href="#cb163-32" aria-hidden="true" tabindex="-1"></a>            area<span class="op">=</span>area,</span>
<span id="cb163-33"><a href="#cb163-33" aria-hidden="true" tabindex="-1"></a>        ))</span>
<span id="cb163-34"><a href="#cb163-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> category, area, bbox <span class="kw">in</span> <span class="bu">zip</span>(categories, areas, bboxes)</span>
<span id="cb163-35"><a href="#cb163-35" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb163-36"><a href="#cb163-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-37"><a href="#cb163-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return dictionary of annotations with format {"image_id": ..., "annotations": ...}</span></span>
<span id="cb163-38"><a href="#cb163-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> asdict(ImageCOCOAnnotations(image_id<span class="op">=</span>image_id,</span>
<span id="cb163-39"><a href="#cb163-39" aria-hidden="true" tabindex="-1"></a>                                       annotations<span class="op">=</span>coco_format_annotations))</span>
<span id="cb163-40"><a href="#cb163-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-41"><a href="#cb163-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's try it out</span></span>
<span id="cb163-42"><a href="#cb163-42" aria-hidden="true" tabindex="-1"></a>image_id <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb163-43"><a href="#cb163-43" aria-hidden="true" tabindex="-1"></a>random_idx <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(dataset[<span class="st">"train"</span>]))</span>
<span id="cb163-44"><a href="#cb163-44" aria-hidden="true" tabindex="-1"></a>random_sample <span class="op">=</span> dataset[<span class="st">"train"</span>][random_idx]</span>
<span id="cb163-45"><a href="#cb163-45" aria-hidden="true" tabindex="-1"></a>random_sample_categories <span class="op">=</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"category_id"</span>]</span>
<span id="cb163-46"><a href="#cb163-46" aria-hidden="true" tabindex="-1"></a>random_sample_areas <span class="op">=</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"area"</span>]</span>
<span id="cb163-47"><a href="#cb163-47" aria-hidden="true" tabindex="-1"></a>random_sample_bboxes <span class="op">=</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"bbox"</span>]</span>
<span id="cb163-48"><a href="#cb163-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-49"><a href="#cb163-49" aria-hidden="true" tabindex="-1"></a>random_sample_coco_annotations <span class="op">=</span> format_image_annotations_as_coco(image_id<span class="op">=</span>image_id,</span>
<span id="cb163-50"><a href="#cb163-50" aria-hidden="true" tabindex="-1"></a>                                                                  categories<span class="op">=</span>random_sample_categories,</span>
<span id="cb163-51"><a href="#cb163-51" aria-hidden="true" tabindex="-1"></a>                                                                  areas<span class="op">=</span>random_sample_areas,</span>
<span id="cb163-52"><a href="#cb163-52" aria-hidden="true" tabindex="-1"></a>                                                                  bboxes<span class="op">=</span>random_sample_bboxes)</span>
<span id="cb163-53"><a href="#cb163-53" aria-hidden="true" tabindex="-1"></a>random_sample_coco_annotations</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="114">
<pre><code>{'image_id': 0,
 'annotations': [{'image_id': 0,
   'category_id': 0,
   'bbox': [452.79998779296875,
    446.6000061035156,
    272.70001220703125,
    388.20001220703125],
   'area': 105862.140625,
   'iscrowd': 0},
  {'image_id': 0,
   'category_id': 0,
   'bbox': [146.5, 487.5, 348.3999938964844, 424.79998779296875],
   'area': 148000.3125,
   'iscrowd': 0},
  {'image_id': 0,
   'category_id': 0,
   'bbox': [8.300000190734863, 522.5, 241.3000030517578, 505.0],
   'area': 121856.5,
   'iscrowd': 0}]}</code></pre>
</div>
</div>
<div id="cell-165" class="cell" data-execution_count="115">
<div class="sourceCode cell-code" id="cb165"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup the model</span></span>
<span id="cb165-2"><a href="#cb165-2" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co">: Can functionize this to create a base model (e.g. a model with all the base settings/untrained weights) </span></span>
<span id="cb165-3"><a href="#cb165-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model():</span>
<span id="cb165-4"><a href="#cb165-4" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> AutoModelForObjectDetection.from_pretrained(</span>
<span id="cb165-5"><a href="#cb165-5" aria-hidden="true" tabindex="-1"></a>                pretrained_model_name_or_path<span class="op">=</span>MODEL_NAME,</span>
<span id="cb165-6"><a href="#cb165-6" aria-hidden="true" tabindex="-1"></a>                label2id<span class="op">=</span>label2id,</span>
<span id="cb165-7"><a href="#cb165-7" aria-hidden="true" tabindex="-1"></a>                id2label<span class="op">=</span>id2label,</span>
<span id="cb165-8"><a href="#cb165-8" aria-hidden="true" tabindex="-1"></a>                ignore_mismatched_sizes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb165-9"><a href="#cb165-9" aria-hidden="true" tabindex="-1"></a>                backbone<span class="op">=</span><span class="st">"resnet50"</span>)</span>
<span id="cb165-10"><a href="#cb165-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb165-11"><a href="#cb165-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-12"><a href="#cb165-12" aria-hidden="true" tabindex="-1"></a>model_aug <span class="op">=</span> create_model()</span>
<span id="cb165-13"><a href="#cb165-13" aria-hidden="true" tabindex="-1"></a>model_aug</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of ConditionalDetrForObjectDetection were not initialized from the model checkpoint at microsoft/conditional-detr-resnet-50 and are newly initialized because the shapes did not match:
- class_labels_classifier.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([7]) in the model instantiated
- class_labels_classifier.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="115">
<pre><code>ConditionalDetrForObjectDetection(
  (model): ConditionalDetrModel(
    (backbone): ConditionalDetrConvModel(
      (conv_encoder): ConditionalDetrConvEncoder(
        (model): FeatureListNet(
          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
          (bn1): ConditionalDetrFrozenBatchNorm2d()
          (act1): ReLU(inplace=True)
          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
          (layer1): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): ConditionalDetrFrozenBatchNorm2d()
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
          )
          (layer2): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): ConditionalDetrFrozenBatchNorm2d()
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
          )
          (layer3): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): ConditionalDetrFrozenBatchNorm2d()
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (4): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (5): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
          )
          (layer4): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): ConditionalDetrFrozenBatchNorm2d()
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
          )
        )
      )
      (position_embedding): ConditionalDetrSinePositionEmbedding()
    )
    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (query_position_embeddings): Embedding(300, 256)
    (encoder): ConditionalDetrEncoder(
      (layers): ModuleList(
        (0-5): 6 x ConditionalDetrEncoderLayer(
          (self_attn): DetrAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (activation_fn): ReLU()
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (decoder): ConditionalDetrDecoder(
      (layers): ModuleList(
        (0): ConditionalDetrDecoderLayer(
          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)
          (self_attn): ConditionalDetrAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_qpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)
          (encoder_attn): ConditionalDetrAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1-5): 5 x ConditionalDetrDecoderLayer(
          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)
          (self_attn): ConditionalDetrAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_qpos_proj): None
          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)
          (encoder_attn): ConditionalDetrAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_scale): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ref_point_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
  )
  (class_labels_classifier): Linear(in_features=256, out_features=7, bias=True)
  (bbox_predictor): ConditionalDetrMLPPredictionHead(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
)</code></pre>
</div>
</div>
</section>
<section id="tk---setup-and-visualize-transforms-augmentations" class="level3" data-number="12.3">
<h3 data-number="12.3" class="anchored" data-anchor-id="tk---setup-and-visualize-transforms-augmentations"><span class="header-section-number">12.3</span> tk - Setup and visualize transforms (augmentations)</h3>
<ul>
<li>TK - explain simple augmentations:
<ul>
<li>RandomHorizontalFlip</li>
<li>ColorJitter
<ul>
<li>Thatâ€™s itâ€¦</li>
<li>Tailor the data augmentations to your own dataset/problem</li>
</ul></li>
</ul></li>
</ul>
<div id="cell-167" class="cell" data-execution_count="116">
<div class="sourceCode cell-code" id="cb168"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb168-2"><a href="#cb168-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb168-3"><a href="#cb168-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-4"><a href="#cb168-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> v2 </span>
<span id="cb168-5"><a href="#cb168-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms.v2.functional <span class="im">import</span> to_pil_image, pil_to_tensor, pad</span>
<span id="cb168-6"><a href="#cb168-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> draw_bounding_boxes</span>
<span id="cb168-7"><a href="#cb168-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-8"><a href="#cb168-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional transform from here: https://arxiv.org/pdf/2012.07177</span></span>
<span id="cb168-9"><a href="#cb168-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale jitter -&gt; pad -&gt; resize </span></span>
<span id="cb168-10"><a href="#cb168-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-11"><a href="#cb168-11" aria-hidden="true" tabindex="-1"></a>train_transforms <span class="op">=</span> v2.Compose([</span>
<span id="cb168-12"><a href="#cb168-12" aria-hidden="true" tabindex="-1"></a>    v2.ToImage(),</span>
<span id="cb168-13"><a href="#cb168-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># v2.RandomResizedCrop(size=(640, 640), antialias=True),</span></span>
<span id="cb168-14"><a href="#cb168-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># v2.Resize(size=(640, 640)),</span></span>
<span id="cb168-15"><a href="#cb168-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># v2.RandomShortestSize(min_size=480, max_size=640),</span></span>
<span id="cb168-16"><a href="#cb168-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># v2.ScaleJitter(target_size=(640, 640)),</span></span>
<span id="cb168-17"><a href="#cb168-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># PadToSize(target_height=640, target_width=640),</span></span>
<span id="cb168-18"><a href="#cb168-18" aria-hidden="true" tabindex="-1"></a>    v2.RandomHorizontalFlip(p<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb168-19"><a href="#cb168-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># v2.RandomPhotometricDistort(p=0.75),</span></span>
<span id="cb168-20"><a href="#cb168-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># v2.RandomShortestSize(min_size=480, max_size=640),</span></span>
<span id="cb168-21"><a href="#cb168-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># v2.Resize(size=(640, 640)),</span></span>
<span id="cb168-22"><a href="#cb168-22" aria-hidden="true" tabindex="-1"></a>    v2.ColorJitter(brightness<span class="op">=</span><span class="fl">0.75</span>, <span class="co"># randomly adjust the brightness </span></span>
<span id="cb168-23"><a href="#cb168-23" aria-hidden="true" tabindex="-1"></a>                   contrast<span class="op">=</span><span class="fl">0.75</span>), <span class="co"># randomly alter the contrast</span></span>
<span id="cb168-24"><a href="#cb168-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># v2.RandomPerspective(distortion_scale=0.3, </span></span>
<span id="cb168-25"><a href="#cb168-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">#                      p=0.3,</span></span>
<span id="cb168-26"><a href="#cb168-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">#                      fill=(123, 117, 104)), # fill with average colour</span></span>
<span id="cb168-27"><a href="#cb168-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># v2.RandomZoomOut(side_range=(1.0, 1.5),</span></span>
<span id="cb168-28"><a href="#cb168-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">#                  fill=(123, 117, 104)),</span></span>
<span id="cb168-29"><a href="#cb168-29" aria-hidden="true" tabindex="-1"></a>    v2.ToDtype(dtype<span class="op">=</span>torch.float32, scale<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb168-30"><a href="#cb168-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-31"><a href="#cb168-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])</span></span>
<span id="cb168-32"><a href="#cb168-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sanitize boxes, recommended to be called at least once at the end of the transform pipeline</span></span>
<span id="cb168-33"><a href="#cb168-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.SanitizeBoundingBoxes.html#torchvision.transforms.v2.SanitizeBoundingBoxes</span></span>
<span id="cb168-34"><a href="#cb168-34" aria-hidden="true" tabindex="-1"></a>    v2.SanitizeBoundingBoxes(labels_getter<span class="op">=</span><span class="va">None</span>) </span>
<span id="cb168-35"><a href="#cb168-35" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="tk---visualize-transforms" class="level3" data-number="12.4">
<h3 data-number="12.4" class="anchored" data-anchor-id="tk---visualize-transforms"><span class="header-section-number">12.4</span> TK - Visualize transforms</h3>
<div id="cell-169" class="cell" data-execution_count="117">
<div class="sourceCode cell-code" id="cb169"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb169-2"><a href="#cb169-2" aria-hidden="true" tabindex="-1"></a>random_idx <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(dataset[<span class="st">"train"</span>]))</span>
<span id="cb169-3"><a href="#cb169-3" aria-hidden="true" tabindex="-1"></a>random_sample <span class="op">=</span> dataset[<span class="st">"train"</span>][random_idx]</span>
<span id="cb169-4"><a href="#cb169-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb169-5"><a href="#cb169-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform transform on image</span></span>
<span id="cb169-6"><a href="#cb169-6" aria-hidden="true" tabindex="-1"></a>random_sample_image <span class="op">=</span> random_sample[<span class="st">"image"</span>]</span>
<span id="cb169-7"><a href="#cb169-7" aria-hidden="true" tabindex="-1"></a>random_sample_image_width, random_sample_image_height <span class="op">=</span> random_sample[<span class="st">"image"</span>].size</span>
<span id="cb169-8"><a href="#cb169-8" aria-hidden="true" tabindex="-1"></a>random_sample_boxes_xywh <span class="op">=</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"bbox"</span>] <span class="co"># these are in XYWH format</span></span>
<span id="cb169-9"><a href="#cb169-9" aria-hidden="true" tabindex="-1"></a>random_sample_boxes_xyxy <span class="op">=</span> torchvision.ops.box_convert(boxes<span class="op">=</span>torch.tensor(random_sample_boxes_xywh),</span>
<span id="cb169-10"><a href="#cb169-10" aria-hidden="true" tabindex="-1"></a>                                                       in_fmt<span class="op">=</span><span class="st">"xywh"</span>,</span>
<span id="cb169-11"><a href="#cb169-11" aria-hidden="true" tabindex="-1"></a>                                                       out_fmt<span class="op">=</span><span class="st">"xyxy"</span>)</span>
<span id="cb169-12"><a href="#cb169-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb169-13"><a href="#cb169-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Format boxes to be xyxy for transforms</span></span>
<span id="cb169-14"><a href="#cb169-14" aria-hidden="true" tabindex="-1"></a>random_sample_boxes_xyxy <span class="op">=</span> torchvision.tv_tensors.BoundingBoxes(</span>
<span id="cb169-15"><a href="#cb169-15" aria-hidden="true" tabindex="-1"></a>    data<span class="op">=</span>random_sample_boxes_xyxy,</span>
<span id="cb169-16"><a href="#cb169-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"XYXY"</span>,</span>
<span id="cb169-17"><a href="#cb169-17" aria-hidden="true" tabindex="-1"></a>    canvas_size<span class="op">=</span>(random_sample_image_height, random_sample_image_width) <span class="co"># comes in the form height, width</span></span>
<span id="cb169-18"><a href="#cb169-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb169-19"><a href="#cb169-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb169-20"><a href="#cb169-20" aria-hidden="true" tabindex="-1"></a>random_sample_image_transformed, random_sample_boxes_transformed <span class="op">=</span> train_transforms(random_sample_image,</span>
<span id="cb169-21"><a href="#cb169-21" aria-hidden="true" tabindex="-1"></a>                                                                                    random_sample_boxes_xyxy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-170" class="cell" data-execution_count="118">
<div class="sourceCode cell-code" id="cb170"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a>random_sample_original_image_with_boxes <span class="op">=</span> to_pil_image(pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb170-2"><a href="#cb170-2" aria-hidden="true" tabindex="-1"></a>                                                       image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>random_sample_image),                    </span>
<span id="cb170-3"><a href="#cb170-3" aria-hidden="true" tabindex="-1"></a>                                                       boxes<span class="op">=</span>random_sample_boxes_xyxy,</span>
<span id="cb170-4"><a href="#cb170-4" aria-hidden="true" tabindex="-1"></a>                                                       labels<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb170-5"><a href="#cb170-5" aria-hidden="true" tabindex="-1"></a>                                                       width<span class="op">=</span><span class="dv">3</span>))</span>
<span id="cb170-6"><a href="#cb170-6" aria-hidden="true" tabindex="-1"></a>random_sample_original_image_with_boxes_size <span class="op">=</span> (random_sample_original_image_with_boxes.size[<span class="dv">1</span>], random_sample_original_image_with_boxes.size[<span class="dv">0</span>])</span>
<span id="cb170-7"><a href="#cb170-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-8"><a href="#cb170-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted boxes on the random test image </span></span>
<span id="cb170-9"><a href="#cb170-9" aria-hidden="true" tabindex="-1"></a>random_sample_transformed_image_with_boxes <span class="op">=</span> to_pil_image(pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb170-10"><a href="#cb170-10" aria-hidden="true" tabindex="-1"></a>                                                          image<span class="op">=</span>random_sample_image_transformed,                    </span>
<span id="cb170-11"><a href="#cb170-11" aria-hidden="true" tabindex="-1"></a>                                                          boxes<span class="op">=</span>random_sample_boxes_transformed,</span>
<span id="cb170-12"><a href="#cb170-12" aria-hidden="true" tabindex="-1"></a>                                                          labels<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb170-13"><a href="#cb170-13" aria-hidden="true" tabindex="-1"></a>                                                          width<span class="op">=</span><span class="dv">3</span>))</span>
<span id="cb170-14"><a href="#cb170-14" aria-hidden="true" tabindex="-1"></a>random_sample_transformed_image_with_boxes_size <span class="op">=</span> (random_sample_transformed_image_with_boxes.size[<span class="dv">1</span>], random_sample_transformed_image_with_boxes.size[<span class="dv">0</span>])</span>
<span id="cb170-15"><a href="#cb170-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-16"><a href="#cb170-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the transformed image </span></span>
<span id="cb170-17"><a href="#cb170-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb170-18"><a href="#cb170-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-19"><a href="#cb170-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a figure with two subplots</span></span>
<span id="cb170-20"><a href="#cb170-20" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb170-21"><a href="#cb170-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-22"><a href="#cb170-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image 1</span></span>
<span id="cb170-23"><a href="#cb170-23" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].imshow(random_sample_original_image_with_boxes)</span>
<span id="cb170-24"><a href="#cb170-24" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axis(<span class="st">"off"</span>)  <span class="co"># Hide axes</span></span>
<span id="cb170-25"><a href="#cb170-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="ss">f"Original Image | Size: </span><span class="sc">{</span>random_sample_original_image_with_boxes_size<span class="sc">}</span><span class="ss"> (hxw)"</span>)</span>
<span id="cb170-26"><a href="#cb170-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-27"><a href="#cb170-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image 2</span></span>
<span id="cb170-28"><a href="#cb170-28" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].imshow(random_sample_transformed_image_with_boxes)</span>
<span id="cb170-29"><a href="#cb170-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axis(<span class="st">"off"</span>)  <span class="co"># Hide axes</span></span>
<span id="cb170-30"><a href="#cb170-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="ss">f"Transformed Image | Size: </span><span class="sc">{</span>random_sample_transformed_image_with_boxes_size<span class="sc">}</span><span class="ss"> (hxw)"</span>)</span>
<span id="cb170-31"><a href="#cb170-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-32"><a href="#cb170-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb170-33"><a href="#cb170-33" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb170-34"><a href="#cb170-34" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-90-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="tk---create-function-to-preprocess-and-transform-batch-of-examples" class="level3" data-number="12.5">
<h3 data-number="12.5" class="anchored" data-anchor-id="tk---create-function-to-preprocess-and-transform-batch-of-examples"><span class="header-section-number">12.5</span> TK - Create function to preprocess and transform batch of examples</h3>
<div id="cell-172" class="cell" data-execution_count="119">
<div class="sourceCode cell-code" id="cb171"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> tv_tensors</span>
<span id="cb171-2"><a href="#cb171-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-3"><a href="#cb171-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess_and_transform_batch(examples,</span>
<span id="cb171-4"><a href="#cb171-4" aria-hidden="true" tabindex="-1"></a>                                   image_processor,</span>
<span id="cb171-5"><a href="#cb171-5" aria-hidden="true" tabindex="-1"></a>                                   transforms<span class="op">=</span><span class="va">None</span> <span class="co"># Note: Could optionally add transforms (e.g. data augmentation) here </span></span>
<span id="cb171-6"><a href="#cb171-6" aria-hidden="true" tabindex="-1"></a>                                   ):</span>
<span id="cb171-7"><a href="#cb171-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb171-8"><a href="#cb171-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Function to preprocess batches of data.</span></span>
<span id="cb171-9"><a href="#cb171-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-10"><a href="#cb171-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Can optionally apply a transform later on.</span></span>
<span id="cb171-11"><a href="#cb171-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb171-12"><a href="#cb171-12" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> []</span>
<span id="cb171-13"><a href="#cb171-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb171-14"><a href="#cb171-14" aria-hidden="true" tabindex="-1"></a>    coco_annotations <span class="op">=</span> [] </span>
<span id="cb171-15"><a href="#cb171-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-16"><a href="#cb171-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> image, image_id, annotations_dict <span class="kw">in</span> <span class="bu">zip</span>(examples[<span class="st">"image"</span>], examples[<span class="st">"image_id"</span>], examples[<span class="st">"annotations"</span>]):</span>
<span id="cb171-17"><a href="#cb171-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note: may need to open image if it is an image path rather than PIL.Image</span></span>
<span id="cb171-18"><a href="#cb171-18" aria-hidden="true" tabindex="-1"></a>        bbox_list <span class="op">=</span> annotations_dict[<span class="st">"bbox"</span>]</span>
<span id="cb171-19"><a href="#cb171-19" aria-hidden="true" tabindex="-1"></a>        category_list <span class="op">=</span> annotations_dict[<span class="st">"category_id"</span>]</span>
<span id="cb171-20"><a href="#cb171-20" aria-hidden="true" tabindex="-1"></a>        area_list <span class="op">=</span> annotations_dict[<span class="st">"area"</span>]</span>
<span id="cb171-21"><a href="#cb171-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb171-22"><a href="#cb171-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note: Could optionally apply a transform here.</span></span>
<span id="cb171-23"><a href="#cb171-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> transforms:</span>
<span id="cb171-24"><a href="#cb171-24" aria-hidden="true" tabindex="-1"></a>            width, height <span class="op">=</span> image.size[<span class="dv">0</span>], image.size[<span class="dv">1</span>]</span>
<span id="cb171-25"><a href="#cb171-25" aria-hidden="true" tabindex="-1"></a>            bbox_list <span class="op">=</span> tv_tensors.BoundingBoxes(data<span class="op">=</span>torch.tensor(bbox_list),</span>
<span id="cb171-26"><a href="#cb171-26" aria-hidden="true" tabindex="-1"></a>                                                 <span class="bu">format</span><span class="op">=</span><span class="st">"XYWH"</span>,</span>
<span id="cb171-27"><a href="#cb171-27" aria-hidden="true" tabindex="-1"></a>                                                 canvas_size<span class="op">=</span>(height, width)) <span class="co"># canvas_size = height, width</span></span>
<span id="cb171-28"><a href="#cb171-28" aria-hidden="true" tabindex="-1"></a>            image, bbox_list <span class="op">=</span> transforms(image, </span>
<span id="cb171-29"><a href="#cb171-29" aria-hidden="true" tabindex="-1"></a>                                          bbox_list)</span>
<span id="cb171-30"><a href="#cb171-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-31"><a href="#cb171-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Format the annotations into COCO format</span></span>
<span id="cb171-32"><a href="#cb171-32" aria-hidden="true" tabindex="-1"></a>        cooc_format_annotations <span class="op">=</span> format_image_annotations_as_coco(image_id<span class="op">=</span>image_id,</span>
<span id="cb171-33"><a href="#cb171-33" aria-hidden="true" tabindex="-1"></a>                                                                   categories<span class="op">=</span>category_list,</span>
<span id="cb171-34"><a href="#cb171-34" aria-hidden="true" tabindex="-1"></a>                                                                   areas<span class="op">=</span>area_list,</span>
<span id="cb171-35"><a href="#cb171-35" aria-hidden="true" tabindex="-1"></a>                                                                   bboxes<span class="op">=</span>bbox_list)</span>
<span id="cb171-36"><a href="#cb171-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb171-37"><a href="#cb171-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add images/annotations to their respective lists</span></span>
<span id="cb171-38"><a href="#cb171-38" aria-hidden="true" tabindex="-1"></a>        images.append(image)</span>
<span id="cb171-39"><a href="#cb171-39" aria-hidden="true" tabindex="-1"></a>        coco_annotations.append(cooc_format_annotations)</span>
<span id="cb171-40"><a href="#cb171-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-41"><a href="#cb171-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb171-42"><a href="#cb171-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply the image processor to lists of images and annotations</span></span>
<span id="cb171-43"><a href="#cb171-43" aria-hidden="true" tabindex="-1"></a>    preprocessed_batch <span class="op">=</span> image_processor.preprocess(images<span class="op">=</span>images,</span>
<span id="cb171-44"><a href="#cb171-44" aria-hidden="true" tabindex="-1"></a>                                                    annotations<span class="op">=</span>coco_annotations,</span>
<span id="cb171-45"><a href="#cb171-45" aria-hidden="true" tabindex="-1"></a>                                                    return_tensors<span class="op">=</span><span class="st">"pt"</span>,</span>
<span id="cb171-46"><a href="#cb171-46" aria-hidden="true" tabindex="-1"></a>                                                    do_rescale<span class="op">=</span><span class="va">False</span> <span class="cf">if</span> transforms <span class="cf">else</span> <span class="va">True</span>,</span>
<span id="cb171-47"><a href="#cb171-47" aria-hidden="true" tabindex="-1"></a>                                                    do_resize<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb171-48"><a href="#cb171-48" aria-hidden="true" tabindex="-1"></a>                                                    do_pad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb171-49"><a href="#cb171-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb171-50"><a href="#cb171-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> preprocessed_batch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-173" class="cell" data-execution_count="120">
<div class="sourceCode cell-code" id="cb172"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb172-3"><a href="#cb172-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Make a transform for different splits</span></span>
<span id="cb172-4"><a href="#cb172-4" aria-hidden="true" tabindex="-1"></a>train_transform_batch <span class="op">=</span> partial(</span>
<span id="cb172-5"><a href="#cb172-5" aria-hidden="true" tabindex="-1"></a>    preprocess_and_transform_batch,</span>
<span id="cb172-6"><a href="#cb172-6" aria-hidden="true" tabindex="-1"></a>    transforms<span class="op">=</span>train_transforms,</span>
<span id="cb172-7"><a href="#cb172-7" aria-hidden="true" tabindex="-1"></a>    image_processor<span class="op">=</span>image_processor</span>
<span id="cb172-8"><a href="#cb172-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb172-9"><a href="#cb172-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb172-10"><a href="#cb172-10" aria-hidden="true" tabindex="-1"></a>validation_transform_batch <span class="op">=</span> partial(</span>
<span id="cb172-11"><a href="#cb172-11" aria-hidden="true" tabindex="-1"></a>    preprocess_and_transform_batch,</span>
<span id="cb172-12"><a href="#cb172-12" aria-hidden="true" tabindex="-1"></a>    transforms<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb172-13"><a href="#cb172-13" aria-hidden="true" tabindex="-1"></a>    image_processor<span class="op">=</span>image_processor</span>
<span id="cb172-14"><a href="#cb172-14" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-174" class="cell" data-execution_count="121">
<div class="sourceCode cell-code" id="cb173"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a>processed_dataset <span class="op">=</span> dataset.copy()</span>
<span id="cb173-2"><a href="#cb173-2" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"train"</span>] <span class="op">=</span> dataset[<span class="st">"train"</span>].with_transform(train_transform_batch)</span>
<span id="cb173-3"><a href="#cb173-3" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"validation"</span>] <span class="op">=</span> dataset[<span class="st">"validation"</span>].with_transform(validation_transform_batch)</span>
<span id="cb173-4"><a href="#cb173-4" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"test"</span>] <span class="op">=</span> dataset[<span class="st">"test"</span>].with_transform(validation_transform_batch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-175" class="cell" data-execution_count="122">
<div class="sourceCode cell-code" id="cb174"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create data_collate_function to collect samples into batches</span></span>
<span id="cb174-2"><a href="#cb174-2" aria-hidden="true" tabindex="-1"></a><span class="co"># TK - want to get a dictionary of {"pixel_mask": [batch_of_samples], "labels": [batch_of_samples], "pixel_mask": [batch_of_samples]}</span></span>
<span id="cb174-3"><a href="#cb174-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> data_collate_function(batch):</span>
<span id="cb174-4"><a href="#cb174-4" aria-hidden="true" tabindex="-1"></a>    collated_data <span class="op">=</span> {} </span>
<span id="cb174-5"><a href="#cb174-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-6"><a href="#cb174-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stack together a collection of pixel_values tensors</span></span>
<span id="cb174-7"><a href="#cb174-7" aria-hidden="true" tabindex="-1"></a>    collated_data[<span class="st">"pixel_values"</span>] <span class="op">=</span> torch.stack([sample[<span class="st">"pixel_values"</span>] <span class="cf">for</span> sample <span class="kw">in</span> batch])</span>
<span id="cb174-8"><a href="#cb174-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-9"><a href="#cb174-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the labels (these are dictionaries so no need to use torch.stack)</span></span>
<span id="cb174-10"><a href="#cb174-10" aria-hidden="true" tabindex="-1"></a>    collated_data[<span class="st">"labels"</span>] <span class="op">=</span> [sample[<span class="st">"labels"</span>] <span class="cf">for</span> sample <span class="kw">in</span> batch]</span>
<span id="cb174-11"><a href="#cb174-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-12"><a href="#cb174-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If there is a pixel_mask key, return the pixel_mask's as well</span></span>
<span id="cb174-13"><a href="#cb174-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"pixel_mask"</span> <span class="kw">in</span> batch[<span class="dv">0</span>]:</span>
<span id="cb174-14"><a href="#cb174-14" aria-hidden="true" tabindex="-1"></a>        collated_data[<span class="st">"pixel_mask"</span>] <span class="op">=</span> torch.stack([sample[<span class="st">"pixel_mask"</span>] <span class="cf">for</span> sample <span class="kw">in</span> batch])</span>
<span id="cb174-15"><a href="#cb174-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-16"><a href="#cb174-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> collated_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-176" class="cell" data-execution_count="123">
<div class="sourceCode cell-code" id="cb175"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb175-1"><a href="#cb175-1" aria-hidden="true" tabindex="-1"></a>model_aug <span class="op">=</span> create_model()</span>
<span id="cb175-2"><a href="#cb175-2" aria-hidden="true" tabindex="-1"></a>model_aug</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of ConditionalDetrForObjectDetection were not initialized from the model checkpoint at microsoft/conditional-detr-resnet-50 and are newly initialized because the shapes did not match:
- class_labels_classifier.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([7]) in the model instantiated
- class_labels_classifier.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="123">
<pre><code>ConditionalDetrForObjectDetection(
  (model): ConditionalDetrModel(
    (backbone): ConditionalDetrConvModel(
      (conv_encoder): ConditionalDetrConvEncoder(
        (model): FeatureListNet(
          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
          (bn1): ConditionalDetrFrozenBatchNorm2d()
          (act1): ReLU(inplace=True)
          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
          (layer1): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): ConditionalDetrFrozenBatchNorm2d()
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
          )
          (layer2): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): ConditionalDetrFrozenBatchNorm2d()
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
          )
          (layer3): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): ConditionalDetrFrozenBatchNorm2d()
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (4): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (5): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
          )
          (layer4): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): ConditionalDetrFrozenBatchNorm2d()
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): ConditionalDetrFrozenBatchNorm2d()
              (act1): ReLU(inplace=True)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): ConditionalDetrFrozenBatchNorm2d()
              (drop_block): Identity()
              (act2): ReLU(inplace=True)
              (aa): Identity()
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): ConditionalDetrFrozenBatchNorm2d()
              (act3): ReLU(inplace=True)
            )
          )
        )
      )
      (position_embedding): ConditionalDetrSinePositionEmbedding()
    )
    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (query_position_embeddings): Embedding(300, 256)
    (encoder): ConditionalDetrEncoder(
      (layers): ModuleList(
        (0-5): 6 x ConditionalDetrEncoderLayer(
          (self_attn): DetrAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (activation_fn): ReLU()
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (decoder): ConditionalDetrDecoder(
      (layers): ModuleList(
        (0): ConditionalDetrDecoderLayer(
          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)
          (self_attn): ConditionalDetrAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_qpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)
          (encoder_attn): ConditionalDetrAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1-5): 5 x ConditionalDetrDecoderLayer(
          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)
          (self_attn): ConditionalDetrAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_qpos_proj): None
          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)
          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)
          (encoder_attn): ConditionalDetrAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_scale): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (ref_point_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
  )
  (class_labels_classifier): Linear(in_features=256, out_features=7, bias=True)
  (bbox_predictor): ConditionalDetrMLPPredictionHead(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
)</code></pre>
</div>
</div>
<div id="cell-177" class="cell" data-execution_count="124">
<div class="sourceCode cell-code" id="cb178"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Depending on the size/speed of your GPU, this may take a while</span></span>
<span id="cb178-2"><a href="#cb178-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-3"><a href="#cb178-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TrainingArguments, Trainer</span>
<span id="cb178-4"><a href="#cb178-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-5"><a href="#cb178-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the batch size according to the memory you have available on your GPU</span></span>
<span id="cb178-6"><a href="#cb178-6" aria-hidden="true" tabindex="-1"></a><span class="co"># e.g. on my NVIDIA RTX 4090 with 24GB of VRAM, I can use a batch size of 32 without running out of memory</span></span>
<span id="cb178-7"><a href="#cb178-7" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb178-8"><a href="#cb178-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-9"><a href="#cb178-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Disable warnings about `max_size` parameter being deprecated (this is okay)</span></span>
<span id="cb178-10"><a href="#cb178-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb178-11"><a href="#cb178-11" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>, message<span class="op">=</span><span class="st">"The `max_size` parameter is deprecated*"</span>)</span>
<span id="cb178-12"><a href="#cb178-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-13"><a href="#cb178-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: AdamW Optimizer is used by default</span></span>
<span id="cb178-14"><a href="#cb178-14" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb178-15"><a href="#cb178-15" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"detr_finetuned_trashify_box_detector_with_data_aug"</span>, <span class="co"># Tk - make sure this is suitable for data aug model</span></span>
<span id="cb178-16"><a href="#cb178-16" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">25</span>,</span>
<span id="cb178-17"><a href="#cb178-17" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb178-18"><a href="#cb178-18" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb178-19"><a href="#cb178-19" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb178-20"><a href="#cb178-20" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">1e-4</span>,</span>
<span id="cb178-21"><a href="#cb178-21" aria-hidden="true" tabindex="-1"></a>    lr_scheduler_type<span class="op">=</span><span class="st">"linear"</span>, <span class="co"># default = "linear", can try others such as "cosine", "constant" etc</span></span>
<span id="cb178-22"><a href="#cb178-22" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">1e-4</span>,</span>
<span id="cb178-23"><a href="#cb178-23" aria-hidden="true" tabindex="-1"></a>    max_grad_norm<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb178-24"><a href="#cb178-24" aria-hidden="true" tabindex="-1"></a>    metric_for_best_model<span class="op">=</span><span class="st">"eval_loss"</span>,</span>
<span id="cb178-25"><a href="#cb178-25" aria-hidden="true" tabindex="-1"></a>    greater_is_better<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb178-26"><a href="#cb178-26" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb178-27"><a href="#cb178-27" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb178-28"><a href="#cb178-28" aria-hidden="true" tabindex="-1"></a>    logging_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb178-29"><a href="#cb178-29" aria-hidden="true" tabindex="-1"></a>    save_total_limit<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb178-30"><a href="#cb178-30" aria-hidden="true" tabindex="-1"></a>    remove_unused_columns<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb178-31"><a href="#cb178-31" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>, <span class="co"># don't save experiments to a third party service</span></span>
<span id="cb178-32"><a href="#cb178-32" aria-hidden="true" tabindex="-1"></a>    dataloader_num_workers<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb178-33"><a href="#cb178-33" aria-hidden="true" tabindex="-1"></a>    warmup_ratio<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb178-34"><a href="#cb178-34" aria-hidden="true" tabindex="-1"></a>    push_to_hub<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb178-35"><a href="#cb178-35" aria-hidden="true" tabindex="-1"></a>    eval_do_concat_batches<span class="op">=</span><span class="va">False</span></span>
<span id="cb178-36"><a href="#cb178-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb178-37"><a href="#cb178-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-38"><a href="#cb178-38" aria-hidden="true" tabindex="-1"></a>model_v2_trainer <span class="op">=</span> Trainer(</span>
<span id="cb178-39"><a href="#cb178-39" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model_aug,</span>
<span id="cb178-40"><a href="#cb178-40" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb178-41"><a href="#cb178-41" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>processed_dataset[<span class="st">"train"</span>],</span>
<span id="cb178-42"><a href="#cb178-42" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>processed_dataset[<span class="st">"validation"</span>],</span>
<span id="cb178-43"><a href="#cb178-43" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>image_processor,</span>
<span id="cb178-44"><a href="#cb178-44" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collate_function,</span>
<span id="cb178-45"><a href="#cb178-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute_metrics=None # </span><span class="al">TODO</span><span class="co">: add a metrics function, just see if model trains first</span></span>
<span id="cb178-46"><a href="#cb178-46" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb178-47"><a href="#cb178-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-48"><a href="#cb178-48" aria-hidden="true" tabindex="-1"></a>model_v2_results <span class="op">=</span> model_v2_trainer.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/daniel/miniconda3/envs/ai/lib/python3.11/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)</code></pre>
</div>
<div class="cell-output cell-output-display">

    <div>
      
      <progress value="1250" max="1250" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [1250/1250 08:19, Epoch 25/25]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>100.473500</td>
<td>8.029722</td>
</tr>
<tr class="even">
<td>2</td>
<td>4.369000</td>
<td>2.737582</td>
</tr>
<tr class="odd">
<td>3</td>
<td>2.551800</td>
<td>2.183892</td>
</tr>
<tr class="even">
<td>4</td>
<td>2.222600</td>
<td>1.922801</td>
</tr>
<tr class="odd">
<td>5</td>
<td>1.990600</td>
<td>1.740759</td>
</tr>
<tr class="even">
<td>6</td>
<td>1.821900</td>
<td>1.557272</td>
</tr>
<tr class="odd">
<td>7</td>
<td>1.697400</td>
<td>1.477890</td>
</tr>
<tr class="even">
<td>8</td>
<td>1.602700</td>
<td>1.451024</td>
</tr>
<tr class="odd">
<td>9</td>
<td>1.551700</td>
<td>1.371128</td>
</tr>
<tr class="even">
<td>10</td>
<td>1.449100</td>
<td>1.317680</td>
</tr>
<tr class="odd">
<td>11</td>
<td>1.433500</td>
<td>1.281066</td>
</tr>
<tr class="even">
<td>12</td>
<td>1.364500</td>
<td>1.247493</td>
</tr>
<tr class="odd">
<td>13</td>
<td>1.331400</td>
<td>1.206003</td>
</tr>
<tr class="even">
<td>14</td>
<td>1.297300</td>
<td>1.187397</td>
</tr>
<tr class="odd">
<td>15</td>
<td>1.250600</td>
<td>1.179421</td>
</tr>
<tr class="even">
<td>16</td>
<td>1.231900</td>
<td>1.165661</td>
</tr>
<tr class="odd">
<td>17</td>
<td>1.147900</td>
<td>1.129974</td>
</tr>
<tr class="even">
<td>18</td>
<td>1.146600</td>
<td>1.117911</td>
</tr>
<tr class="odd">
<td>19</td>
<td>1.113800</td>
<td>1.109535</td>
</tr>
<tr class="even">
<td>20</td>
<td>1.115300</td>
<td>1.096120</td>
</tr>
<tr class="odd">
<td>21</td>
<td>1.089400</td>
<td>1.078995</td>
</tr>
<tr class="even">
<td>22</td>
<td>1.069100</td>
<td>1.087004</td>
</tr>
<tr class="odd">
<td>23</td>
<td>1.061900</td>
<td>1.080366</td>
</tr>
<tr class="even">
<td>24</td>
<td>1.045900</td>
<td>1.071728</td>
</tr>
<tr class="odd">
<td>25</td>
<td>1.036300</td>
<td>1.070385</td>
</tr>
</tbody>
</table>
<p>
</p></div>
</div>
<p>TK - Note: You might get the following issue (negative bounding box coordinate predictions), can try again for more stable predictions (predictions are inherently random to begin with) or use a learning rate warmup to help stabilize predictions:</p>
<blockquote class="blockquote">
<p>ValueError: boxes1 must be in [x0, y0, x1, y1] (corner) format, but got tensor([[ 0.5796, 0.5566, 0.9956, 0.9492], [ 0.5718, 0.0610, 0.7202, 0.1738], [ 0.8218, 0.5107, 0.9878, 0.6289], â€¦, [ 0.1379, 0.1403, 0.6709, 0.6138], [ 0.7471, 0.4319, 1.0088, 0.5864], [-0.0660, 0.2052, 0.2067, 0.5107]], device=â€˜cuda:0â€™, dtype=torch.float16)</p>
</blockquote>
</section>
<section id="tk---save-the-trained-model" class="level3" data-number="12.6">
<h3 data-number="12.6" class="anchored" data-anchor-id="tk---save-the-trained-model"><span class="header-section-number">12.6</span> TK - Save the trained model</h3>
<div id="cell-180" class="cell" data-execution_count="125">
<div class="sourceCode cell-code" id="cb180"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb180-2"><a href="#cb180-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime</span>
<span id="cb180-3"><a href="#cb180-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-4"><a href="#cb180-4" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co">: update this save path so we know when the model was saved and what its parameters were</span></span>
<span id="cb180-5"><a href="#cb180-5" aria-hidden="true" tabindex="-1"></a>training_epochs_ <span class="op">=</span> training_args.num_train_epochs</span>
<span id="cb180-6"><a href="#cb180-6" aria-hidden="true" tabindex="-1"></a>learning_rate_ <span class="op">=</span> <span class="st">"</span><span class="sc">{:.0e}</span><span class="st">"</span>.<span class="bu">format</span>(training_args.learning_rate)</span>
<span id="cb180-7"><a href="#cb180-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-8"><a href="#cb180-8" aria-hidden="true" tabindex="-1"></a>model_v2_save_path <span class="op">=</span> <span class="ss">f"models/learn_hf_microsoft_detr_finetuned_trashify_box_dataset_only_manual_data_with_aug_</span><span class="sc">{</span>training_epochs_<span class="sc">}</span><span class="ss">_epochs_lr_</span><span class="sc">{</span>learning_rate_<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb180-9"><a href="#cb180-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Saving model to: </span><span class="sc">{</span>model_v2_save_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb180-10"><a href="#cb180-10" aria-hidden="true" tabindex="-1"></a>model_v2_trainer.save_model(model_v2_save_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Saving model to: models/learn_hf_microsoft_detr_finetuned_trashify_box_dataset_only_manual_data_with_aug_25_epochs_lr_1e-04</code></pre>
</div>
</div>
</section>
</section>
<section id="tk---upload-augmentation-model-to-hugging-face-hub" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="tk---upload-augmentation-model-to-hugging-face-hub"><span class="header-section-number">13</span> TK - Upload Augmentation Model to Hugging Face Hub</h2>
<div id="cell-182" class="cell">
<div class="sourceCode cell-code" id="cb182"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb182-1"><a href="#cb182-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Push the model to the Hugging Face Hub</span></span>
<span id="cb182-2"><a href="#cb182-2" aria-hidden="true" tabindex="-1"></a><span class="co"># TK Note: This will require you to have your Hugging Face account setup (e.g. see the setup guide, tk - link to setup guide)</span></span>
<span id="cb182-3"><a href="#cb182-3" aria-hidden="true" tabindex="-1"></a><span class="co"># TK - this will push to the parameter `output_dir="detr_finetuned_trashify_box_detector_with_data_aug"`</span></span>
<span id="cb182-4"><a href="#cb182-4" aria-hidden="true" tabindex="-1"></a>model_v2_trainer.push_to_hub(commit_message<span class="op">=</span><span class="st">"upload trashify object detection model with data augmentation"</span></span>
<span id="cb182-5"><a href="#cb182-5" aria-hidden="true" tabindex="-1"></a>                             <span class="co"># token=None, # Optional to add token manually</span></span>
<span id="cb182-6"><a href="#cb182-6" aria-hidden="true" tabindex="-1"></a>                            )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2e6829f58185476ba947dc09556c17bc","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"6a7acdabf33649d4a74398cb94b11d43","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"85d6fec5b30346dabe5c8d2fb6fda5ad","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="126">
<pre><code>CommitInfo(commit_url='https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug/commit/2f5f3ed0a205b13ddf2a0e3b76120412e33b0861', commit_message='upload trashify object detection model with data augmentation', commit_description='', oid='2f5f3ed0a205b13ddf2a0e3b76120412e33b0861', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug', endpoint='https://huggingface.co', repo_type='model', repo_id='mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug'), pr_revision=None, pr_num=None)</code></pre>
</div>
</div>
</section>
<section id="tk---compare-results-of-different-models" class="level2" data-number="14">
<h2 data-number="14" class="anchored" data-anchor-id="tk---compare-results-of-different-models"><span class="header-section-number">14</span> TK - Compare results of different models</h2>
<p>UPTOHERE - Showcase model 2 doing better because of augmentation (harder to learn)</p>
<ul>
<li>TK - Compare v1 model to v2
<ul>
<li>TK - Get model_v1 results into a variable and save it for later</li>
<li>Compare both of these as plots against each other, e.g.&nbsp;have the training curves for aug/no_aug on one plot and the curves for validation data for aug/no_aug on another plot</li>
</ul></li>
<li>TK - offer extensions to improve the model
<ul>
<li>TK - training model for longer, potentially using synthetic dataâ€¦?
<ul>
<li>TK - could I use 1000 high quality synthetic data samples to improve our model?</li>
</ul></li>
<li>TK - try use a different learning rate</li>
</ul></li>
</ul>
<div id="cell-184" class="cell" data-execution_count="127">
<div class="sourceCode cell-code" id="cb184"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb184-1"><a href="#cb184-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TK - Turn this workflow into a function e.g. def get_history_from_trainer() -&gt; df/dict of history</span></span>
<span id="cb184-2"><a href="#cb184-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_history_metrics_from_trainer(trainer):</span>
<span id="cb184-3"><a href="#cb184-3" aria-hidden="true" tabindex="-1"></a>    trainer_history <span class="op">=</span> trainer.state.log_history </span>
<span id="cb184-4"><a href="#cb184-4" aria-hidden="true" tabindex="-1"></a>    trainer_history_metrics <span class="op">=</span> trainer_history[:<span class="op">-</span><span class="dv">1</span>] <span class="co"># get everything except the training time metrics (we've seen these already)</span></span>
<span id="cb184-5"><a href="#cb184-5" aria-hidden="true" tabindex="-1"></a>    trainer_history_training_time <span class="op">=</span> trainer_history[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb184-6"><a href="#cb184-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb184-7"><a href="#cb184-7" aria-hidden="true" tabindex="-1"></a>    model_train_loss <span class="op">=</span> [item[<span class="st">"loss"</span>] <span class="cf">for</span> item <span class="kw">in</span> trainer_history_metrics <span class="cf">if</span> <span class="st">"loss"</span> <span class="kw">in</span> item.keys()]</span>
<span id="cb184-8"><a href="#cb184-8" aria-hidden="true" tabindex="-1"></a>    model_eval_loss <span class="op">=</span> [item[<span class="st">"eval_loss"</span>] <span class="cf">for</span> item <span class="kw">in</span> trainer_history_metrics <span class="cf">if</span> <span class="st">"eval_loss"</span> <span class="kw">in</span> item.keys()]</span>
<span id="cb184-9"><a href="#cb184-9" aria-hidden="true" tabindex="-1"></a>    model_learning_rate <span class="op">=</span> [item[<span class="st">"learning_rate"</span>] <span class="cf">for</span> item <span class="kw">in</span> trainer_history_metrics <span class="cf">if</span> <span class="st">"learning_rate"</span> <span class="kw">in</span> item.keys()] </span>
<span id="cb184-10"><a href="#cb184-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb184-11"><a href="#cb184-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model_train_loss, model_eval_loss, model_learning_rate, trainer_history_training_time</span>
<span id="cb184-12"><a href="#cb184-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb184-13"><a href="#cb184-13" aria-hidden="true" tabindex="-1"></a>model_v1_train_loss, model_v1_eval_loss, model_v1_learning_rate, _ <span class="op">=</span> get_history_metrics_from_trainer(trainer<span class="op">=</span>model_v1_trainer)</span>
<span id="cb184-14"><a href="#cb184-14" aria-hidden="true" tabindex="-1"></a>model_v2_train_loss, model_v2_eval_loss, model_v2_learning_rate, _ <span class="op">=</span> get_history_metrics_from_trainer(trainer<span class="op">=</span>model_v2_trainer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-185" class="cell" data-execution_count="128">
<div class="sourceCode cell-code" id="cb185"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb185-2"><a href="#cb185-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb185-3"><a href="#cb185-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot model loss curves against each other for same model</span></span>
<span id="cb185-4"><a href="#cb185-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Start from index 1 onwards to remove large loss spike at beginning of training </span></span>
<span id="cb185-5"><a href="#cb185-5" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb185-6"><a href="#cb185-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(model_v1_train_loss[<span class="dv">1</span>:], label<span class="op">=</span><span class="st">"Model V1 Train Loss"</span>)</span>
<span id="cb185-7"><a href="#cb185-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(model_v1_eval_loss[<span class="dv">1</span>:], label<span class="op">=</span><span class="st">"Model V1 Eval Loss"</span>)</span>
<span id="cb185-8"><a href="#cb185-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"Model V1 Loss Curves"</span>)</span>
<span id="cb185-9"><a href="#cb185-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb185-10"><a href="#cb185-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb185-11"><a href="#cb185-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].legend()</span>
<span id="cb185-12"><a href="#cb185-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb185-13"><a href="#cb185-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(model_v2_train_loss[<span class="dv">1</span>:], label<span class="op">=</span><span class="st">"Model V2 Train Loss"</span>)</span>
<span id="cb185-14"><a href="#cb185-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(model_v2_eval_loss[<span class="dv">1</span>:], label<span class="op">=</span><span class="st">"Model V2 Eval Loss"</span>)</span>
<span id="cb185-15"><a href="#cb185-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Model V2 Loss Curves"</span>)</span>
<span id="cb185-16"><a href="#cb185-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb185-17"><a href="#cb185-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb185-18"><a href="#cb185-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-100-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>tk - notice the overfitting begin to happen with model v1 (no data augmentation) but model v2 has less overfitting and achieves a lower validation loss</p>
<div id="cell-187" class="cell" data-execution_count="129">
<div class="sourceCode cell-code" id="cb186"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb186-1"><a href="#cb186-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb186-2"><a href="#cb186-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb186-3"><a href="#cb186-3" aria-hidden="true" tabindex="-1"></a>plt.plot(model_v1_learning_rate, label<span class="op">=</span><span class="st">"Model V1"</span>)</span>
<span id="cb186-4"><a href="#cb186-4" aria-hidden="true" tabindex="-1"></a>plt.plot(model_v2_learning_rate, label<span class="op">=</span><span class="st">"Model V2"</span>)</span>
<span id="cb186-5"><a href="#cb186-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Model Learning Rate vs. Epoch"</span>)</span>
<span id="cb186-6"><a href="#cb186-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Learning Rate"</span>)</span>
<span id="cb186-7"><a href="#cb186-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb186-8"><a href="#cb186-8" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-101-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-188" class="cell" data-execution_count="130">
<div class="sourceCode cell-code" id="cb187"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb187-1"><a href="#cb187-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot loss values against each other</span></span>
<span id="cb187-2"><a href="#cb187-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb187-3"><a href="#cb187-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-4"><a href="#cb187-4" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb187-5"><a href="#cb187-5" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(model_v1_train_loss))</span>
<span id="cb187-6"><a href="#cb187-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(model_v1_train_loss[<span class="dv">1</span>:], label<span class="op">=</span><span class="st">"Model 1 Training Loss"</span>)</span>
<span id="cb187-7"><a href="#cb187-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(model_v2_train_loss[<span class="dv">1</span>:], label<span class="op">=</span><span class="st">"Model 2 Training Loss"</span>)</span>
<span id="cb187-8"><a href="#cb187-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"Model Training Loss Curves"</span>)</span>
<span id="cb187-9"><a href="#cb187-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Training Loss"</span>)</span>
<span id="cb187-10"><a href="#cb187-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb187-11"><a href="#cb187-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].legend()</span>
<span id="cb187-12"><a href="#cb187-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-13"><a href="#cb187-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(model_v1_eval_loss[<span class="dv">1</span>:], label<span class="op">=</span><span class="st">"Model 1 Eval Loss"</span>)</span>
<span id="cb187-14"><a href="#cb187-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(model_v2_eval_loss[<span class="dv">1</span>:], label<span class="op">=</span><span class="st">"Model 2 Eval Loss"</span>)</span>
<span id="cb187-15"><a href="#cb187-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Model Eval Loss Curves"</span>)</span>
<span id="cb187-16"><a href="#cb187-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">"Eval Loss"</span>)</span>
<span id="cb187-17"><a href="#cb187-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb187-18"><a href="#cb187-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-102-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>tk - describe the loss curves here, model 2 curves may be higher for training loss but they really start to accelerate on the evaluation set towards the end</p>
</section>
<section id="tk---create-demo-with-augmentation-model" class="level2" data-number="15">
<h2 data-number="15" class="anchored" data-anchor-id="tk---create-demo-with-augmentation-model"><span class="header-section-number">15</span> TK - Create demo with Augmentation Model</h2>
<div id="cell-191" class="cell" data-execution_count="131">
<div class="sourceCode cell-code" id="cb188"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make directory for demo</span></span>
<span id="cb188-2"><a href="#cb188-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb188-3"><a href="#cb188-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb188-4"><a href="#cb188-4" aria-hidden="true" tabindex="-1"></a>trashify_data_aug_model_dir <span class="op">=</span> Path(<span class="st">"demos/trashify_object_detector_data_aug_model/"</span>)</span>
<span id="cb188-5"><a href="#cb188-5" aria-hidden="true" tabindex="-1"></a>trashify_data_aug_model_dir.mkdir(exist_ok<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-192" class="cell" data-execution_count="132">
<div class="sourceCode cell-code" id="cb189"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile demos<span class="op">/</span>trashify_object_detector_data_aug_model<span class="op">/</span>README.md</span>
<span id="cb189-2"><a href="#cb189-2" aria-hidden="true" tabindex="-1"></a><span class="op">---</span></span>
<span id="cb189-3"><a href="#cb189-3" aria-hidden="true" tabindex="-1"></a>title: Trashify Demo V2 ðŸš®</span>
<span id="cb189-4"><a href="#cb189-4" aria-hidden="true" tabindex="-1"></a>emoji: ðŸ—‘ï¸</span>
<span id="cb189-5"><a href="#cb189-5" aria-hidden="true" tabindex="-1"></a>colorFrom: purple</span>
<span id="cb189-6"><a href="#cb189-6" aria-hidden="true" tabindex="-1"></a>colorTo: blue</span>
<span id="cb189-7"><a href="#cb189-7" aria-hidden="true" tabindex="-1"></a>sdk: gradio</span>
<span id="cb189-8"><a href="#cb189-8" aria-hidden="true" tabindex="-1"></a>sdk_version: <span class="fl">4.40.0</span></span>
<span id="cb189-9"><a href="#cb189-9" aria-hidden="true" tabindex="-1"></a>app_file: app.py</span>
<span id="cb189-10"><a href="#cb189-10" aria-hidden="true" tabindex="-1"></a>pinned: false</span>
<span id="cb189-11"><a href="#cb189-11" aria-hidden="true" tabindex="-1"></a>license: apache<span class="op">-</span><span class="fl">2.0</span></span>
<span id="cb189-12"><a href="#cb189-12" aria-hidden="true" tabindex="-1"></a><span class="op">---</span></span>
<span id="cb189-13"><a href="#cb189-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-14"><a href="#cb189-14" aria-hidden="true" tabindex="-1"></a><span class="co"># ðŸš® Trashify Object Detector Demo V2</span></span>
<span id="cb189-15"><a href="#cb189-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-16"><a href="#cb189-16" aria-hidden="true" tabindex="-1"></a>Object detection demo to detect `trash`, `bin`, `hand`, `trash_arm`, `not_trash`, `not_bin`, `not_hand`. </span>
<span id="cb189-17"><a href="#cb189-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-18"><a href="#cb189-18" aria-hidden="true" tabindex="-1"></a>Used <span class="im">as</span> example <span class="cf">for</span> encouraging people to cleanup their local area.</span>
<span id="cb189-19"><a href="#cb189-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-20"><a href="#cb189-20" aria-hidden="true" tabindex="-1"></a>If `trash`, `hand`, `bin` <span class="bu">all</span> detected <span class="op">=</span> <span class="op">+</span><span class="dv">1</span> point.</span>
<span id="cb189-21"><a href="#cb189-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-22"><a href="#cb189-22" aria-hidden="true" tabindex="-1"></a><span class="co">## Dataset</span></span>
<span id="cb189-23"><a href="#cb189-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-24"><a href="#cb189-24" aria-hidden="true" tabindex="-1"></a>All Trashify models are trained on a custom hand<span class="op">-</span>labelled dataset of people picking up trash <span class="kw">and</span> placing it <span class="kw">in</span> a <span class="bu">bin</span>.</span>
<span id="cb189-25"><a href="#cb189-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-26"><a href="#cb189-26" aria-hidden="true" tabindex="-1"></a>The dataset can be found on Hugging Face <span class="im">as</span> [`mrdbourke<span class="op">/</span>trashify_manual_labelled_images`](https:<span class="op">//</span>huggingface.co<span class="op">/</span>datasets<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_manual_labelled_images).</span>
<span id="cb189-27"><a href="#cb189-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-28"><a href="#cb189-28" aria-hidden="true" tabindex="-1"></a><span class="co">## Demos</span></span>
<span id="cb189-29"><a href="#cb189-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-30"><a href="#cb189-30" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> [V1](https:<span class="op">//</span>huggingface.co<span class="op">/</span>spaces<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_demo_v1) <span class="op">=</span> Fine<span class="op">-</span>tuned DETR model trained <span class="op">*</span>without<span class="op">*</span> data augmentation.</span>
<span id="cb189-31"><a href="#cb189-31" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> [V2](https:<span class="op">//</span>huggingface.co<span class="op">/</span>spaces<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_demo_v2) <span class="op">=</span> Fine<span class="op">-</span>tuned DETR model trained <span class="op">*</span><span class="cf">with</span><span class="op">*</span> data augmentation.</span>
<span id="cb189-32"><a href="#cb189-32" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> [V3](https:<span class="op">//</span>huggingface.co<span class="op">/</span>spaces<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_demo_v3) <span class="op">=</span> Fine<span class="op">-</span>tuned DETR model trained <span class="op">*</span><span class="cf">with</span><span class="op">*</span> data augmentation (same <span class="im">as</span> V2) <span class="cf">with</span> an NMS (Non Maximum Suppression) post<span class="op">-</span>processing step.</span>
<span id="cb189-33"><a href="#cb189-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-34"><a href="#cb189-34" aria-hidden="true" tabindex="-1"></a>TK <span class="op">-</span> finish the README.md <span class="op">+</span> update <span class="cf">with</span> links to materials</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting demos/trashify_object_detector_data_aug_model/README.md</code></pre>
</div>
</div>
<div id="cell-193" class="cell" data-execution_count="133">
<div class="sourceCode cell-code" id="cb191"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile demos<span class="op">/</span>trashify_object_detector_data_aug_model<span class="op">/</span>requirements.txt</span>
<span id="cb191-2"><a href="#cb191-2" aria-hidden="true" tabindex="-1"></a>timm</span>
<span id="cb191-3"><a href="#cb191-3" aria-hidden="true" tabindex="-1"></a>gradio</span>
<span id="cb191-4"><a href="#cb191-4" aria-hidden="true" tabindex="-1"></a>torch</span>
<span id="cb191-5"><a href="#cb191-5" aria-hidden="true" tabindex="-1"></a>transformers</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting demos/trashify_object_detector_data_aug_model/requirements.txt</code></pre>
</div>
</div>
<div id="cell-194" class="cell" data-execution_count="134">
<div class="sourceCode cell-code" id="cb193"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile demos<span class="op">/</span>trashify_object_detector_data_aug_model<span class="op">/</span>app.py</span>
<span id="cb193-2"><a href="#cb193-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb193-3"><a href="#cb193-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb193-4"><a href="#cb193-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image, ImageDraw, ImageFont</span>
<span id="cb193-5"><a href="#cb193-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-6"><a href="#cb193-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoImageProcessor</span>
<span id="cb193-7"><a href="#cb193-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForObjectDetection</span>
<span id="cb193-8"><a href="#cb193-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-9"><a href="#cb193-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Can load from Hugging Face or can load from local.</span></span>
<span id="cb193-10"><a href="#cb193-10" aria-hidden="true" tabindex="-1"></a><span class="co"># You will have to replace {mrdbourke} for your own username if the model is on your Hugging Face account.</span></span>
<span id="cb193-11"><a href="#cb193-11" aria-hidden="true" tabindex="-1"></a>model_save_path <span class="op">=</span> <span class="st">"mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug"</span> </span>
<span id="cb193-12"><a href="#cb193-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-13"><a href="#cb193-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model and preprocessor</span></span>
<span id="cb193-14"><a href="#cb193-14" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoImageProcessor.from_pretrained(model_save_path)</span>
<span id="cb193-15"><a href="#cb193-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForObjectDetection.from_pretrained(model_save_path)</span>
<span id="cb193-16"><a href="#cb193-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-17"><a href="#cb193-17" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb193-18"><a href="#cb193-18" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb193-19"><a href="#cb193-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-20"><a href="#cb193-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the id2label dictionary from the model</span></span>
<span id="cb193-21"><a href="#cb193-21" aria-hidden="true" tabindex="-1"></a>id2label <span class="op">=</span> model.config.id2label</span>
<span id="cb193-22"><a href="#cb193-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-23"><a href="#cb193-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up a colour dictionary for plotting boxes with different colours</span></span>
<span id="cb193-24"><a href="#cb193-24" aria-hidden="true" tabindex="-1"></a>color_dict <span class="op">=</span> {   </span>
<span id="cb193-25"><a href="#cb193-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">"bin"</span>: <span class="st">"green"</span>,</span>
<span id="cb193-26"><a href="#cb193-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">"trash"</span>: <span class="st">"blue"</span>,</span>
<span id="cb193-27"><a href="#cb193-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">"hand"</span>: <span class="st">"purple"</span>,</span>
<span id="cb193-28"><a href="#cb193-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">"trash_arm"</span>: <span class="st">"yellow"</span>,</span>
<span id="cb193-29"><a href="#cb193-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not_trash"</span>: <span class="st">"red"</span>,</span>
<span id="cb193-30"><a href="#cb193-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not_bin"</span>: <span class="st">"red"</span>,</span>
<span id="cb193-31"><a href="#cb193-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not_hand"</span>: <span class="st">"red"</span>,</span>
<span id="cb193-32"><a href="#cb193-32" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb193-33"><a href="#cb193-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-34"><a href="#cb193-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Create helper functions for seeing if items from one list are in another </span></span>
<span id="cb193-35"><a href="#cb193-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> any_in_list(list_a, list_b):</span>
<span id="cb193-36"><a href="#cb193-36" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Returns True if any item from list_a is in list_b, otherwise False."</span></span>
<span id="cb193-37"><a href="#cb193-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">any</span>(item <span class="kw">in</span> list_b <span class="cf">for</span> item <span class="kw">in</span> list_a)</span>
<span id="cb193-38"><a href="#cb193-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-39"><a href="#cb193-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> all_in_list(list_a, list_b):</span>
<span id="cb193-40"><a href="#cb193-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Returns True if all items from list_a are in list_b, otherwise False."</span></span>
<span id="cb193-41"><a href="#cb193-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">all</span>(item <span class="kw">in</span> list_b <span class="cf">for</span> item <span class="kw">in</span> list_a)</span>
<span id="cb193-42"><a href="#cb193-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-43"><a href="#cb193-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_on_image(image, conf_threshold):</span>
<span id="cb193-44"><a href="#cb193-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb193-45"><a href="#cb193-45" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> image_processor(images<span class="op">=</span>[image], return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb193-46"><a href="#cb193-46" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>inputs.to(device))</span>
<span id="cb193-47"><a href="#cb193-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-48"><a href="#cb193-48" aria-hidden="true" tabindex="-1"></a>        target_sizes <span class="op">=</span> torch.tensor([[image.size[<span class="dv">1</span>], image.size[<span class="dv">0</span>]]]) <span class="co"># height, width </span></span>
<span id="cb193-49"><a href="#cb193-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-50"><a href="#cb193-50" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> image_processor.post_process_object_detection(outputs,</span>
<span id="cb193-51"><a href="#cb193-51" aria-hidden="true" tabindex="-1"></a>                                                                threshold<span class="op">=</span>conf_threshold,</span>
<span id="cb193-52"><a href="#cb193-52" aria-hidden="true" tabindex="-1"></a>                                                                target_sizes<span class="op">=</span>target_sizes)[<span class="dv">0</span>]</span>
<span id="cb193-53"><a href="#cb193-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return all items in results to CPU</span></span>
<span id="cb193-54"><a href="#cb193-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> key, value <span class="kw">in</span> results.items():</span>
<span id="cb193-55"><a href="#cb193-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb193-56"><a href="#cb193-56" aria-hidden="true" tabindex="-1"></a>            results[key] <span class="op">=</span> value.item().cpu() <span class="co"># can't get scalar as .item() so add try/except block</span></span>
<span id="cb193-57"><a href="#cb193-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span>:</span>
<span id="cb193-58"><a href="#cb193-58" aria-hidden="true" tabindex="-1"></a>            results[key] <span class="op">=</span> value.cpu()</span>
<span id="cb193-59"><a href="#cb193-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-60"><a href="#cb193-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Can return results as plotted on a PIL image (then display the image)</span></span>
<span id="cb193-61"><a href="#cb193-61" aria-hidden="true" tabindex="-1"></a>    draw <span class="op">=</span> ImageDraw.Draw(image)</span>
<span id="cb193-62"><a href="#cb193-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-63"><a href="#cb193-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get a font from ImageFont</span></span>
<span id="cb193-64"><a href="#cb193-64" aria-hidden="true" tabindex="-1"></a>    font <span class="op">=</span> ImageFont.load_default(size<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb193-65"><a href="#cb193-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-66"><a href="#cb193-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get class names as text for print out</span></span>
<span id="cb193-67"><a href="#cb193-67" aria-hidden="true" tabindex="-1"></a>    class_name_text_labels <span class="op">=</span> []</span>
<span id="cb193-68"><a href="#cb193-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-69"><a href="#cb193-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> box, score, label <span class="kw">in</span> <span class="bu">zip</span>(results[<span class="st">"boxes"</span>], results[<span class="st">"scores"</span>], results[<span class="st">"labels"</span>]):</span>
<span id="cb193-70"><a href="#cb193-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create coordinates</span></span>
<span id="cb193-71"><a href="#cb193-71" aria-hidden="true" tabindex="-1"></a>        x, y, x2, y2 <span class="op">=</span> <span class="bu">tuple</span>(box.tolist())</span>
<span id="cb193-72"><a href="#cb193-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-73"><a href="#cb193-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get label_name</span></span>
<span id="cb193-74"><a href="#cb193-74" aria-hidden="true" tabindex="-1"></a>        label_name <span class="op">=</span> id2label[label.item()]</span>
<span id="cb193-75"><a href="#cb193-75" aria-hidden="true" tabindex="-1"></a>        targ_color <span class="op">=</span> color_dict[label_name]</span>
<span id="cb193-76"><a href="#cb193-76" aria-hidden="true" tabindex="-1"></a>        class_name_text_labels.append(label_name)</span>
<span id="cb193-77"><a href="#cb193-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-78"><a href="#cb193-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw the rectangle</span></span>
<span id="cb193-79"><a href="#cb193-79" aria-hidden="true" tabindex="-1"></a>        draw.rectangle(xy<span class="op">=</span>(x, y, x2, y2), </span>
<span id="cb193-80"><a href="#cb193-80" aria-hidden="true" tabindex="-1"></a>                       outline<span class="op">=</span>targ_color,</span>
<span id="cb193-81"><a href="#cb193-81" aria-hidden="true" tabindex="-1"></a>                       width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb193-82"><a href="#cb193-82" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb193-83"><a href="#cb193-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a text string to display</span></span>
<span id="cb193-84"><a href="#cb193-84" aria-hidden="true" tabindex="-1"></a>        text_string_to_show <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>label_name<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score.item(), <span class="dv">3</span>)<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb193-85"><a href="#cb193-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-86"><a href="#cb193-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw the text on the image</span></span>
<span id="cb193-87"><a href="#cb193-87" aria-hidden="true" tabindex="-1"></a>        draw.text(xy<span class="op">=</span>(x, y),</span>
<span id="cb193-88"><a href="#cb193-88" aria-hidden="true" tabindex="-1"></a>                  text<span class="op">=</span>text_string_to_show,</span>
<span id="cb193-89"><a href="#cb193-89" aria-hidden="true" tabindex="-1"></a>                  fill<span class="op">=</span><span class="st">"white"</span>,</span>
<span id="cb193-90"><a href="#cb193-90" aria-hidden="true" tabindex="-1"></a>                  font<span class="op">=</span>font)</span>
<span id="cb193-91"><a href="#cb193-91" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb193-92"><a href="#cb193-92" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove the draw each time</span></span>
<span id="cb193-93"><a href="#cb193-93" aria-hidden="true" tabindex="-1"></a>    <span class="kw">del</span> draw</span>
<span id="cb193-94"><a href="#cb193-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-95"><a href="#cb193-95" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setup blank string to print out</span></span>
<span id="cb193-96"><a href="#cb193-96" aria-hidden="true" tabindex="-1"></a>    return_string <span class="op">=</span> <span class="st">""</span></span>
<span id="cb193-97"><a href="#cb193-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-98"><a href="#cb193-98" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setup list of target items to discover</span></span>
<span id="cb193-99"><a href="#cb193-99" aria-hidden="true" tabindex="-1"></a>    target_items <span class="op">=</span> [<span class="st">"trash"</span>, <span class="st">"bin"</span>, <span class="st">"hand"</span>]</span>
<span id="cb193-100"><a href="#cb193-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-101"><a href="#cb193-101" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If no items detected or trash, bin, hand not in list, return notification </span></span>
<span id="cb193-102"><a href="#cb193-102" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="bu">len</span>(class_name_text_labels) <span class="op">==</span> <span class="dv">0</span>) <span class="kw">or</span> <span class="kw">not</span> (any_in_list(list_a<span class="op">=</span>target_items, list_b<span class="op">=</span>class_name_text_labels)):</span>
<span id="cb193-103"><a href="#cb193-103" aria-hidden="true" tabindex="-1"></a>        return_string <span class="op">=</span> <span class="ss">f"No trash, bin or hand detected at confidence threshold </span><span class="sc">{</span>conf_threshold<span class="sc">}</span><span class="ss">. Try another image or lowering the confidence threshold."</span></span>
<span id="cb193-104"><a href="#cb193-104" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, return_string</span>
<span id="cb193-105"><a href="#cb193-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-106"><a href="#cb193-106" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If there are some missing, print the ones which are missing</span></span>
<span id="cb193-107"><a href="#cb193-107" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="kw">not</span> all_in_list(list_a<span class="op">=</span>target_items, list_b<span class="op">=</span>class_name_text_labels):</span>
<span id="cb193-108"><a href="#cb193-108" aria-hidden="true" tabindex="-1"></a>        missing_items <span class="op">=</span> []</span>
<span id="cb193-109"><a href="#cb193-109" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> item <span class="kw">in</span> target_items:</span>
<span id="cb193-110"><a href="#cb193-110" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> item <span class="kw">not</span> <span class="kw">in</span> class_name_text_labels:</span>
<span id="cb193-111"><a href="#cb193-111" aria-hidden="true" tabindex="-1"></a>                missing_items.append(item)</span>
<span id="cb193-112"><a href="#cb193-112" aria-hidden="true" tabindex="-1"></a>        return_string <span class="op">=</span> <span class="ss">f"Detected the following items: </span><span class="sc">{</span>class_name_text_labels<span class="sc">}</span><span class="ss">. But missing the following in order to get +1: </span><span class="sc">{</span>missing_items<span class="sc">}</span><span class="ss">. If this is an error, try another image or altering the confidence threshold. Otherwise, the model may need to be updated with better data."</span></span>
<span id="cb193-113"><a href="#cb193-113" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb193-114"><a href="#cb193-114" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If all 3 trash, bin, hand occur = + 1</span></span>
<span id="cb193-115"><a href="#cb193-115" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> all_in_list(list_a<span class="op">=</span>target_items, list_b<span class="op">=</span>class_name_text_labels):</span>
<span id="cb193-116"><a href="#cb193-116" aria-hidden="true" tabindex="-1"></a>        return_string <span class="op">=</span> <span class="ss">f"+1! Found the following items: </span><span class="sc">{</span>class_name_text_labels<span class="sc">}</span><span class="ss">, thank you for cleaning up the area!"</span></span>
<span id="cb193-117"><a href="#cb193-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-118"><a href="#cb193-118" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(return_string)</span>
<span id="cb193-119"><a href="#cb193-119" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb193-120"><a href="#cb193-120" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image, return_string</span>
<span id="cb193-121"><a href="#cb193-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-122"><a href="#cb193-122" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the interface</span></span>
<span id="cb193-123"><a href="#cb193-123" aria-hidden="true" tabindex="-1"></a>demo <span class="op">=</span> gr.Interface(</span>
<span id="cb193-124"><a href="#cb193-124" aria-hidden="true" tabindex="-1"></a>    fn<span class="op">=</span>predict_on_image,</span>
<span id="cb193-125"><a href="#cb193-125" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>[</span>
<span id="cb193-126"><a href="#cb193-126" aria-hidden="true" tabindex="-1"></a>        gr.Image(<span class="bu">type</span><span class="op">=</span><span class="st">"pil"</span>, label<span class="op">=</span><span class="st">"Target Image"</span>),</span>
<span id="cb193-127"><a href="#cb193-127" aria-hidden="true" tabindex="-1"></a>        gr.Slider(minimum<span class="op">=</span><span class="dv">0</span>, maximum<span class="op">=</span><span class="dv">1</span>, value<span class="op">=</span><span class="fl">0.25</span>, label<span class="op">=</span><span class="st">"Confidence Threshold"</span>)</span>
<span id="cb193-128"><a href="#cb193-128" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb193-129"><a href="#cb193-129" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>[</span>
<span id="cb193-130"><a href="#cb193-130" aria-hidden="true" tabindex="-1"></a>        gr.Image(<span class="bu">type</span><span class="op">=</span><span class="st">"pil"</span>, label<span class="op">=</span><span class="st">"Image Output"</span>),</span>
<span id="cb193-131"><a href="#cb193-131" aria-hidden="true" tabindex="-1"></a>        gr.Text(label<span class="op">=</span><span class="st">"Text Output"</span>)</span>
<span id="cb193-132"><a href="#cb193-132" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb193-133"><a href="#cb193-133" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"ðŸš® Trashify Object Detection Demo V2"</span>,</span>
<span id="cb193-134"><a href="#cb193-134" aria-hidden="true" tabindex="-1"></a>    description<span class="op">=</span><span class="st">"""Help clean up your local area! Upload an image and get +1 if there is all of the following items detected: trash, bin, hand.</span></span>
<span id="cb193-135"><a href="#cb193-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-136"><a href="#cb193-136" aria-hidden="true" tabindex="-1"></a><span class="st">    The [model](https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug) in V2 has been trained with data augmentation preprocessing (color jitter, horizontal flipping) to improve robustness. </span></span>
<span id="cb193-137"><a href="#cb193-137" aria-hidden="true" tabindex="-1"></a><span class="st">    """</span>,</span>
<span id="cb193-138"><a href="#cb193-138" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Examples come in the form of a list of lists, where each inner list contains elements to prefill the `inputs` parameter with</span></span>
<span id="cb193-139"><a href="#cb193-139" aria-hidden="true" tabindex="-1"></a>    examples<span class="op">=</span>[</span>
<span id="cb193-140"><a href="#cb193-140" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"examples/trashify_example_1.jpeg"</span>, <span class="fl">0.25</span>],</span>
<span id="cb193-141"><a href="#cb193-141" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"examples/trashify_example_2.jpeg"</span>, <span class="fl">0.25</span>],</span>
<span id="cb193-142"><a href="#cb193-142" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"examples/trashify_example_3.jpeg"</span>, <span class="fl">0.25</span>]</span>
<span id="cb193-143"><a href="#cb193-143" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb193-144"><a href="#cb193-144" aria-hidden="true" tabindex="-1"></a>    cache_examples<span class="op">=</span><span class="va">True</span></span>
<span id="cb193-145"><a href="#cb193-145" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb193-146"><a href="#cb193-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-147"><a href="#cb193-147" aria-hidden="true" tabindex="-1"></a><span class="co"># Launch the demo</span></span>
<span id="cb193-148"><a href="#cb193-148" aria-hidden="true" tabindex="-1"></a>demo.launch()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting demos/trashify_object_detector_data_aug_model/app.py</code></pre>
</div>
</div>
<div id="cell-195" class="cell" data-execution_count="135">
<div class="sourceCode cell-code" id="cb195"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Import the required methods for uploading to the Hugging Face Hub</span></span>
<span id="cb195-2"><a href="#cb195-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> (</span>
<span id="cb195-3"><a href="#cb195-3" aria-hidden="true" tabindex="-1"></a>    create_repo,</span>
<span id="cb195-4"><a href="#cb195-4" aria-hidden="true" tabindex="-1"></a>    get_full_repo_name,</span>
<span id="cb195-5"><a href="#cb195-5" aria-hidden="true" tabindex="-1"></a>    upload_file, <span class="co"># for uploading a single file (if necessary)</span></span>
<span id="cb195-6"><a href="#cb195-6" aria-hidden="true" tabindex="-1"></a>    upload_folder <span class="co"># for uploading multiple files (in a folder)</span></span>
<span id="cb195-7"><a href="#cb195-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb195-8"><a href="#cb195-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-9"><a href="#cb195-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Define the parameters we'd like to use for the upload</span></span>
<span id="cb195-10"><a href="#cb195-10" aria-hidden="true" tabindex="-1"></a>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD <span class="op">=</span> <span class="st">"demos/trashify_object_detector_data_aug_model"</span> <span class="co"># TK - update this path </span></span>
<span id="cb195-11"><a href="#cb195-11" aria-hidden="true" tabindex="-1"></a>HF_TARGET_SPACE_NAME <span class="op">=</span> <span class="st">"trashify_demo_v2"</span></span>
<span id="cb195-12"><a href="#cb195-12" aria-hidden="true" tabindex="-1"></a>HF_REPO_TYPE <span class="op">=</span> <span class="st">"space"</span> <span class="co"># we're creating a Hugging Face Space</span></span>
<span id="cb195-13"><a href="#cb195-13" aria-hidden="true" tabindex="-1"></a>HF_SPACE_SDK <span class="op">=</span> <span class="st">"gradio"</span></span>
<span id="cb195-14"><a href="#cb195-14" aria-hidden="true" tabindex="-1"></a>HF_TOKEN <span class="op">=</span> <span class="st">""</span> <span class="co"># optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)</span></span>
<span id="cb195-15"><a href="#cb195-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-16"><a href="#cb195-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Create a Space repository on Hugging Face Hub </span></span>
<span id="cb195-17"><a href="#cb195-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Creating repo on Hugging Face Hub with name: </span><span class="sc">{</span>HF_TARGET_SPACE_NAME<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb195-18"><a href="#cb195-18" aria-hidden="true" tabindex="-1"></a>create_repo(</span>
<span id="cb195-19"><a href="#cb195-19" aria-hidden="true" tabindex="-1"></a>    repo_id<span class="op">=</span>HF_TARGET_SPACE_NAME,</span>
<span id="cb195-20"><a href="#cb195-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)</span></span>
<span id="cb195-21"><a href="#cb195-21" aria-hidden="true" tabindex="-1"></a>    repo_type<span class="op">=</span>HF_REPO_TYPE,</span>
<span id="cb195-22"><a href="#cb195-22" aria-hidden="true" tabindex="-1"></a>    private<span class="op">=</span><span class="va">False</span>, <span class="co"># set to True if you don't want your Space to be accessible to others</span></span>
<span id="cb195-23"><a href="#cb195-23" aria-hidden="true" tabindex="-1"></a>    space_sdk<span class="op">=</span>HF_SPACE_SDK,</span>
<span id="cb195-24"><a href="#cb195-24" aria-hidden="true" tabindex="-1"></a>    exist_ok<span class="op">=</span><span class="va">True</span>, <span class="co"># set to False if you want an error to raise if the repo_id already exists </span></span>
<span id="cb195-25"><a href="#cb195-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb195-26"><a href="#cb195-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-27"><a href="#cb195-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})</span></span>
<span id="cb195-28"><a href="#cb195-28" aria-hidden="true" tabindex="-1"></a>full_hf_repo_name <span class="op">=</span> get_full_repo_name(model_id<span class="op">=</span>HF_TARGET_SPACE_NAME)</span>
<span id="cb195-29"><a href="#cb195-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Full Hugging Face Hub repo name: </span><span class="sc">{</span>full_hf_repo_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb195-30"><a href="#cb195-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-31"><a href="#cb195-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Upload our demo folder</span></span>
<span id="cb195-32"><a href="#cb195-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Uploading </span><span class="sc">{</span>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD<span class="sc">}</span><span class="ss"> to repo: </span><span class="sc">{</span>full_hf_repo_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb195-33"><a href="#cb195-33" aria-hidden="true" tabindex="-1"></a>folder_upload_url <span class="op">=</span> upload_folder(</span>
<span id="cb195-34"><a href="#cb195-34" aria-hidden="true" tabindex="-1"></a>    repo_id<span class="op">=</span>full_hf_repo_name,</span>
<span id="cb195-35"><a href="#cb195-35" aria-hidden="true" tabindex="-1"></a>    folder_path<span class="op">=</span>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,</span>
<span id="cb195-36"><a href="#cb195-36" aria-hidden="true" tabindex="-1"></a>    path_in_repo<span class="op">=</span><span class="st">"."</span>, <span class="co"># upload our folder to the root directory ("." means "base" or "root", this is the default)</span></span>
<span id="cb195-37"><a href="#cb195-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># token=HF_TOKEN, # optional: set token manually</span></span>
<span id="cb195-38"><a href="#cb195-38" aria-hidden="true" tabindex="-1"></a>    repo_type<span class="op">=</span>HF_REPO_TYPE,</span>
<span id="cb195-39"><a href="#cb195-39" aria-hidden="true" tabindex="-1"></a>    commit_message<span class="op">=</span><span class="st">"Uploading Trashify V2 box detection model (with data augmentation) app.py"</span></span>
<span id="cb195-40"><a href="#cb195-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb195-41"><a href="#cb195-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Demo folder successfully uploaded with commit URL: </span><span class="sc">{</span>folder_upload_url<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Creating repo on Hugging Face Hub with name: trashify_demo_v2
[INFO] Full Hugging Face Hub repo name: mrdbourke/trashify_demo_v2
[INFO] Uploading demos/trashify_object_detector_data_aug_model to repo: mrdbourke/trashify_demo_v2
[INFO] Demo folder successfully uploaded with commit URL: https://huggingface.co/spaces/mrdbourke/trashify_demo_v2/tree/main/.</code></pre>
</div>
</div>
<div id="cell-196" class="cell" data-execution_count="136">
<div class="sourceCode cell-code" id="cb197"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Next:</span></span>
<span id="cb197-2"><a href="#cb197-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Upload augmentation model to Hugging Face Hub âœ…</span></span>
<span id="cb197-3"><a href="#cb197-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create demo for augmentation model âœ…</span></span>
<span id="cb197-4"><a href="#cb197-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare results from augmentation model to non-augmentation model âœ…</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="tk---make-a-prediction-on-a-random-test-sample-with-model-using-data-aug-model" class="level3" data-number="15.1">
<h3 data-number="15.1" class="anchored" data-anchor-id="tk---make-a-prediction-on-a-random-test-sample-with-model-using-data-aug-model"><span class="header-section-number">15.1</span> TK - Make a prediction on a random test sample with model using data aug model</h3>
<div id="cell-198" class="cell" data-execution_count="137">
<div class="sourceCode cell-code" id="cb198"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb198-1"><a href="#cb198-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a random sample from the test preds</span></span>
<span id="cb198-2"><a href="#cb198-2" aria-hidden="true" tabindex="-1"></a>random_test_pred_index <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(processed_dataset[<span class="st">"test"</span>]))</span>
<span id="cb198-3"><a href="#cb198-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Making predictions on test item with index: </span><span class="sc">{</span>random_test_pred_index<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb198-4"><a href="#cb198-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb198-5"><a href="#cb198-5" aria-hidden="true" tabindex="-1"></a>random_test_sample <span class="op">=</span> processed_dataset[<span class="st">"test"</span>][random_test_pred_index]</span>
<span id="cb198-6"><a href="#cb198-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb198-7"><a href="#cb198-7" aria-hidden="true" tabindex="-1"></a><span class="co"># # Do a single forward pass with the model</span></span>
<span id="cb198-8"><a href="#cb198-8" aria-hidden="true" tabindex="-1"></a>random_test_sample_outputs <span class="op">=</span> model_aug(pixel_values<span class="op">=</span>random_test_sample[<span class="st">"pixel_values"</span>].unsqueeze(<span class="dv">0</span>).to(<span class="st">"cuda"</span>), <span class="co"># model expects input [batch_size, color_channels, height, width]</span></span>
<span id="cb198-9"><a href="#cb198-9" aria-hidden="true" tabindex="-1"></a>                                       pixel_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb198-10"><a href="#cb198-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb198-11"><a href="#cb198-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Post process a random item from test preds</span></span>
<span id="cb198-12"><a href="#cb198-12" aria-hidden="true" tabindex="-1"></a>random_test_sample_outputs_post_processed <span class="op">=</span> image_processor.post_process_object_detection(</span>
<span id="cb198-13"><a href="#cb198-13" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>random_test_sample_outputs,</span>
<span id="cb198-14"><a href="#cb198-14" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span><span class="fl">0.25</span>, <span class="co"># prediction probability threshold for boxes (note: boxes from an untrained model will likely be bad)</span></span>
<span id="cb198-15"><a href="#cb198-15" aria-hidden="true" tabindex="-1"></a>    target_sizes<span class="op">=</span>[random_test_sample[<span class="st">"labels"</span>][<span class="st">"orig_size"</span>]] <span class="co"># original input image size (or whichever target size you'd like), required to be same number of input items in a list</span></span>
<span id="cb198-16"><a href="#cb198-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb198-17"><a href="#cb198-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb198-18"><a href="#cb198-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the random sample test preds</span></span>
<span id="cb198-19"><a href="#cb198-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract scores, labels and boxes</span></span>
<span id="cb198-20"><a href="#cb198-20" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_scores <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"scores"</span>]</span>
<span id="cb198-21"><a href="#cb198-21" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_labels <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"labels"</span>]</span>
<span id="cb198-22"><a href="#cb198-22" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_boxes <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"boxes"</span>]</span>
<span id="cb198-23"><a href="#cb198-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb198-24"><a href="#cb198-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of labels to plot on the boxes </span></span>
<span id="cb198-25"><a href="#cb198-25" aria-hidden="true" tabindex="-1"></a>random_test_sample_labels_to_plot <span class="op">=</span> [<span class="ss">f"Pred: </span><span class="sc">{</span>id2label[label_pred.item()]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score_pred.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">)"</span> </span>
<span id="cb198-26"><a href="#cb198-26" aria-hidden="true" tabindex="-1"></a>                  <span class="cf">for</span> label_pred, score_pred <span class="kw">in</span> <span class="bu">zip</span>(random_test_sample_pred_labels, random_test_sample_pred_scores)]</span>
<span id="cb198-27"><a href="#cb198-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb198-28"><a href="#cb198-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Labels with scores: </span><span class="sc">{</span>random_test_sample_labels_to_plot<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb198-29"><a href="#cb198-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Boxes:"</span>)</span>
<span id="cb198-30"><a href="#cb198-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> item <span class="kw">in</span> random_test_sample_pred_boxes:</span>
<span id="cb198-31"><a href="#cb198-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(item.detach().cpu())</span>
<span id="cb198-32"><a href="#cb198-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Total preds: </span><span class="sc">{</span><span class="bu">len</span>(random_test_sample_labels_to_plot)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb198-33"><a href="#cb198-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb198-34"><a href="#cb198-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted boxes on the random test image </span></span>
<span id="cb198-35"><a href="#cb198-35" aria-hidden="true" tabindex="-1"></a>to_pil_image(</span>
<span id="cb198-36"><a href="#cb198-36" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb198-37"><a href="#cb198-37" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>dataset[<span class="st">"test"</span>][random_test_pred_index][<span class="st">"image"</span>]),</span>
<span id="cb198-38"><a href="#cb198-38" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>random_test_sample_pred_boxes,</span>
<span id="cb198-39"><a href="#cb198-39" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>random_test_sample_labels_to_plot,</span>
<span id="cb198-40"><a href="#cb198-40" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">3</span></span>
<span id="cb198-41"><a href="#cb198-41" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb198-42"><a href="#cb198-42" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Making predictions on test item with index: 163
[INFO] Labels with scores: ['Pred: bin (0.6625)', 'Pred: hand (0.5412)', 'Pred: trash (0.5007)', 'Pred: trash (0.4147)', 'Pred: trash (0.396)', 'Pred: not_trash (0.3237)', 'Pred: hand (0.2799)']
[INFO] Boxes:
tensor([  10.7812,  393.1250,  950.1562, 1160.6250])
tensor([ 149.8828,  667.9688,  471.6797, 1018.2812])
tensor([405.0000, 679.1406, 668.4375, 972.1094])
tensor([248.2031, 472.6562, 675.7031, 994.8438])
tensor([ 140.6250,  467.3438,  675.9375, 1002.6562])
tensor([ 373.2422,  896.4844,  648.6328, 1063.5156])
tensor([  10.3125,  667.9688,  472.0312, 1264.5312])
[INFO] Total preds: 7</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="137">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-109-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="tk---model-v3---cleaning-up-predictions-with-nms-non-max-suppression" class="level2" data-number="16">
<h2 data-number="16" class="anchored" data-anchor-id="tk---model-v3---cleaning-up-predictions-with-nms-non-max-suppression"><span class="header-section-number">16</span> TK - Model V3 - Cleaning up predictions with NMS (Non-max Suppression)</h2>
<p>UPTOHERE * Take preds from model v2 and perform NMS on them to see what happens * Need to calculate: * IoU (intersection over union) * Can write about these in a blog post as extension material * Test image index good to practice on: * 163, 108 * Create a demo which compares NMS-free boxes to boxes <em>with</em> NMS</p>
<section id="tk---nms-filtering-logic-to-do" class="level3" data-number="16.1">
<h3 data-number="16.1" class="anchored" data-anchor-id="tk---nms-filtering-logic-to-do"><span class="header-section-number">16.1</span> TK - NMS filtering logic to do</h3>
<p>TK - create a table of different items here</p>
<ol type="1">
<li>Simplest filtering: keep only 1x class label with the highest score per image (e.g.&nbsp;if there are two â€œhandâ€ predictions, keep only the highest scoring one) âœ…
<ul>
<li>TK - problem with simple filtering is that it might take out a box that wouldâ€™ve been helpful, it also assumes that thereâ€™s little false positives (e.g.&nbsp;each box is predicting the class that it should predict)</li>
</ul></li>
<li>Greedy IoU filtering: Filter boxes which have IoU &gt; 0.9 (big overlap) and keep the box with the higher score âœ…
<ul>
<li>TK - problem here is that it may filter heavily overlapping classes (e.g.&nbsp;if there are many boxes of different classes clustered together because your objects overlap, such as on a plate of food, items may overlap)</li>
</ul></li>
<li>Class-aware IoU filtering: Filter boxes which have the same label and have IoU &gt; 0.5 and keep the box with the higher score</li>
</ol>
<p>Other potential NMS options: * Greedy NMS (good for distinct boxes, just take the highest scoring box per class) * Soft-NMS with linear penalty (good for boxes which may have overlap, e.g.&nbsp;smaller boxes in clusters) * Class-aware NMS (only perform NMS on same class of boxes)</p>
<ul>
<li><p>See this video here: https://youtu.be/VAo84c1hQX8?si=dYftsYADb9Kq-bul</p></li>
<li><p>TK - show prediction with more boxes than ideal, then introduce NMS as a technique to fix the predictions (e.g.&nbsp;on the same sample)</p>
<ul>
<li>TK - NMS doesnâ€™t need an extra model, just a way to</li>
</ul></li>
<li><p>TK - test index 163 is a good example with many boxes that could be shortened to a few</p></li>
</ul>
</section>
<section id="tk---simple-nms---keep-only-highest-scoring-class-per-prediction" class="level3" data-number="16.2">
<h3 data-number="16.2" class="anchored" data-anchor-id="tk---simple-nms---keep-only-highest-scoring-class-per-prediction"><span class="header-section-number">16.2</span> TK - Simple NMS - Keep only highest scoring class per prediction</h3>
<p>TK - This is the simplest method and simply iterates through the boxes and keep the highest scoring box per class (e.g.&nbsp;if there are two â€œhandâ€ prediction boxes, only keep the higher scoring one).</p>
<div id="cell-202" class="cell" data-execution_count="138">
<div class="sourceCode cell-code" id="cb200"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb200-1"><a href="#cb200-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> filter_highest_scoring_box_per_class(boxes, labels, scores):</span>
<span id="cb200-2"><a href="#cb200-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb200-3"><a href="#cb200-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform NMS (Non-max Supression) to only keep the top scoring box per class.</span></span>
<span id="cb200-4"><a href="#cb200-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb200-5"><a href="#cb200-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb200-6"><a href="#cb200-6" aria-hidden="true" tabindex="-1"></a><span class="co">        boxes: tensor of shape (N, 4)</span></span>
<span id="cb200-7"><a href="#cb200-7" aria-hidden="true" tabindex="-1"></a><span class="co">        labels: tensor of shape (N,)</span></span>
<span id="cb200-8"><a href="#cb200-8" aria-hidden="true" tabindex="-1"></a><span class="co">        scores: tensor of shape (N,)</span></span>
<span id="cb200-9"><a href="#cb200-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb200-10"><a href="#cb200-10" aria-hidden="true" tabindex="-1"></a><span class="co">        boxes: tensor of shape (N, 4) filtered for max scoring item per class</span></span>
<span id="cb200-11"><a href="#cb200-11" aria-hidden="true" tabindex="-1"></a><span class="co">        labels: tensor of shape (N,) filtered for max scoring item per class</span></span>
<span id="cb200-12"><a href="#cb200-12" aria-hidden="true" tabindex="-1"></a><span class="co">        scores: tensor of shape (N,) filtered for max scoring item per class</span></span>
<span id="cb200-13"><a href="#cb200-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb200-14"><a href="#cb200-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Start with a blank keep mask (e.g. all False and then update the boxes to keep with True)</span></span>
<span id="cb200-15"><a href="#cb200-15" aria-hidden="true" tabindex="-1"></a>    keep_mask <span class="op">=</span> torch.zeros(<span class="bu">len</span>(boxes), dtype<span class="op">=</span>torch.<span class="bu">bool</span>)</span>
<span id="cb200-16"><a href="#cb200-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb200-17"><a href="#cb200-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For each unique class</span></span>
<span id="cb200-18"><a href="#cb200-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> class_id <span class="kw">in</span> labels.unique():</span>
<span id="cb200-19"><a href="#cb200-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the indicies for the target class</span></span>
<span id="cb200-20"><a href="#cb200-20" aria-hidden="true" tabindex="-1"></a>        class_mask <span class="op">=</span> labels <span class="op">==</span> class_id</span>
<span id="cb200-21"><a href="#cb200-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb200-22"><a href="#cb200-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If any of the labels match the current class_id</span></span>
<span id="cb200-23"><a href="#cb200-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> class_mask.<span class="bu">any</span>():</span>
<span id="cb200-24"><a href="#cb200-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Find the index of highest scoring box for this specific class</span></span>
<span id="cb200-25"><a href="#cb200-25" aria-hidden="true" tabindex="-1"></a>            class_scores <span class="op">=</span> scores[class_mask]</span>
<span id="cb200-26"><a href="#cb200-26" aria-hidden="true" tabindex="-1"></a>            highest_score_idx <span class="op">=</span> class_scores.argmax()</span>
<span id="cb200-27"><a href="#cb200-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb200-28"><a href="#cb200-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convert back to the original index</span></span>
<span id="cb200-29"><a href="#cb200-29" aria-hidden="true" tabindex="-1"></a>            original_idx <span class="op">=</span> torch.where(class_mask)[<span class="dv">0</span>][highest_score_idx]</span>
<span id="cb200-30"><a href="#cb200-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb200-31"><a href="#cb200-31" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update the index in the keep mask to keep the highest scoring box </span></span>
<span id="cb200-32"><a href="#cb200-32" aria-hidden="true" tabindex="-1"></a>            keep_mask[original_idx] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb200-33"><a href="#cb200-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb200-34"><a href="#cb200-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> boxes[keep_mask], labels[keep_mask], scores[keep_mask]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-203" class="cell" data-execution_count="139">
<div class="sourceCode cell-code" id="cb201"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb201-1"><a href="#cb201-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Mask with simple NMS keep mask</span></span>
<span id="cb201-2"><a href="#cb201-2" aria-hidden="true" tabindex="-1"></a>keep_boxes, keep_labels, keep_scores <span class="op">=</span> filter_highest_scoring_box_per_class(boxes<span class="op">=</span>random_test_sample_pred_boxes,</span>
<span id="cb201-3"><a href="#cb201-3" aria-hidden="true" tabindex="-1"></a>                                                                            labels<span class="op">=</span>random_test_sample_pred_labels,</span>
<span id="cb201-4"><a href="#cb201-4" aria-hidden="true" tabindex="-1"></a>                                                                            scores<span class="op">=</span>random_test_sample_pred_scores)</span>
<span id="cb201-5"><a href="#cb201-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb201-6"><a href="#cb201-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(random_test_sample_pred_boxes), <span class="bu">len</span>(random_test_sample_pred_labels), <span class="bu">len</span>(random_test_sample_pred_scores))</span>
<span id="cb201-7"><a href="#cb201-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(keep_scores), <span class="bu">len</span>(keep_labels), <span class="bu">len</span>(keep_boxes))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>7 7 7
4 4 4</code></pre>
</div>
</div>
<div id="cell-204" class="cell" data-execution_count="140">
<div class="sourceCode cell-code" id="cb203"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb203-1"><a href="#cb203-1" aria-hidden="true" tabindex="-1"></a>keep_boxes, keep_labels, keep_scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="140">
<pre><code>(tensor([[  10.7812,  393.1250,  950.1562, 1160.6250],
         [ 149.8828,  667.9688,  471.6797, 1018.2812],
         [ 405.0000,  679.1406,  668.4375,  972.1094],
         [ 373.2422,  896.4844,  648.6328, 1063.5156]], device='cuda:0',
        grad_fn=&lt;IndexBackward0&gt;),
 tensor([0, 1, 5, 4], device='cuda:0'),
 tensor([0.6625, 0.5412, 0.5007, 0.3237], device='cuda:0',
        grad_fn=&lt;IndexBackward0&gt;))</code></pre>
</div>
</div>
<div id="cell-205" class="cell" data-execution_count="141">
<div class="sourceCode cell-code" id="cb205"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb205-1"><a href="#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of labels to plot on the boxes </span></span>
<span id="cb205-2"><a href="#cb205-2" aria-hidden="true" tabindex="-1"></a>random_test_sample_labels_to_plot <span class="op">=</span> [<span class="ss">f"Pred: </span><span class="sc">{</span>id2label[label_pred.item()]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score_pred.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">)"</span> </span>
<span id="cb205-3"><a href="#cb205-3" aria-hidden="true" tabindex="-1"></a>                  <span class="cf">for</span> label_pred, score_pred <span class="kw">in</span> <span class="bu">zip</span>(random_test_sample_pred_labels, random_test_sample_pred_scores)]</span>
<span id="cb205-4"><a href="#cb205-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-5"><a href="#cb205-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Labels with scores: </span><span class="sc">{</span>random_test_sample_labels_to_plot<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb205-6"><a href="#cb205-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-7"><a href="#cb205-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted boxes on the random test image </span></span>
<span id="cb205-8"><a href="#cb205-8" aria-hidden="true" tabindex="-1"></a>test_image_with_preds_original <span class="op">=</span> to_pil_image(</span>
<span id="cb205-9"><a href="#cb205-9" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb205-10"><a href="#cb205-10" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>dataset[<span class="st">"test"</span>][random_test_pred_index][<span class="st">"image"</span>]),</span>
<span id="cb205-11"><a href="#cb205-11" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>random_test_sample_pred_boxes,</span>
<span id="cb205-12"><a href="#cb205-12" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>random_test_sample_labels_to_plot,</span>
<span id="cb205-13"><a href="#cb205-13" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">3</span></span>
<span id="cb205-14"><a href="#cb205-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb205-15"><a href="#cb205-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb205-16"><a href="#cb205-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-17"><a href="#cb205-17" aria-hidden="true" tabindex="-1"></a><span class="co">### Create image with filtered boxes</span></span>
<span id="cb205-18"><a href="#cb205-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-19"><a href="#cb205-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of labels to plot on the boxes </span></span>
<span id="cb205-20"><a href="#cb205-20" aria-hidden="true" tabindex="-1"></a>random_test_sample_labels_to_plot_filtered <span class="op">=</span> [<span class="ss">f"Pred: </span><span class="sc">{</span>id2label[label_pred.item()]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score_pred.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">)"</span> </span>
<span id="cb205-21"><a href="#cb205-21" aria-hidden="true" tabindex="-1"></a>                  <span class="cf">for</span> label_pred, score_pred <span class="kw">in</span> <span class="bu">zip</span>(keep_labels, keep_scores)]</span>
<span id="cb205-22"><a href="#cb205-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-23"><a href="#cb205-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Labels with scores: </span><span class="sc">{</span>random_test_sample_labels_to_plot_filtered<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb205-24"><a href="#cb205-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-25"><a href="#cb205-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted boxes on the random test image </span></span>
<span id="cb205-26"><a href="#cb205-26" aria-hidden="true" tabindex="-1"></a>test_image_with_preds_filtered <span class="op">=</span> to_pil_image(</span>
<span id="cb205-27"><a href="#cb205-27" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb205-28"><a href="#cb205-28" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>dataset[<span class="st">"test"</span>][random_test_pred_index][<span class="st">"image"</span>]),</span>
<span id="cb205-29"><a href="#cb205-29" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>keep_boxes,</span>
<span id="cb205-30"><a href="#cb205-30" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>random_test_sample_labels_to_plot_filtered,</span>
<span id="cb205-31"><a href="#cb205-31" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">3</span></span>
<span id="cb205-32"><a href="#cb205-32" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb205-33"><a href="#cb205-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb205-34"><a href="#cb205-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-35"><a href="#cb205-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the transformed image </span></span>
<span id="cb205-36"><a href="#cb205-36" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb205-37"><a href="#cb205-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-38"><a href="#cb205-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a figure with two subplots</span></span>
<span id="cb205-39"><a href="#cb205-39" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">10</span>))</span>
<span id="cb205-40"><a href="#cb205-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-41"><a href="#cb205-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image 1</span></span>
<span id="cb205-42"><a href="#cb205-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].imshow(test_image_with_preds_original)</span>
<span id="cb205-43"><a href="#cb205-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axis(<span class="st">"off"</span>)  <span class="co"># Hide axes</span></span>
<span id="cb205-44"><a href="#cb205-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="ss">f"Original Image Preds (total: </span><span class="sc">{</span><span class="bu">len</span>(random_test_sample_pred_boxes)<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb205-45"><a href="#cb205-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-46"><a href="#cb205-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image 2</span></span>
<span id="cb205-47"><a href="#cb205-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].imshow(test_image_with_preds_filtered)</span>
<span id="cb205-48"><a href="#cb205-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axis(<span class="st">"off"</span>)  <span class="co"># Hide axes</span></span>
<span id="cb205-49"><a href="#cb205-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="ss">f"Filtered Image Preds (total: </span><span class="sc">{</span><span class="bu">len</span>(keep_boxes)<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb205-50"><a href="#cb205-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-51"><a href="#cb205-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb205-52"><a href="#cb205-52" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">"Simple NMS - Only keep the highest scoring box per prediction"</span>)</span>
<span id="cb205-53"><a href="#cb205-53" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb205-54"><a href="#cb205-54" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Labels with scores: ['Pred: bin (0.6625)', 'Pred: hand (0.5412)', 'Pred: trash (0.5007)', 'Pred: trash (0.4147)', 'Pred: trash (0.396)', 'Pred: not_trash (0.3237)', 'Pred: hand (0.2799)']
[INFO] Labels with scores: ['Pred: bin (0.6625)', 'Pred: hand (0.5412)', 'Pred: trash (0.5007)', 'Pred: not_trash (0.3237)']</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-113-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>TK - problem with simple filtering is that it might take out a box that wouldâ€™ve been helpful, it also assumes that thereâ€™s little false positives (e.g.&nbsp;each box is predicting the class that it should predict)</p>
</section>
<section id="tk---greedy-iou-filtering---intersection-over-union---if-a-pair-of-boxes-have-an-iou-over-a-certain-threshold-keep-the-box-with-the-higher-score" class="level3" data-number="16.3">
<h3 data-number="16.3" class="anchored" data-anchor-id="tk---greedy-iou-filtering---intersection-over-union---if-a-pair-of-boxes-have-an-iou-over-a-certain-threshold-keep-the-box-with-the-higher-score"><span class="header-section-number">16.3</span> TK - Greedy IoU Filtering - Intersection over Union - If a pair of boxes have an IoU over a certain threshold, keep the box with the higher score</h3>
<ul>
<li>IoU in torchmetrics - https://lightning.ai/docs/torchmetrics/stable/detection/intersection_over_union.html</li>
</ul>
<p>To calculate the <strong>Intersection over Union (IoU)</strong> between two bounding boxes:</p>
<ol type="1">
<li><strong>Coordinates of the intersection rectangle</strong>: <span class="math display">\[
x_{\text{left}} = \max(x_{1A}, x_{1B})
\]</span> <span class="math display">\[
y_{\text{top}} = \max(y_{1A}, y_{1B})
\]</span> <span class="math display">\[
x_{\text{right}} = \min(x_{2A}, x_{2B})
\]</span> <span class="math display">\[
y_{\text{bottom}} = \min(y_{2A}, y_{2B})
\]</span></li>
</ol>
<p>Where:</p>
<p><span class="math display">\[
   \text{A} = \text{Box 1}
\]</span> <span class="math display">\[
   \text{B} = \text{Box 2}
\]</span></p>
<ol start="2" type="1">
<li><p><strong>Width and height of the intersection</strong>: <span class="math display">\[
\text{intersection\_width} = \max(0, x_{\text{right}} - x_{\text{left}})
\]</span> <span class="math display">\[
\text{intersection\_height} = \max(0, y_{\text{bottom}} - y_{\text{top}})
\]</span></p></li>
<li><p><strong>Area of Overlap</strong>: <span class="math display">\[
\text{Area of Overlap} = \text{intersection\_width} \times \text{intersection\_height}
\]</span></p></li>
<li><p><strong>Area of Union</strong>: <span class="math display">\[
\text{Area of Union} = \text{Area of Box 1} + \text{Area of Box 2} - \text{Area of Overlap}
\]</span></p></li>
<li><p><strong>Intersection over Union (IoU)</strong>: $$ = / </p></li>
</ol>
<div id="cell-208" class="cell" data-execution_count="142">
<div class="sourceCode cell-code" id="cb207"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb207-1"><a href="#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="co"># IoU = Intersection / Union</span></span>
<span id="cb207-2"><a href="#cb207-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Inserction =</span></span>
<span id="cb207-3"><a href="#cb207-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># x_left = max(x1_A, x1_B)</span></span>
<span id="cb207-4"><a href="#cb207-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y_top = max(y1_A, y1_B)</span></span>
<span id="cb207-5"><a href="#cb207-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># x_right = min(x2_A, x2_B)</span></span>
<span id="cb207-6"><a href="#cb207-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y_bottom = min(y2_A, x2_B)</span></span>
<span id="cb207-7"><a href="#cb207-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span></span>
<span id="cb207-8"><a href="#cb207-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Where: </span></span>
<span id="cb207-9"><a href="#cb207-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A = Box 1</span></span>
<span id="cb207-10"><a href="#cb207-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B = Box 2</span></span>
<span id="cb207-11"><a href="#cb207-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># intersection_width = max(0, x_right - x_left)</span></span>
<span id="cb207-12"><a href="#cb207-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># interesection_height = max(0, y_bottom - y_top)</span></span>
<span id="cb207-13"><a href="#cb207-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># area_intersection = intersection_width * intersection_height</span></span>
<span id="cb207-14"><a href="#cb207-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Union = area_box_1 + area_box_2 - intersection</span></span>
<span id="cb207-15"><a href="#cb207-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-16"><a href="#cb207-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> intersection_over_union_score(box_1, box_2):</span>
<span id="cb207-17"><a href="#cb207-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculates Intersection over Union (IoU) score for two given boxes in XYXY format."""</span></span>
<span id="cb207-18"><a href="#cb207-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(box_1) <span class="op">==</span> <span class="dv">4</span>, <span class="ss">f"Box 1 should have four elements in the format [x_1, y_1, x_2, y_2] but has: </span><span class="sc">{</span><span class="bu">len</span>(box_1)<span class="sc">}</span><span class="ss">, see: </span><span class="sc">{</span>box_1<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb207-19"><a href="#cb207-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(box_2) <span class="op">==</span> <span class="dv">4</span>, <span class="ss">f"Box 2 should have four elements in the format [x_1, y_1, x_2, y_2] but has: </span><span class="sc">{</span><span class="bu">len</span>(box_2)<span class="sc">}</span><span class="ss">, see: </span><span class="sc">{</span>box_2<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb207-20"><a href="#cb207-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-21"><a href="#cb207-21" aria-hidden="true" tabindex="-1"></a>    x1_box_1, y1_box_1, x2_box_1, y2_box_1 <span class="op">=</span> box_1[<span class="dv">0</span>], box_1[<span class="dv">1</span>], box_1[<span class="dv">2</span>], box_1[<span class="dv">3</span>]</span>
<span id="cb207-22"><a href="#cb207-22" aria-hidden="true" tabindex="-1"></a>    x1_box_2, y1_box_2, x2_box_2, y2_box_2 <span class="op">=</span> box_2[<span class="dv">0</span>], box_2[<span class="dv">1</span>], box_2[<span class="dv">2</span>], box_2[<span class="dv">3</span>]</span>
<span id="cb207-23"><a href="#cb207-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-24"><a href="#cb207-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get coordinates of overlapping box (note: there may not be any overlapping box)</span></span>
<span id="cb207-25"><a href="#cb207-25" aria-hidden="true" tabindex="-1"></a>    x_left <span class="op">=</span> torch.<span class="bu">max</span>(x1_box_1, x1_box_2)</span>
<span id="cb207-26"><a href="#cb207-26" aria-hidden="true" tabindex="-1"></a>    y_top <span class="op">=</span> torch.<span class="bu">max</span>(y1_box_1, y1_box_2)</span>
<span id="cb207-27"><a href="#cb207-27" aria-hidden="true" tabindex="-1"></a>    x_right <span class="op">=</span> torch.<span class="bu">min</span>(x2_box_1, x2_box_2)</span>
<span id="cb207-28"><a href="#cb207-28" aria-hidden="true" tabindex="-1"></a>    y_bottom <span class="op">=</span> torch.<span class="bu">min</span>(y2_box_1, y2_box_2)</span>
<span id="cb207-29"><a href="#cb207-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-30"><a href="#cb207-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the intersection width and height (we take the max of 0 and the value to find non-overlapping boxes)</span></span>
<span id="cb207-31"><a href="#cb207-31" aria-hidden="true" tabindex="-1"></a>    intersection_width <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, x_right <span class="op">-</span> x_left)</span>
<span id="cb207-32"><a href="#cb207-32" aria-hidden="true" tabindex="-1"></a>    intersection_height <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, y_bottom <span class="op">-</span> y_top)</span>
<span id="cb207-33"><a href="#cb207-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-34"><a href="#cb207-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the area of intersection (note: this will 0 if either width or height are 0)</span></span>
<span id="cb207-35"><a href="#cb207-35" aria-hidden="true" tabindex="-1"></a>    area_of_intersection <span class="op">=</span> intersection_height <span class="op">*</span> intersection_width</span>
<span id="cb207-36"><a href="#cb207-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-37"><a href="#cb207-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate individual box areas</span></span>
<span id="cb207-38"><a href="#cb207-38" aria-hidden="true" tabindex="-1"></a>    box_1_area <span class="op">=</span> (x2_box_1 <span class="op">-</span> x1_box_1) <span class="op">*</span> (y2_box_1 <span class="op">-</span> y1_box_1) <span class="co"># width * height </span></span>
<span id="cb207-39"><a href="#cb207-39" aria-hidden="true" tabindex="-1"></a>    box_2_area <span class="op">=</span> (x2_box_2 <span class="op">-</span> x1_box_2) <span class="op">*</span> (y2_box_2 <span class="op">-</span> y1_box_2)</span>
<span id="cb207-40"><a href="#cb207-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-41"><a href="#cb207-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calcuate area of union (sum of box areas minus the intersection area)</span></span>
<span id="cb207-42"><a href="#cb207-42" aria-hidden="true" tabindex="-1"></a>    area_of_union <span class="op">=</span> box_1_area <span class="op">+</span> box_2_area <span class="op">-</span> area_of_intersection</span>
<span id="cb207-43"><a href="#cb207-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-44"><a href="#cb207-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the IoU score</span></span>
<span id="cb207-45"><a href="#cb207-45" aria-hidden="true" tabindex="-1"></a>    iou_score <span class="op">=</span> area_of_intersection <span class="op">/</span> area_of_union</span>
<span id="cb207-46"><a href="#cb207-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-47"><a href="#cb207-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> iou_score</span>
<span id="cb207-48"><a href="#cb207-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-49"><a href="#cb207-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-50"><a href="#cb207-50" aria-hidden="true" tabindex="-1"></a>iou_score_test_pred_boxes <span class="op">=</span> intersection_over_union_score(box_1<span class="op">=</span>random_test_sample_pred_boxes[<span class="dv">4</span>],</span>
<span id="cb207-51"><a href="#cb207-51" aria-hidden="true" tabindex="-1"></a>                                                          box_2<span class="op">=</span>random_test_sample_pred_boxes[<span class="dv">3</span>])</span>
<span id="cb207-52"><a href="#cb207-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-53"><a href="#cb207-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] IoU Score: </span><span class="sc">{</span>iou_score_test_pred_boxes<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb207-54"><a href="#cb207-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-55"><a href="#cb207-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-56"><a href="#cb207-56" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_boxes[<span class="dv">0</span>], random_test_sample_pred_boxes[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] IoU Score: 0.7790185809135437</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="142">
<pre><code>(tensor([  10.7812,  393.1250,  950.1562, 1160.6250], device='cuda:0',
        grad_fn=&lt;SelectBackward0&gt;),
 tensor([ 149.8828,  667.9688,  471.6797, 1018.2812], device='cuda:0',
        grad_fn=&lt;SelectBackward0&gt;))</code></pre>
</div>
</div>
<div id="cell-209" class="cell" data-execution_count="143">
<div class="sourceCode cell-code" id="cb210"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb210-1"><a href="#cb210-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TK - for visualization purposes, write code to highlight the intersecting points on a box and print the IoU score in the middle of the box</span></span>
<span id="cb210-2"><a href="#cb210-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb210-3"><a href="#cb210-3" aria-hidden="true" tabindex="-1"></a><span class="co"># IoU logic</span></span>
<span id="cb210-4"><a href="#cb210-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. General IoU threshold (removing boxes at a global level, regardless of label)</span></span>
<span id="cb210-5"><a href="#cb210-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -&gt; for box pairs with IoU &gt; 0.9, keep the higher scoring box </span></span>
<span id="cb210-6"><a href="#cb210-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Label specific IoU threshold (only concern is comparing boxes with the same label)</span></span>
<span id="cb210-7"><a href="#cb210-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -&gt; for box pairs with same label and IoU &gt; 0.5, keep the higher scoring box</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-210" class="cell" data-execution_count="144">
<div class="sourceCode cell-code" id="cb211"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb211-1"><a href="#cb211-1" aria-hidden="true" tabindex="-1"></a>keep_boxes <span class="op">=</span> []</span>
<span id="cb211-2"><a href="#cb211-2" aria-hidden="true" tabindex="-1"></a>keep_scores <span class="op">=</span> []</span>
<span id="cb211-3"><a href="#cb211-3" aria-hidden="true" tabindex="-1"></a>keep_labels <span class="op">=</span> []</span>
<span id="cb211-4"><a href="#cb211-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-5"><a href="#cb211-5" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_scores <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"scores"</span>]</span>
<span id="cb211-6"><a href="#cb211-6" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_labels <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"labels"</span>]</span>
<span id="cb211-7"><a href="#cb211-7" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_boxes <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"boxes"</span>]</span>
<span id="cb211-8"><a href="#cb211-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-9"><a href="#cb211-9" aria-hidden="true" tabindex="-1"></a>keep_indexes <span class="op">=</span> torch.ones(<span class="bu">len</span>(random_test_sample_pred_boxes), dtype<span class="op">=</span>torch.<span class="bu">bool</span>)</span>
<span id="cb211-10"><a href="#cb211-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-11"><a href="#cb211-11" aria-hidden="true" tabindex="-1"></a>iou_general_threshold <span class="op">=</span> <span class="fl">0.9</span> <span class="co"># general threshold = remove the lower scoring box in box pairs with over iou_general_threshold regardless of the label</span></span>
<span id="cb211-12"><a href="#cb211-12" aria-hidden="true" tabindex="-1"></a>iou_class_level_threshold <span class="op">=</span> <span class="fl">0.5</span> <span class="co"># remove overlapping similar classes</span></span>
<span id="cb211-13"><a href="#cb211-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-14"><a href="#cb211-14" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co">: Add a clause here to include if class labels are the same, then filter based on the class-specifc IoU threshold</span></span>
<span id="cb211-15"><a href="#cb211-15" aria-hidden="true" tabindex="-1"></a>filter_global <span class="op">=</span> <span class="va">True</span></span>
<span id="cb211-16"><a href="#cb211-16" aria-hidden="true" tabindex="-1"></a>filter_same_label <span class="op">=</span> <span class="va">True</span></span>
<span id="cb211-17"><a href="#cb211-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-18"><a href="#cb211-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Count the total loops</span></span>
<span id="cb211-19"><a href="#cb211-19" aria-hidden="true" tabindex="-1"></a>total_loops <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb211-20"><a href="#cb211-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-21"><a href="#cb211-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, box_A <span class="kw">in</span> <span class="bu">enumerate</span>(random_test_sample_pred_boxes):</span>
<span id="cb211-22"><a href="#cb211-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> keep_indexes[i]: <span class="co"># insert clause to prevent calculating on already filtered labels</span></span>
<span id="cb211-23"><a href="#cb211-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span> </span>
<span id="cb211-24"><a href="#cb211-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-25"><a href="#cb211-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j, box_B <span class="kw">in</span> <span class="bu">enumerate</span>(random_test_sample_pred_boxes):</span>
<span id="cb211-26"><a href="#cb211-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> keep_indexes[i]:</span>
<span id="cb211-27"><a href="#cb211-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb211-28"><a href="#cb211-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-29"><a href="#cb211-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Only calculate IoU score if indexes aren't the same (saves comparing the same index boxes for unwanted calculations)</span></span>
<span id="cb211-30"><a href="#cb211-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i <span class="op">!=</span> j): </span>
<span id="cb211-31"><a href="#cb211-31" aria-hidden="true" tabindex="-1"></a>            iou_score <span class="op">=</span> intersection_over_union_score(box_1<span class="op">=</span>box_A, box_2<span class="op">=</span>box_B)</span>
<span id="cb211-32"><a href="#cb211-32" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"[INFO] IoU Score for box </span><span class="sc">{</span>(i, j)<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>iou_score<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb211-33"><a href="#cb211-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-34"><a href="#cb211-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> filter_global:</span>
<span id="cb211-35"><a href="#cb211-35" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> iou_score <span class="op">&gt;</span> iou_general_threshold:</span>
<span id="cb211-36"><a href="#cb211-36" aria-hidden="true" tabindex="-1"></a>                    score_A, score_B <span class="op">=</span> random_test_sample_pred_scores[i], random_test_sample_pred_scores[j]</span>
<span id="cb211-37"><a href="#cb211-37" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> score_A <span class="op">&gt;</span> score_B:</span>
<span id="cb211-38"><a href="#cb211-38" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"[INFO] Box to keep index: </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span>box_A<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb211-39"><a href="#cb211-39" aria-hidden="true" tabindex="-1"></a>                        keep_indexes[j] <span class="op">=</span> <span class="va">False</span></span>
<span id="cb211-40"><a href="#cb211-40" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb211-41"><a href="#cb211-41" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"[INFO] Box to keep index: </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span>box_B<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb211-42"><a href="#cb211-42" aria-hidden="true" tabindex="-1"></a>                        keep_indexes[i] <span class="op">=</span> <span class="va">False</span></span>
<span id="cb211-43"><a href="#cb211-43" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb211-44"><a href="#cb211-44" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> filter_same_label:</span>
<span id="cb211-45"><a href="#cb211-45" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> iou_score <span class="op">&gt;</span> iou_class_level_threshold:</span>
<span id="cb211-46"><a href="#cb211-46" aria-hidden="true" tabindex="-1"></a>                    i_label <span class="op">=</span> random_test_sample_pred_labels[i]</span>
<span id="cb211-47"><a href="#cb211-47" aria-hidden="true" tabindex="-1"></a>                    j_label <span class="op">=</span> random_test_sample_pred_labels[j]</span>
<span id="cb211-48"><a href="#cb211-48" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> i_label <span class="op">==</span> j_label:</span>
<span id="cb211-49"><a href="#cb211-49" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">print</span>(<span class="ss">f"Labels are equal: </span><span class="sc">{</span>i_label<span class="sc">,</span> j_label<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb211-50"><a href="#cb211-50" aria-hidden="true" tabindex="-1"></a>                        score_A, score_B <span class="op">=</span> random_test_sample_pred_scores[i], random_test_sample_pred_scores[j]</span>
<span id="cb211-51"><a href="#cb211-51" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> score_A <span class="op">&gt;</span> score_B:</span>
<span id="cb211-52"><a href="#cb211-52" aria-hidden="true" tabindex="-1"></a>                            <span class="bu">print</span>(<span class="ss">f"[INFO] Box to keep index: </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span>box_A<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb211-53"><a href="#cb211-53" aria-hidden="true" tabindex="-1"></a>                            keep_indexes[j] <span class="op">=</span> <span class="va">False</span></span>
<span id="cb211-54"><a href="#cb211-54" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">else</span>:</span>
<span id="cb211-55"><a href="#cb211-55" aria-hidden="true" tabindex="-1"></a>                            <span class="bu">print</span>(<span class="ss">f"[INFO] Box to keep index: </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span>box_B<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb211-56"><a href="#cb211-56" aria-hidden="true" tabindex="-1"></a>                            keep_indexes[i] <span class="op">=</span> <span class="va">False</span></span>
<span id="cb211-57"><a href="#cb211-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-58"><a href="#cb211-58" aria-hidden="true" tabindex="-1"></a>        total_loops <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb211-59"><a href="#cb211-59" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb211-60"><a href="#cb211-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(keep_indexes)</span>
<span id="cb211-61"><a href="#cb211-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-62"><a href="#cb211-62" aria-hidden="true" tabindex="-1"></a>keep_scores <span class="op">=</span> random_test_sample_pred_scores[keep_indexes]</span>
<span id="cb211-63"><a href="#cb211-63" aria-hidden="true" tabindex="-1"></a>keep_labels <span class="op">=</span> random_test_sample_pred_labels[keep_indexes]</span>
<span id="cb211-64"><a href="#cb211-64" aria-hidden="true" tabindex="-1"></a>keep_boxes <span class="op">=</span> random_test_sample_pred_boxes[keep_indexes]</span>
<span id="cb211-65"><a href="#cb211-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-66"><a href="#cb211-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(random_test_sample_pred_boxes), <span class="bu">len</span>(random_test_sample_pred_labels), <span class="bu">len</span>(random_test_sample_pred_boxes))</span>
<span id="cb211-67"><a href="#cb211-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(keep_scores), <span class="bu">len</span>(keep_labels), <span class="bu">len</span>(keep_boxes), <span class="bu">sum</span>(keep_indexes))</span>
<span id="cb211-68"><a href="#cb211-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-69"><a href="#cb211-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Number of total loops: </span><span class="sc">{</span>total_loops<span class="sc">}</span><span class="ss">, max possible loops: </span><span class="sc">{</span><span class="bu">len</span>(random_test_sample_pred_boxes)<span class="op">**</span><span class="dv">2</span><span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] IoU Score for box (0, 1): 0.156358003616333
[INFO] IoU Score for box (0, 2): 0.10704872757196426
[INFO] IoU Score for box (0, 3): 0.3096315264701843
[INFO] IoU Score for box (0, 4): 0.3974636495113373
[INFO] IoU Score for box (0, 5): 0.06380129605531693
[INFO] IoU Score for box (0, 6): 0.2954297661781311
[INFO] IoU Score for box (1, 0): 0.156358003616333
[INFO] IoU Score for box (1, 2): 0.11466032266616821
[INFO] IoU Score for box (1, 3): 0.2778415083885193
[INFO] IoU Score for box (1, 4): 0.36936208605766296
[INFO] IoU Score for box (1, 5): 0.08170551061630249
[INFO] IoU Score for box (1, 6): 0.4092644155025482
[INFO] IoU Score for box (2, 0): 0.10704872757196426
[INFO] IoU Score for box (2, 1): 0.11466032266616821
[INFO] IoU Score for box (2, 3): 0.34572935104370117
[INFO] IoU Score for box (2, 4): 0.26932957768440247
[INFO] IoU Score for box (2, 5): 0.17588727176189423
[INFO] IoU Score for box (2, 6): 0.058975815773010254
[INFO] IoU Score for box (3, 0): 0.3096315264701843
[INFO] IoU Score for box (3, 1): 0.2778415083885193
[INFO] IoU Score for box (3, 2): 0.34572935104370117
[INFO] IoU Score for box (3, 4): 0.7790185809135437
Labels are equal: (tensor(5, device='cuda:0'), tensor(5, device='cuda:0'))
[INFO] Box to keep index: 3 -&gt; tensor([248.2031, 472.6562, 675.7031, 994.8438], device='cuda:0',
       grad_fn=&lt;UnbindBackward0&gt;)
[INFO] IoU Score for box (3, 5): 0.11186295002698898
[INFO] IoU Score for box (3, 6): 0.1719416379928589
[INFO] IoU Score for box (5, 0): 0.06380129605531693
[INFO] IoU Score for box (5, 1): 0.08170551061630249
[INFO] IoU Score for box (5, 2): 0.17588727176189423
[INFO] IoU Score for box (5, 3): 0.11186295002698898
[INFO] IoU Score for box (5, 4): 0.0963958203792572
[INFO] IoU Score for box (5, 6): 0.05411146208643913
[INFO] IoU Score for box (6, 0): 0.2954297661781311
[INFO] IoU Score for box (6, 1): 0.4092644155025482
[INFO] IoU Score for box (6, 2): 0.058975815773010254
[INFO] IoU Score for box (6, 3): 0.1719416379928589
[INFO] IoU Score for box (6, 4): 0.24588997662067413
[INFO] IoU Score for box (6, 5): 0.05411146208643913
tensor([ True,  True,  True,  True, False,  True,  True])
7 7 7
6 6 6 tensor(6)
[INFO] Number of total loops: 42, max possible loops: 49</code></pre>
</div>
</div>
<div id="cell-211" class="cell" data-execution_count="145">
<div class="sourceCode cell-code" id="cb213"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb213-1"><a href="#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tensor([ True,  True,  True,  True,  True, False,  True, False])</span></span>
<span id="cb213-2"><a href="#cb213-2" aria-hidden="true" tabindex="-1"></a><span class="co"># tensor([ True,  True,  True,  True,  True, False,  True, False])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-212" class="cell" data-execution_count="146">
<div class="sourceCode cell-code" id="cb214"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb214-1"><a href="#cb214-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of labels to plot on the boxes </span></span>
<span id="cb214-2"><a href="#cb214-2" aria-hidden="true" tabindex="-1"></a>random_test_sample_labels_to_plot <span class="op">=</span> [<span class="ss">f"Pred: </span><span class="sc">{</span>id2label[label_pred.item()]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score_pred.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">)"</span> </span>
<span id="cb214-3"><a href="#cb214-3" aria-hidden="true" tabindex="-1"></a>                  <span class="cf">for</span> label_pred, score_pred <span class="kw">in</span> <span class="bu">zip</span>(random_test_sample_pred_labels, random_test_sample_pred_scores)]</span>
<span id="cb214-4"><a href="#cb214-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-5"><a href="#cb214-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Labels with scores: </span><span class="sc">{</span>random_test_sample_labels_to_plot<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb214-6"><a href="#cb214-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-7"><a href="#cb214-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted boxes on the random test image </span></span>
<span id="cb214-8"><a href="#cb214-8" aria-hidden="true" tabindex="-1"></a>test_image_with_preds_original <span class="op">=</span> to_pil_image(</span>
<span id="cb214-9"><a href="#cb214-9" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb214-10"><a href="#cb214-10" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>dataset[<span class="st">"test"</span>][random_test_pred_index][<span class="st">"image"</span>]),</span>
<span id="cb214-11"><a href="#cb214-11" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>random_test_sample_pred_boxes,</span>
<span id="cb214-12"><a href="#cb214-12" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>random_test_sample_labels_to_plot,</span>
<span id="cb214-13"><a href="#cb214-13" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">3</span></span>
<span id="cb214-14"><a href="#cb214-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb214-15"><a href="#cb214-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb214-16"><a href="#cb214-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-17"><a href="#cb214-17" aria-hidden="true" tabindex="-1"></a><span class="co">### Create image with filtered boxes</span></span>
<span id="cb214-18"><a href="#cb214-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-19"><a href="#cb214-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of labels to plot on the boxes </span></span>
<span id="cb214-20"><a href="#cb214-20" aria-hidden="true" tabindex="-1"></a>random_test_sample_labels_to_plot_filtered <span class="op">=</span> [<span class="ss">f"Pred: </span><span class="sc">{</span>id2label[label_pred.item()]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score_pred.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">)"</span> </span>
<span id="cb214-21"><a href="#cb214-21" aria-hidden="true" tabindex="-1"></a>                  <span class="cf">for</span> label_pred, score_pred <span class="kw">in</span> <span class="bu">zip</span>(keep_labels, keep_scores)]</span>
<span id="cb214-22"><a href="#cb214-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-23"><a href="#cb214-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Labels with scores: </span><span class="sc">{</span>random_test_sample_labels_to_plot_filtered<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb214-24"><a href="#cb214-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-25"><a href="#cb214-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted boxes on the random test image </span></span>
<span id="cb214-26"><a href="#cb214-26" aria-hidden="true" tabindex="-1"></a>test_image_with_preds_filtered <span class="op">=</span> to_pil_image(</span>
<span id="cb214-27"><a href="#cb214-27" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb214-28"><a href="#cb214-28" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>dataset[<span class="st">"test"</span>][random_test_pred_index][<span class="st">"image"</span>]),</span>
<span id="cb214-29"><a href="#cb214-29" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>keep_boxes,</span>
<span id="cb214-30"><a href="#cb214-30" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>random_test_sample_labels_to_plot_filtered,</span>
<span id="cb214-31"><a href="#cb214-31" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">3</span></span>
<span id="cb214-32"><a href="#cb214-32" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb214-33"><a href="#cb214-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb214-34"><a href="#cb214-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-35"><a href="#cb214-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the transformed image </span></span>
<span id="cb214-36"><a href="#cb214-36" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb214-37"><a href="#cb214-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-38"><a href="#cb214-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a figure with two subplots</span></span>
<span id="cb214-39"><a href="#cb214-39" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">10</span>))</span>
<span id="cb214-40"><a href="#cb214-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-41"><a href="#cb214-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image 1</span></span>
<span id="cb214-42"><a href="#cb214-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].imshow(test_image_with_preds_original)</span>
<span id="cb214-43"><a href="#cb214-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axis(<span class="st">"off"</span>)  <span class="co"># Hide axes</span></span>
<span id="cb214-44"><a href="#cb214-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="ss">f"Original Image Preds (total: </span><span class="sc">{</span><span class="bu">len</span>(random_test_sample_pred_boxes)<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb214-45"><a href="#cb214-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-46"><a href="#cb214-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image 2</span></span>
<span id="cb214-47"><a href="#cb214-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].imshow(test_image_with_preds_filtered)</span>
<span id="cb214-48"><a href="#cb214-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axis(<span class="st">"off"</span>)  <span class="co"># Hide axes</span></span>
<span id="cb214-49"><a href="#cb214-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="ss">f"Filtered Image Preds (total: </span><span class="sc">{</span><span class="bu">len</span>(keep_boxes)<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb214-50"><a href="#cb214-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-51"><a href="#cb214-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb214-52"><a href="#cb214-52" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="ss">f"Greedy IoU Filtering (General) - For boxes with IoU &gt; </span><span class="sc">{</span>iou_general_threshold<span class="sc">}</span><span class="ss">, keep the higher scoring box"</span>)</span>
<span id="cb214-53"><a href="#cb214-53" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb214-54"><a href="#cb214-54" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Labels with scores: ['Pred: bin (0.6625)', 'Pred: hand (0.5412)', 'Pred: trash (0.5007)', 'Pred: trash (0.4147)', 'Pred: trash (0.396)', 'Pred: not_trash (0.3237)', 'Pred: hand (0.2799)']
[INFO] Labels with scores: ['Pred: bin (0.6625)', 'Pred: hand (0.5412)', 'Pred: trash (0.5007)', 'Pred: trash (0.4147)', 'Pred: not_trash (0.3237)', 'Pred: hand (0.2799)']</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-118-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-213" class="cell">
<div class="sourceCode cell-code" id="cb216"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb216-1"><a href="#cb216-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TK - more NMS logic:</span></span>
<span id="cb216-2"><a href="#cb216-2" aria-hidden="true" tabindex="-1"></a><span class="co"># If there are more than two hands, keep the one with the higher score...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="tk---create-a-demo-with-simple-nms-filtering-only-keep-the-highest-scoring-boxes-per-image" class="level2" data-number="17">
<h2 data-number="17" class="anchored" data-anchor-id="tk---create-a-demo-with-simple-nms-filtering-only-keep-the-highest-scoring-boxes-per-image"><span class="header-section-number">17</span> TK - Create a Demo with Simple NMS Filtering (only keep the highest scoring boxes per image)</h2>
<p>UPTOHERE:</p>
<ul>
<li>upload the demo to Hugging Face Spaces as Trashify V3</li>
<li>Make sure the demo works</li>
<li>Go back through the code and start tidying up/explaining things
<ul>
<li>Create a blog post to discuss different box formats in object detection</li>
<li>Create a blog post for NMS + IoU filtering (can create an IoU function that colours in the intersection parts)</li>
<li>Create an extension for longer training + synthetic data + evaluation metrics + deploying on transformers.js</li>
</ul></li>
</ul>
<div id="cell-215" class="cell" data-execution_count="147">
<div class="sourceCode cell-code" id="cb217"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb217-1"><a href="#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make directory for demo</span></span>
<span id="cb217-2"><a href="#cb217-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb217-3"><a href="#cb217-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb217-4"><a href="#cb217-4" aria-hidden="true" tabindex="-1"></a>trashify_data_aug_model_dir <span class="op">=</span> Path(<span class="st">"demos/trashify_object_detector_data_aug_model_with_nms/"</span>)</span>
<span id="cb217-5"><a href="#cb217-5" aria-hidden="true" tabindex="-1"></a>trashify_data_aug_model_dir.mkdir(exist_ok<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-216" class="cell" data-execution_count="148">
<div class="sourceCode cell-code" id="cb218"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb218-1"><a href="#cb218-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile demos<span class="op">/</span>trashify_object_detector_data_aug_model_with_nms<span class="op">/</span>requirements.txt</span>
<span id="cb218-2"><a href="#cb218-2" aria-hidden="true" tabindex="-1"></a>timm</span>
<span id="cb218-3"><a href="#cb218-3" aria-hidden="true" tabindex="-1"></a>gradio</span>
<span id="cb218-4"><a href="#cb218-4" aria-hidden="true" tabindex="-1"></a>torch</span>
<span id="cb218-5"><a href="#cb218-5" aria-hidden="true" tabindex="-1"></a>transformers</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting demos/trashify_object_detector_data_aug_model_with_nms/requirements.txt</code></pre>
</div>
</div>
<div id="cell-217" class="cell" data-execution_count="149">
<div class="sourceCode cell-code" id="cb220"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb220-1"><a href="#cb220-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile demos<span class="op">/</span>trashify_object_detector_data_aug_model_with_nms<span class="op">/</span>README.md</span>
<span id="cb220-2"><a href="#cb220-2" aria-hidden="true" tabindex="-1"></a><span class="op">---</span></span>
<span id="cb220-3"><a href="#cb220-3" aria-hidden="true" tabindex="-1"></a>title: Trashify Demo V3 ðŸš®</span>
<span id="cb220-4"><a href="#cb220-4" aria-hidden="true" tabindex="-1"></a>emoji: ðŸ—‘ï¸</span>
<span id="cb220-5"><a href="#cb220-5" aria-hidden="true" tabindex="-1"></a>colorFrom: purple</span>
<span id="cb220-6"><a href="#cb220-6" aria-hidden="true" tabindex="-1"></a>colorTo: blue</span>
<span id="cb220-7"><a href="#cb220-7" aria-hidden="true" tabindex="-1"></a>sdk: gradio</span>
<span id="cb220-8"><a href="#cb220-8" aria-hidden="true" tabindex="-1"></a>sdk_version: <span class="fl">4.40.0</span></span>
<span id="cb220-9"><a href="#cb220-9" aria-hidden="true" tabindex="-1"></a>app_file: app.py</span>
<span id="cb220-10"><a href="#cb220-10" aria-hidden="true" tabindex="-1"></a>pinned: false</span>
<span id="cb220-11"><a href="#cb220-11" aria-hidden="true" tabindex="-1"></a>license: apache<span class="op">-</span><span class="fl">2.0</span></span>
<span id="cb220-12"><a href="#cb220-12" aria-hidden="true" tabindex="-1"></a><span class="op">---</span></span>
<span id="cb220-13"><a href="#cb220-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-14"><a href="#cb220-14" aria-hidden="true" tabindex="-1"></a><span class="co"># ðŸš® Trashify Object Detector Demo V3</span></span>
<span id="cb220-15"><a href="#cb220-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-16"><a href="#cb220-16" aria-hidden="true" tabindex="-1"></a>Object detection demo to detect `trash`, `bin`, `hand`, `trash_arm`, `not_trash`, `not_bin`, `not_hand`. </span>
<span id="cb220-17"><a href="#cb220-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-18"><a href="#cb220-18" aria-hidden="true" tabindex="-1"></a>Used <span class="im">as</span> example <span class="cf">for</span> encouraging people to cleanup their local area.</span>
<span id="cb220-19"><a href="#cb220-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-20"><a href="#cb220-20" aria-hidden="true" tabindex="-1"></a>If `trash`, `hand`, `bin` <span class="bu">all</span> detected <span class="op">=</span> <span class="op">+</span><span class="dv">1</span> point.</span>
<span id="cb220-21"><a href="#cb220-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-22"><a href="#cb220-22" aria-hidden="true" tabindex="-1"></a><span class="co">## Dataset</span></span>
<span id="cb220-23"><a href="#cb220-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-24"><a href="#cb220-24" aria-hidden="true" tabindex="-1"></a>All Trashify models are trained on a custom hand<span class="op">-</span>labelled dataset of people picking up trash <span class="kw">and</span> placing it <span class="kw">in</span> a <span class="bu">bin</span>.</span>
<span id="cb220-25"><a href="#cb220-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-26"><a href="#cb220-26" aria-hidden="true" tabindex="-1"></a>The dataset can be found on Hugging Face <span class="im">as</span> [`mrdbourke<span class="op">/</span>trashify_manual_labelled_images`](https:<span class="op">//</span>huggingface.co<span class="op">/</span>datasets<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_manual_labelled_images).</span>
<span id="cb220-27"><a href="#cb220-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-28"><a href="#cb220-28" aria-hidden="true" tabindex="-1"></a><span class="co">## Demos</span></span>
<span id="cb220-29"><a href="#cb220-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-30"><a href="#cb220-30" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> [V1](https:<span class="op">//</span>huggingface.co<span class="op">/</span>spaces<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_demo_v1) <span class="op">=</span> Fine<span class="op">-</span>tuned DETR model trained <span class="op">*</span>without<span class="op">*</span> data augmentation.</span>
<span id="cb220-31"><a href="#cb220-31" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> [V2](https:<span class="op">//</span>huggingface.co<span class="op">/</span>spaces<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_demo_v2) <span class="op">=</span> Fine<span class="op">-</span>tuned DETR model trained <span class="op">*</span><span class="cf">with</span><span class="op">*</span> data augmentation.</span>
<span id="cb220-32"><a href="#cb220-32" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> [V3](https:<span class="op">//</span>huggingface.co<span class="op">/</span>spaces<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_demo_v3) <span class="op">=</span> Fine<span class="op">-</span>tuned DETR model trained <span class="op">*</span><span class="cf">with</span><span class="op">*</span> data augmentation (same <span class="im">as</span> V2) <span class="cf">with</span> an NMS (Non Maximum Suppression) post<span class="op">-</span>processing step.</span>
<span id="cb220-33"><a href="#cb220-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-34"><a href="#cb220-34" aria-hidden="true" tabindex="-1"></a>TK <span class="op">-</span> finish the README.md <span class="op">+</span> update <span class="cf">with</span> links to materials</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting demos/trashify_object_detector_data_aug_model_with_nms/README.md</code></pre>
</div>
</div>
<div id="cell-218" class="cell" data-execution_count="150">
<div class="sourceCode cell-code" id="cb222"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb222-1"><a href="#cb222-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile demos<span class="op">/</span>trashify_object_detector_data_aug_model_with_nms<span class="op">/</span>app.py</span>
<span id="cb222-2"><a href="#cb222-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb222-3"><a href="#cb222-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb222-4"><a href="#cb222-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image, ImageDraw, ImageFont</span>
<span id="cb222-5"><a href="#cb222-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-6"><a href="#cb222-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoImageProcessor</span>
<span id="cb222-7"><a href="#cb222-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForObjectDetection</span>
<span id="cb222-8"><a href="#cb222-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-9"><a href="#cb222-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Can load from Hugging Face or can load from local.</span></span>
<span id="cb222-10"><a href="#cb222-10" aria-hidden="true" tabindex="-1"></a><span class="co"># You will have to replace {mrdbourke} for your own username if the model is on your Hugging Face account.</span></span>
<span id="cb222-11"><a href="#cb222-11" aria-hidden="true" tabindex="-1"></a>model_save_path <span class="op">=</span> <span class="st">"mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug"</span> </span>
<span id="cb222-12"><a href="#cb222-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-13"><a href="#cb222-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model and preprocessor</span></span>
<span id="cb222-14"><a href="#cb222-14" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoImageProcessor.from_pretrained(model_save_path)</span>
<span id="cb222-15"><a href="#cb222-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForObjectDetection.from_pretrained(model_save_path)</span>
<span id="cb222-16"><a href="#cb222-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-17"><a href="#cb222-17" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb222-18"><a href="#cb222-18" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb222-19"><a href="#cb222-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-20"><a href="#cb222-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the id2label dictionary from the model</span></span>
<span id="cb222-21"><a href="#cb222-21" aria-hidden="true" tabindex="-1"></a>id2label <span class="op">=</span> model.config.id2label</span>
<span id="cb222-22"><a href="#cb222-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-23"><a href="#cb222-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up a colour dictionary for plotting boxes with different colours</span></span>
<span id="cb222-24"><a href="#cb222-24" aria-hidden="true" tabindex="-1"></a>color_dict <span class="op">=</span> {   </span>
<span id="cb222-25"><a href="#cb222-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">"bin"</span>: <span class="st">"green"</span>,</span>
<span id="cb222-26"><a href="#cb222-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">"trash"</span>: <span class="st">"blue"</span>,</span>
<span id="cb222-27"><a href="#cb222-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">"hand"</span>: <span class="st">"purple"</span>,</span>
<span id="cb222-28"><a href="#cb222-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">"trash_arm"</span>: <span class="st">"yellow"</span>,</span>
<span id="cb222-29"><a href="#cb222-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not_trash"</span>: <span class="st">"red"</span>,</span>
<span id="cb222-30"><a href="#cb222-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not_bin"</span>: <span class="st">"red"</span>,</span>
<span id="cb222-31"><a href="#cb222-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not_hand"</span>: <span class="st">"red"</span>,</span>
<span id="cb222-32"><a href="#cb222-32" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb222-33"><a href="#cb222-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-34"><a href="#cb222-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Create helper functions for seeing if items from one list are in another </span></span>
<span id="cb222-35"><a href="#cb222-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> any_in_list(list_a, list_b):</span>
<span id="cb222-36"><a href="#cb222-36" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Returns True if any item from list_a is in list_b, otherwise False."</span></span>
<span id="cb222-37"><a href="#cb222-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">any</span>(item <span class="kw">in</span> list_b <span class="cf">for</span> item <span class="kw">in</span> list_a)</span>
<span id="cb222-38"><a href="#cb222-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-39"><a href="#cb222-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> all_in_list(list_a, list_b):</span>
<span id="cb222-40"><a href="#cb222-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Returns True if all items from list_a are in list_b, otherwise False."</span></span>
<span id="cb222-41"><a href="#cb222-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">all</span>(item <span class="kw">in</span> list_b <span class="cf">for</span> item <span class="kw">in</span> list_a)</span>
<span id="cb222-42"><a href="#cb222-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-43"><a href="#cb222-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> filter_highest_scoring_box_per_class(boxes, labels, scores):</span>
<span id="cb222-44"><a href="#cb222-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb222-45"><a href="#cb222-45" aria-hidden="true" tabindex="-1"></a><span class="co">    Perform NMS (Non-max Supression) to only keep the top scoring box per class.</span></span>
<span id="cb222-46"><a href="#cb222-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-47"><a href="#cb222-47" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb222-48"><a href="#cb222-48" aria-hidden="true" tabindex="-1"></a><span class="co">        boxes: tensor of shape (N, 4)</span></span>
<span id="cb222-49"><a href="#cb222-49" aria-hidden="true" tabindex="-1"></a><span class="co">        labels: tensor of shape (N,)</span></span>
<span id="cb222-50"><a href="#cb222-50" aria-hidden="true" tabindex="-1"></a><span class="co">        scores: tensor of shape (N,)</span></span>
<span id="cb222-51"><a href="#cb222-51" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb222-52"><a href="#cb222-52" aria-hidden="true" tabindex="-1"></a><span class="co">        boxes: tensor of shape (N, 4) filtered for max scoring item per class</span></span>
<span id="cb222-53"><a href="#cb222-53" aria-hidden="true" tabindex="-1"></a><span class="co">        labels: tensor of shape (N,) filtered for max scoring item per class</span></span>
<span id="cb222-54"><a href="#cb222-54" aria-hidden="true" tabindex="-1"></a><span class="co">        scores: tensor of shape (N,) filtered for max scoring item per class</span></span>
<span id="cb222-55"><a href="#cb222-55" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb222-56"><a href="#cb222-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Start with a blank keep mask (e.g. all False and then update the boxes to keep with True)</span></span>
<span id="cb222-57"><a href="#cb222-57" aria-hidden="true" tabindex="-1"></a>    keep_mask <span class="op">=</span> torch.zeros(<span class="bu">len</span>(boxes), dtype<span class="op">=</span>torch.<span class="bu">bool</span>)</span>
<span id="cb222-58"><a href="#cb222-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-59"><a href="#cb222-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For each unique class</span></span>
<span id="cb222-60"><a href="#cb222-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> class_id <span class="kw">in</span> labels.unique():</span>
<span id="cb222-61"><a href="#cb222-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the indicies for the target class</span></span>
<span id="cb222-62"><a href="#cb222-62" aria-hidden="true" tabindex="-1"></a>        class_mask <span class="op">=</span> labels <span class="op">==</span> class_id</span>
<span id="cb222-63"><a href="#cb222-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-64"><a href="#cb222-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If any of the labels match the current class_id</span></span>
<span id="cb222-65"><a href="#cb222-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> class_mask.<span class="bu">any</span>():</span>
<span id="cb222-66"><a href="#cb222-66" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Find the index of highest scoring box for this specific class</span></span>
<span id="cb222-67"><a href="#cb222-67" aria-hidden="true" tabindex="-1"></a>            class_scores <span class="op">=</span> scores[class_mask]</span>
<span id="cb222-68"><a href="#cb222-68" aria-hidden="true" tabindex="-1"></a>            highest_score_idx <span class="op">=</span> class_scores.argmax()</span>
<span id="cb222-69"><a href="#cb222-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-70"><a href="#cb222-70" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convert back to the original index</span></span>
<span id="cb222-71"><a href="#cb222-71" aria-hidden="true" tabindex="-1"></a>            original_idx <span class="op">=</span> torch.where(class_mask)[<span class="dv">0</span>][highest_score_idx]</span>
<span id="cb222-72"><a href="#cb222-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-73"><a href="#cb222-73" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update the index in the keep mask to keep the highest scoring box </span></span>
<span id="cb222-74"><a href="#cb222-74" aria-hidden="true" tabindex="-1"></a>            keep_mask[original_idx] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb222-75"><a href="#cb222-75" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb222-76"><a href="#cb222-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> boxes[keep_mask], labels[keep_mask], scores[keep_mask]</span>
<span id="cb222-77"><a href="#cb222-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-78"><a href="#cb222-78" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_return_string(list_of_predicted_labels, target_items<span class="op">=</span>[<span class="st">"trash"</span>, <span class="st">"bin"</span>, <span class="st">"hand"</span>]):</span>
<span id="cb222-79"><a href="#cb222-79" aria-hidden="true" tabindex="-1"></a>     <span class="co"># Setup blank string to print out</span></span>
<span id="cb222-80"><a href="#cb222-80" aria-hidden="true" tabindex="-1"></a>    return_string <span class="op">=</span> <span class="st">""</span></span>
<span id="cb222-81"><a href="#cb222-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-82"><a href="#cb222-82" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If no items detected or trash, bin, hand not in list, return notification </span></span>
<span id="cb222-83"><a href="#cb222-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="bu">len</span>(list_of_predicted_labels) <span class="op">==</span> <span class="dv">0</span>) <span class="kw">or</span> <span class="kw">not</span> (any_in_list(list_a<span class="op">=</span>target_items, list_b<span class="op">=</span>list_of_predicted_labels)):</span>
<span id="cb222-84"><a href="#cb222-84" aria-hidden="true" tabindex="-1"></a>        return_string <span class="op">=</span> <span class="ss">f"No trash, bin or hand detected at confidence threshold </span><span class="sc">{</span>conf_threshold<span class="sc">}</span><span class="ss">. Try another image or lowering the confidence threshold."</span></span>
<span id="cb222-85"><a href="#cb222-85" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> return_string</span>
<span id="cb222-86"><a href="#cb222-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-87"><a href="#cb222-87" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If there are some missing, print the ones which are missing</span></span>
<span id="cb222-88"><a href="#cb222-88" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="kw">not</span> all_in_list(list_a<span class="op">=</span>target_items, list_b<span class="op">=</span>list_of_predicted_labels):</span>
<span id="cb222-89"><a href="#cb222-89" aria-hidden="true" tabindex="-1"></a>        missing_items <span class="op">=</span> []</span>
<span id="cb222-90"><a href="#cb222-90" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> item <span class="kw">in</span> target_items:</span>
<span id="cb222-91"><a href="#cb222-91" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> item <span class="kw">not</span> <span class="kw">in</span> list_of_predicted_labels:</span>
<span id="cb222-92"><a href="#cb222-92" aria-hidden="true" tabindex="-1"></a>                missing_items.append(item)</span>
<span id="cb222-93"><a href="#cb222-93" aria-hidden="true" tabindex="-1"></a>        return_string <span class="op">=</span> <span class="ss">f"Detected the following items: </span><span class="sc">{</span>list_of_predicted_labels<span class="sc">}</span><span class="ss"> (total: </span><span class="sc">{</span><span class="bu">len</span>(list_of_predicted_labels)<span class="sc">}</span><span class="ss">). But missing the following in order to get +1: </span><span class="sc">{</span>missing_items<span class="sc">}</span><span class="ss">. If this is an error, try another image or altering the confidence threshold. Otherwise, the model may need to be updated with better data."</span></span>
<span id="cb222-94"><a href="#cb222-94" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb222-95"><a href="#cb222-95" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If all 3 trash, bin, hand occur = + 1</span></span>
<span id="cb222-96"><a href="#cb222-96" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> all_in_list(list_a<span class="op">=</span>target_items, list_b<span class="op">=</span>list_of_predicted_labels):</span>
<span id="cb222-97"><a href="#cb222-97" aria-hidden="true" tabindex="-1"></a>        return_string <span class="op">=</span> <span class="ss">f"+1! Found the following items: </span><span class="sc">{</span>list_of_predicted_labels<span class="sc">}</span><span class="ss"> (total: </span><span class="sc">{</span><span class="bu">len</span>(list_of_predicted_labels)<span class="sc">}</span><span class="ss">), thank you for cleaning up the area!"</span></span>
<span id="cb222-98"><a href="#cb222-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-99"><a href="#cb222-99" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(return_string)</span>
<span id="cb222-100"><a href="#cb222-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-101"><a href="#cb222-101" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> return_string</span>
<span id="cb222-102"><a href="#cb222-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-103"><a href="#cb222-103" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_on_image(image, conf_threshold):</span>
<span id="cb222-104"><a href="#cb222-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb222-105"><a href="#cb222-105" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> image_processor(images<span class="op">=</span>[image], return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb222-106"><a href="#cb222-106" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>inputs.to(device))</span>
<span id="cb222-107"><a href="#cb222-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-108"><a href="#cb222-108" aria-hidden="true" tabindex="-1"></a>        target_sizes <span class="op">=</span> torch.tensor([[image.size[<span class="dv">1</span>], image.size[<span class="dv">0</span>]]]) <span class="co"># height, width </span></span>
<span id="cb222-109"><a href="#cb222-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-110"><a href="#cb222-110" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> image_processor.post_process_object_detection(outputs,</span>
<span id="cb222-111"><a href="#cb222-111" aria-hidden="true" tabindex="-1"></a>                                                                threshold<span class="op">=</span>conf_threshold,</span>
<span id="cb222-112"><a href="#cb222-112" aria-hidden="true" tabindex="-1"></a>                                                                target_sizes<span class="op">=</span>target_sizes)[<span class="dv">0</span>]</span>
<span id="cb222-113"><a href="#cb222-113" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return all items in results to CPU</span></span>
<span id="cb222-114"><a href="#cb222-114" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> key, value <span class="kw">in</span> results.items():</span>
<span id="cb222-115"><a href="#cb222-115" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb222-116"><a href="#cb222-116" aria-hidden="true" tabindex="-1"></a>            results[key] <span class="op">=</span> value.item().cpu() <span class="co"># can't get scalar as .item() so add try/except block</span></span>
<span id="cb222-117"><a href="#cb222-117" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span>:</span>
<span id="cb222-118"><a href="#cb222-118" aria-hidden="true" tabindex="-1"></a>            results[key] <span class="op">=</span> value.cpu()</span>
<span id="cb222-119"><a href="#cb222-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-120"><a href="#cb222-120" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Can return results as plotted on a PIL image (then display the image)</span></span>
<span id="cb222-121"><a href="#cb222-121" aria-hidden="true" tabindex="-1"></a>    draw <span class="op">=</span> ImageDraw.Draw(image)</span>
<span id="cb222-122"><a href="#cb222-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-123"><a href="#cb222-123" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a copy of the image to draw on it for NMS</span></span>
<span id="cb222-124"><a href="#cb222-124" aria-hidden="true" tabindex="-1"></a>    image_nms <span class="op">=</span> image.copy()</span>
<span id="cb222-125"><a href="#cb222-125" aria-hidden="true" tabindex="-1"></a>    draw_nms <span class="op">=</span> ImageDraw.Draw(image_nms)</span>
<span id="cb222-126"><a href="#cb222-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-127"><a href="#cb222-127" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get a font from ImageFont</span></span>
<span id="cb222-128"><a href="#cb222-128" aria-hidden="true" tabindex="-1"></a>    font <span class="op">=</span> ImageFont.load_default(size<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb222-129"><a href="#cb222-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-130"><a href="#cb222-130" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get class names as text for print out</span></span>
<span id="cb222-131"><a href="#cb222-131" aria-hidden="true" tabindex="-1"></a>    class_name_text_labels <span class="op">=</span> []</span>
<span id="cb222-132"><a href="#cb222-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-133"><a href="#cb222-133" aria-hidden="true" tabindex="-1"></a>    <span class="co"># TK - update this for NMS</span></span>
<span id="cb222-134"><a href="#cb222-134" aria-hidden="true" tabindex="-1"></a>    class_name_text_labels_nms <span class="op">=</span> []</span>
<span id="cb222-135"><a href="#cb222-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-136"><a href="#cb222-136" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get original boxes, scores, labels</span></span>
<span id="cb222-137"><a href="#cb222-137" aria-hidden="true" tabindex="-1"></a>    original_boxes <span class="op">=</span> results[<span class="st">"boxes"</span>]</span>
<span id="cb222-138"><a href="#cb222-138" aria-hidden="true" tabindex="-1"></a>    original_labels <span class="op">=</span> results[<span class="st">"labels"</span>]</span>
<span id="cb222-139"><a href="#cb222-139" aria-hidden="true" tabindex="-1"></a>    original_scores <span class="op">=</span> results[<span class="st">"scores"</span>]</span>
<span id="cb222-140"><a href="#cb222-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-141"><a href="#cb222-141" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Filter boxes and only keep 1x of each label with highest score</span></span>
<span id="cb222-142"><a href="#cb222-142" aria-hidden="true" tabindex="-1"></a>    filtered_boxes, filtered_labels, filtered_scores <span class="op">=</span> filter_highest_scoring_box_per_class(boxes<span class="op">=</span>original_boxes,</span>
<span id="cb222-143"><a href="#cb222-143" aria-hidden="true" tabindex="-1"></a>                                                                                            labels<span class="op">=</span>original_labels,</span>
<span id="cb222-144"><a href="#cb222-144" aria-hidden="true" tabindex="-1"></a>                                                                                            scores<span class="op">=</span>original_scores)</span>
<span id="cb222-145"><a href="#cb222-145" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: turn this into a function so it's cleaner?</span></span>
<span id="cb222-146"><a href="#cb222-146" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> box, label, score <span class="kw">in</span> <span class="bu">zip</span>(original_boxes, original_labels, original_scores):</span>
<span id="cb222-147"><a href="#cb222-147" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create coordinates</span></span>
<span id="cb222-148"><a href="#cb222-148" aria-hidden="true" tabindex="-1"></a>        x, y, x2, y2 <span class="op">=</span> <span class="bu">tuple</span>(box.tolist())</span>
<span id="cb222-149"><a href="#cb222-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-150"><a href="#cb222-150" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get label_name</span></span>
<span id="cb222-151"><a href="#cb222-151" aria-hidden="true" tabindex="-1"></a>        label_name <span class="op">=</span> id2label[label.item()]</span>
<span id="cb222-152"><a href="#cb222-152" aria-hidden="true" tabindex="-1"></a>        targ_color <span class="op">=</span> color_dict[label_name]</span>
<span id="cb222-153"><a href="#cb222-153" aria-hidden="true" tabindex="-1"></a>        class_name_text_labels.append(label_name)</span>
<span id="cb222-154"><a href="#cb222-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-155"><a href="#cb222-155" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw the rectangle</span></span>
<span id="cb222-156"><a href="#cb222-156" aria-hidden="true" tabindex="-1"></a>        draw.rectangle(xy<span class="op">=</span>(x, y, x2, y2), </span>
<span id="cb222-157"><a href="#cb222-157" aria-hidden="true" tabindex="-1"></a>                       outline<span class="op">=</span>targ_color,</span>
<span id="cb222-158"><a href="#cb222-158" aria-hidden="true" tabindex="-1"></a>                       width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb222-159"><a href="#cb222-159" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb222-160"><a href="#cb222-160" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a text string to display</span></span>
<span id="cb222-161"><a href="#cb222-161" aria-hidden="true" tabindex="-1"></a>        text_string_to_show <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>label_name<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score.item(), <span class="dv">3</span>)<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb222-162"><a href="#cb222-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-163"><a href="#cb222-163" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw the text on the image</span></span>
<span id="cb222-164"><a href="#cb222-164" aria-hidden="true" tabindex="-1"></a>        draw.text(xy<span class="op">=</span>(x, y),</span>
<span id="cb222-165"><a href="#cb222-165" aria-hidden="true" tabindex="-1"></a>                  text<span class="op">=</span>text_string_to_show,</span>
<span id="cb222-166"><a href="#cb222-166" aria-hidden="true" tabindex="-1"></a>                  fill<span class="op">=</span><span class="st">"white"</span>,</span>
<span id="cb222-167"><a href="#cb222-167" aria-hidden="true" tabindex="-1"></a>                  font<span class="op">=</span>font)</span>
<span id="cb222-168"><a href="#cb222-168" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb222-169"><a href="#cb222-169" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: turn this into a function so it's cleaner?</span></span>
<span id="cb222-170"><a href="#cb222-170" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> box, label, score <span class="kw">in</span> <span class="bu">zip</span>(filtered_boxes, filtered_labels, filtered_scores):</span>
<span id="cb222-171"><a href="#cb222-171" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create coordinates</span></span>
<span id="cb222-172"><a href="#cb222-172" aria-hidden="true" tabindex="-1"></a>        x, y, x2, y2 <span class="op">=</span> <span class="bu">tuple</span>(box.tolist())</span>
<span id="cb222-173"><a href="#cb222-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-174"><a href="#cb222-174" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get label_name</span></span>
<span id="cb222-175"><a href="#cb222-175" aria-hidden="true" tabindex="-1"></a>        label_name <span class="op">=</span> id2label[label.item()]</span>
<span id="cb222-176"><a href="#cb222-176" aria-hidden="true" tabindex="-1"></a>        targ_color <span class="op">=</span> color_dict[label_name]</span>
<span id="cb222-177"><a href="#cb222-177" aria-hidden="true" tabindex="-1"></a>        class_name_text_labels_nms.append(label_name)</span>
<span id="cb222-178"><a href="#cb222-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-179"><a href="#cb222-179" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw the rectangle</span></span>
<span id="cb222-180"><a href="#cb222-180" aria-hidden="true" tabindex="-1"></a>        draw_nms.rectangle(xy<span class="op">=</span>(x, y, x2, y2), </span>
<span id="cb222-181"><a href="#cb222-181" aria-hidden="true" tabindex="-1"></a>                       outline<span class="op">=</span>targ_color,</span>
<span id="cb222-182"><a href="#cb222-182" aria-hidden="true" tabindex="-1"></a>                       width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb222-183"><a href="#cb222-183" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb222-184"><a href="#cb222-184" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a text string to display</span></span>
<span id="cb222-185"><a href="#cb222-185" aria-hidden="true" tabindex="-1"></a>        text_string_to_show <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>label_name<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score.item(), <span class="dv">3</span>)<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb222-186"><a href="#cb222-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-187"><a href="#cb222-187" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw the text on the image</span></span>
<span id="cb222-188"><a href="#cb222-188" aria-hidden="true" tabindex="-1"></a>        draw_nms.text(xy<span class="op">=</span>(x, y),</span>
<span id="cb222-189"><a href="#cb222-189" aria-hidden="true" tabindex="-1"></a>                  text<span class="op">=</span>text_string_to_show,</span>
<span id="cb222-190"><a href="#cb222-190" aria-hidden="true" tabindex="-1"></a>                  fill<span class="op">=</span><span class="st">"white"</span>,</span>
<span id="cb222-191"><a href="#cb222-191" aria-hidden="true" tabindex="-1"></a>                  font<span class="op">=</span>font)</span>
<span id="cb222-192"><a href="#cb222-192" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb222-193"><a href="#cb222-193" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb222-194"><a href="#cb222-194" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove the draw each time</span></span>
<span id="cb222-195"><a href="#cb222-195" aria-hidden="true" tabindex="-1"></a>    <span class="kw">del</span> draw</span>
<span id="cb222-196"><a href="#cb222-196" aria-hidden="true" tabindex="-1"></a>    <span class="kw">del</span> draw_nms</span>
<span id="cb222-197"><a href="#cb222-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-198"><a href="#cb222-198" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the return string</span></span>
<span id="cb222-199"><a href="#cb222-199" aria-hidden="true" tabindex="-1"></a>    return_string <span class="op">=</span> create_return_string(list_of_predicted_labels<span class="op">=</span>class_name_text_labels)</span>
<span id="cb222-200"><a href="#cb222-200" aria-hidden="true" tabindex="-1"></a>    return_string_nms <span class="op">=</span> create_return_string(list_of_predicted_labels<span class="op">=</span>class_name_text_labels_nms)</span>
<span id="cb222-201"><a href="#cb222-201" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb222-202"><a href="#cb222-202" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image, return_string, image_nms, return_string_nms</span>
<span id="cb222-203"><a href="#cb222-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-204"><a href="#cb222-204" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the interface</span></span>
<span id="cb222-205"><a href="#cb222-205" aria-hidden="true" tabindex="-1"></a>demo <span class="op">=</span> gr.Interface(</span>
<span id="cb222-206"><a href="#cb222-206" aria-hidden="true" tabindex="-1"></a>    fn<span class="op">=</span>predict_on_image,</span>
<span id="cb222-207"><a href="#cb222-207" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>[</span>
<span id="cb222-208"><a href="#cb222-208" aria-hidden="true" tabindex="-1"></a>        gr.Image(<span class="bu">type</span><span class="op">=</span><span class="st">"pil"</span>, label<span class="op">=</span><span class="st">"Target Image"</span>),</span>
<span id="cb222-209"><a href="#cb222-209" aria-hidden="true" tabindex="-1"></a>        gr.Slider(minimum<span class="op">=</span><span class="dv">0</span>, maximum<span class="op">=</span><span class="dv">1</span>, value<span class="op">=</span><span class="fl">0.25</span>, label<span class="op">=</span><span class="st">"Confidence Threshold"</span>)</span>
<span id="cb222-210"><a href="#cb222-210" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb222-211"><a href="#cb222-211" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>[</span>
<span id="cb222-212"><a href="#cb222-212" aria-hidden="true" tabindex="-1"></a>        gr.Image(<span class="bu">type</span><span class="op">=</span><span class="st">"pil"</span>, label<span class="op">=</span><span class="st">"Image Output (no filtering)"</span>),</span>
<span id="cb222-213"><a href="#cb222-213" aria-hidden="true" tabindex="-1"></a>        gr.Text(label<span class="op">=</span><span class="st">"Text Output (no filtering)"</span>),</span>
<span id="cb222-214"><a href="#cb222-214" aria-hidden="true" tabindex="-1"></a>        gr.Image(<span class="bu">type</span><span class="op">=</span><span class="st">"pil"</span>, label<span class="op">=</span><span class="st">"Image Output (with max score per class box filtering)"</span>),</span>
<span id="cb222-215"><a href="#cb222-215" aria-hidden="true" tabindex="-1"></a>        gr.Text(label<span class="op">=</span><span class="st">"Text Output (with max score per class box filtering)"</span>)</span>
<span id="cb222-216"><a href="#cb222-216" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb222-217"><a href="#cb222-217" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb222-218"><a href="#cb222-218" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"ðŸš® Trashify Object Detection Demo V3"</span>,</span>
<span id="cb222-219"><a href="#cb222-219" aria-hidden="true" tabindex="-1"></a>    description<span class="op">=</span><span class="st">"""Help clean up your local area! Upload an image and get +1 if there is all of the following items detected: trash, bin, hand.</span></span>
<span id="cb222-220"><a href="#cb222-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-221"><a href="#cb222-221" aria-hidden="true" tabindex="-1"></a><span class="st">    The model in V3 is [same model](https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug) as in [V2](https://huggingface.co/spaces/mrdbourke/trashify_demo_v2) (trained with data augmentation) but has an additional post-processing step (NMS or [Non Maximum Suppression](https://paperswithcode.com/method/non-maximum-suppression)) to filter classes for only the highest scoring box of each class. </span></span>
<span id="cb222-222"><a href="#cb222-222" aria-hidden="true" tabindex="-1"></a><span class="st">    """</span>,</span>
<span id="cb222-223"><a href="#cb222-223" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Examples come in the form of a list of lists, where each inner list contains elements to prefill the `inputs` parameter with</span></span>
<span id="cb222-224"><a href="#cb222-224" aria-hidden="true" tabindex="-1"></a>    examples<span class="op">=</span>[</span>
<span id="cb222-225"><a href="#cb222-225" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"examples/trashify_example_1.jpeg"</span>, <span class="fl">0.25</span>],</span>
<span id="cb222-226"><a href="#cb222-226" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"examples/trashify_example_2.jpeg"</span>, <span class="fl">0.25</span>],</span>
<span id="cb222-227"><a href="#cb222-227" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"examples/trashify_example_3.jpeg"</span>, <span class="fl">0.25</span>]</span>
<span id="cb222-228"><a href="#cb222-228" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb222-229"><a href="#cb222-229" aria-hidden="true" tabindex="-1"></a>    cache_examples<span class="op">=</span><span class="va">True</span></span>
<span id="cb222-230"><a href="#cb222-230" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb222-231"><a href="#cb222-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-232"><a href="#cb222-232" aria-hidden="true" tabindex="-1"></a><span class="co"># Launch the demo</span></span>
<span id="cb222-233"><a href="#cb222-233" aria-hidden="true" tabindex="-1"></a>demo.launch()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting demos/trashify_object_detector_data_aug_model_with_nms/app.py</code></pre>
</div>
</div>
<section id="tk---upload-our-demo-to-the-hugging-face-hub" class="level3" data-number="17.1">
<h3 data-number="17.1" class="anchored" data-anchor-id="tk---upload-our-demo-to-the-hugging-face-hub"><span class="header-section-number">17.1</span> TK - Upload our demo to the Hugging Face Hub</h3>
<div id="cell-220" class="cell" data-execution_count="151">
<div class="sourceCode cell-code" id="cb224"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb224-1"><a href="#cb224-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Import the required methods for uploading to the Hugging Face Hub</span></span>
<span id="cb224-2"><a href="#cb224-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> (</span>
<span id="cb224-3"><a href="#cb224-3" aria-hidden="true" tabindex="-1"></a>    create_repo,</span>
<span id="cb224-4"><a href="#cb224-4" aria-hidden="true" tabindex="-1"></a>    get_full_repo_name,</span>
<span id="cb224-5"><a href="#cb224-5" aria-hidden="true" tabindex="-1"></a>    upload_file, <span class="co"># for uploading a single file (if necessary)</span></span>
<span id="cb224-6"><a href="#cb224-6" aria-hidden="true" tabindex="-1"></a>    upload_folder <span class="co"># for uploading multiple files (in a folder)</span></span>
<span id="cb224-7"><a href="#cb224-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb224-8"><a href="#cb224-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb224-9"><a href="#cb224-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Define the parameters we'd like to use for the upload</span></span>
<span id="cb224-10"><a href="#cb224-10" aria-hidden="true" tabindex="-1"></a>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD <span class="op">=</span> <span class="st">"demos/trashify_object_detector_data_aug_model_with_nms"</span> <span class="co"># TK - update this path </span></span>
<span id="cb224-11"><a href="#cb224-11" aria-hidden="true" tabindex="-1"></a>HF_TARGET_SPACE_NAME <span class="op">=</span> <span class="st">"trashify_demo_v3"</span></span>
<span id="cb224-12"><a href="#cb224-12" aria-hidden="true" tabindex="-1"></a>HF_REPO_TYPE <span class="op">=</span> <span class="st">"space"</span> <span class="co"># we're creating a Hugging Face Space</span></span>
<span id="cb224-13"><a href="#cb224-13" aria-hidden="true" tabindex="-1"></a>HF_SPACE_SDK <span class="op">=</span> <span class="st">"gradio"</span></span>
<span id="cb224-14"><a href="#cb224-14" aria-hidden="true" tabindex="-1"></a>HF_TOKEN <span class="op">=</span> <span class="st">""</span> <span class="co"># optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)</span></span>
<span id="cb224-15"><a href="#cb224-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb224-16"><a href="#cb224-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Create a Space repository on Hugging Face Hub </span></span>
<span id="cb224-17"><a href="#cb224-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Creating repo on Hugging Face Hub with name: </span><span class="sc">{</span>HF_TARGET_SPACE_NAME<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb224-18"><a href="#cb224-18" aria-hidden="true" tabindex="-1"></a>create_repo(</span>
<span id="cb224-19"><a href="#cb224-19" aria-hidden="true" tabindex="-1"></a>    repo_id<span class="op">=</span>HF_TARGET_SPACE_NAME,</span>
<span id="cb224-20"><a href="#cb224-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)</span></span>
<span id="cb224-21"><a href="#cb224-21" aria-hidden="true" tabindex="-1"></a>    repo_type<span class="op">=</span>HF_REPO_TYPE,</span>
<span id="cb224-22"><a href="#cb224-22" aria-hidden="true" tabindex="-1"></a>    private<span class="op">=</span><span class="va">False</span>, <span class="co"># set to True if you don't want your Space to be accessible to others</span></span>
<span id="cb224-23"><a href="#cb224-23" aria-hidden="true" tabindex="-1"></a>    space_sdk<span class="op">=</span>HF_SPACE_SDK,</span>
<span id="cb224-24"><a href="#cb224-24" aria-hidden="true" tabindex="-1"></a>    exist_ok<span class="op">=</span><span class="va">True</span>, <span class="co"># set to False if you want an error to raise if the repo_id already exists </span></span>
<span id="cb224-25"><a href="#cb224-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb224-26"><a href="#cb224-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb224-27"><a href="#cb224-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})</span></span>
<span id="cb224-28"><a href="#cb224-28" aria-hidden="true" tabindex="-1"></a>full_hf_repo_name <span class="op">=</span> get_full_repo_name(model_id<span class="op">=</span>HF_TARGET_SPACE_NAME)</span>
<span id="cb224-29"><a href="#cb224-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Full Hugging Face Hub repo name: </span><span class="sc">{</span>full_hf_repo_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb224-30"><a href="#cb224-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb224-31"><a href="#cb224-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Upload our demo folder</span></span>
<span id="cb224-32"><a href="#cb224-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Uploading </span><span class="sc">{</span>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD<span class="sc">}</span><span class="ss"> to repo: </span><span class="sc">{</span>full_hf_repo_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb224-33"><a href="#cb224-33" aria-hidden="true" tabindex="-1"></a>folder_upload_url <span class="op">=</span> upload_folder(</span>
<span id="cb224-34"><a href="#cb224-34" aria-hidden="true" tabindex="-1"></a>    repo_id<span class="op">=</span>full_hf_repo_name,</span>
<span id="cb224-35"><a href="#cb224-35" aria-hidden="true" tabindex="-1"></a>    folder_path<span class="op">=</span>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,</span>
<span id="cb224-36"><a href="#cb224-36" aria-hidden="true" tabindex="-1"></a>    path_in_repo<span class="op">=</span><span class="st">"."</span>, <span class="co"># upload our folder to the root directory ("." means "base" or "root", this is the default)</span></span>
<span id="cb224-37"><a href="#cb224-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># token=HF_TOKEN, # optional: set token manually</span></span>
<span id="cb224-38"><a href="#cb224-38" aria-hidden="true" tabindex="-1"></a>    repo_type<span class="op">=</span>HF_REPO_TYPE,</span>
<span id="cb224-39"><a href="#cb224-39" aria-hidden="true" tabindex="-1"></a>    commit_message<span class="op">=</span><span class="st">"Uploading Trashify box detection model v3 app.py with NMS post processing"</span></span>
<span id="cb224-40"><a href="#cb224-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb224-41"><a href="#cb224-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Demo folder successfully uploaded with commit URL: </span><span class="sc">{</span>folder_upload_url<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Creating repo on Hugging Face Hub with name: trashify_demo_v3
[INFO] Full Hugging Face Hub repo name: mrdbourke/trashify_demo_v3
[INFO] Uploading demos/trashify_object_detector_data_aug_model_with_nms to repo: mrdbourke/trashify_demo_v3
[INFO] Demo folder successfully uploaded with commit URL: https://huggingface.co/spaces/mrdbourke/trashify_demo_v3/tree/main/.</code></pre>
</div>
</div>
</section>
<section id="tk---embed-the-space-to-test-the-model" class="level3" data-number="17.2">
<h3 data-number="17.2" class="anchored" data-anchor-id="tk---embed-the-space-to-test-the-model"><span class="header-section-number">17.2</span> tK - Embed the Space to Test the Model</h3>
<div id="cell-222" class="cell" data-execution_count="158">
<div class="sourceCode cell-code" id="cb226"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb226-1"><a href="#cb226-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb226-2"><a href="#cb226-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb226-3"><a href="#cb226-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb226-4"><a href="#cb226-4" aria-hidden="true" tabindex="-1"></a><span class="co"># You can get embeddable HTML code for your demo by clicking the "Embed" button on the demo page</span></span>
<span id="cb226-5"><a href="#cb226-5" aria-hidden="true" tabindex="-1"></a>HTML(data<span class="op">=</span><span class="st">'''</span></span>
<span id="cb226-6"><a href="#cb226-6" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;iframe</span></span>
<span id="cb226-7"><a href="#cb226-7" aria-hidden="true" tabindex="-1"></a><span class="st">    src="https://mrdbourke-trashify-demo-v3.hf.space"</span></span>
<span id="cb226-8"><a href="#cb226-8" aria-hidden="true" tabindex="-1"></a><span class="st">    frameborder="0"</span></span>
<span id="cb226-9"><a href="#cb226-9" aria-hidden="true" tabindex="-1"></a><span class="st">    width="1000"</span></span>
<span id="cb226-10"><a href="#cb226-10" aria-hidden="true" tabindex="-1"></a><span class="st">    height="1600"</span></span>
<span id="cb226-11"><a href="#cb226-11" aria-hidden="true" tabindex="-1"></a><span class="st">&gt;&lt;/iframe&gt;     </span></span>
<span id="cb226-12"><a href="#cb226-12" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="158">

<iframe src="https://mrdbourke-trashify-demo-v3.hf.space" frameborder="0" width="1000" height="1600"></iframe>     
</div>
</div>
<div id="cell-224" class="cell">
<div class="sourceCode cell-code" id="cb227"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb227-1"><a href="#cb227-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UPTOHERE</span></span>
<span id="cb227-2"><a href="#cb227-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Next, focus on a single input -&gt; output âœ…</span></span>
<span id="cb227-3"><a href="#cb227-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Show case what an output from the model looks like untrained (e.g. plot the next boxes on it) âœ…</span></span>
<span id="cb227-4"><a href="#cb227-4" aria-hidden="true" tabindex="-1"></a><span class="co"># After showcasing 1x prediction, move onto training a model and seeing if we can get it to improve âœ…</span></span>
<span id="cb227-5"><a href="#cb227-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Continually focus on 1 input, 1 output until we can scale up âœ…</span></span>
<span id="cb227-6"><a href="#cb227-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a demo of our model and upload it to Hugging Face âœ…</span></span>
<span id="cb227-7"><a href="#cb227-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add examples to test the demo âœ…</span></span>
<span id="cb227-8"><a href="#cb227-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Write code to upload the demo to Hugging Face âœ…</span></span>
<span id="cb227-9"><a href="#cb227-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create visualization of input and output of data augmentation âœ…</span></span>
<span id="cb227-10"><a href="#cb227-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create demo of model with data augmentation âœ…</span></span>
<span id="cb227-11"><a href="#cb227-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 2: Try improve our model with data augmentation âœ…</span></span>
<span id="cb227-12"><a href="#cb227-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualize data augmentation examples in and out of the model </span></span>
<span id="cb227-13"><a href="#cb227-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Note: looks like augmentation may hurt our results... ðŸ¤”, this is because our data is so similar, potentially could help with more diverse data, e.g. synthetic data </span></span>
<span id="cb227-14"><a href="#cb227-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Try in a demo and see how it works -&gt; Trashify Demo V2 âœ… </span></span>
<span id="cb227-15"><a href="#cb227-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extension: Also try a model training for longer </span></span>
<span id="cb227-16"><a href="#cb227-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Model 3 (just improve with NMS): Create NMS option so only highest quality boxes are kept for each class âœ…</span></span>
<span id="cb227-17"><a href="#cb227-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb227-18"><a href="#cb227-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Next:</span></span>
<span id="cb227-19"><a href="#cb227-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb227-20"><a href="#cb227-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Go through notebook and clean it up for </span></span>
<span id="cb227-21"><a href="#cb227-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Once we've got a better performing model, introduce evaluation metrics</span></span>
<span id="cb227-22"><a href="#cb227-22" aria-hidden="true" tabindex="-1"></a><span class="co"># End: three models, three demos, one without data augmentation, one with it, one with NMS (post-processing) + can have as an extension to train the model for longer and see what happens</span></span>
<span id="cb227-23"><a href="#cb227-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb227-24"><a href="#cb227-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Extensions:</span></span>
<span id="cb227-25"><a href="#cb227-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a model for longer and see if it improves (e.g. 72 epochs) </span></span>
<span id="cb227-26"><a href="#cb227-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb227-27"><a href="#cb227-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Workflow:</span></span>
<span id="cb227-28"><a href="#cb227-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Untrained model -&gt; input/output -&gt; poor results (always visualize, visualize, visualize!)</span></span>
<span id="cb227-29"><a href="#cb227-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Trained model -&gt; input/output -&gt; better results (always visualize, visualize, visualize!)</span></span>
<span id="cb227-30"><a href="#cb227-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb227-31"><a href="#cb227-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Outline:</span></span>
<span id="cb227-32"><a href="#cb227-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Single input/output with untrained model (bad output)</span></span>
<span id="cb227-33"><a href="#cb227-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Train model to improve on single input/output</span></span>
<span id="cb227-34"><a href="#cb227-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Introduce evaluation metric</span></span>
<span id="cb227-35"><a href="#cb227-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Introduce data augmentation, see D-FINE paper for data augmentation options (we can keep it simple)</span></span>
<span id="cb227-36"><a href="#cb227-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># See: https://arxiv.org/pdf/2410.13842 </span></span>
<span id="cb227-37"><a href="#cb227-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># "The total batch size is 32 across all variants. Training schedules include 72 epochs with advanced augmentation (RandomPhotometricDistort, RandomZoomOut, RandomIoUCrop, and RMultiScaleInput)</span></span>
<span id="cb227-38"><a href="#cb227-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># followed by 2 epochs without advanced augmentation for D-FINE-X and D-FINE-L, and 120 epochs with advanced augmentation followed by 4</span></span>
<span id="cb227-39"><a href="#cb227-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># epochs without advanced augmentation for D-FINE-M and D-FINE-S (RT-DETRv2 Training Strategy (Lv et al., 2024) in Table 3)"</span></span>
<span id="cb227-40"><a href="#cb227-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Read RT-DETRv2 training strategy from paper mentioned above</span></span>
<span id="cb227-41"><a href="#cb227-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Read PP-YOLO data augmentation paper (keep it simple to begin with, can increase when needed)</span></span>
<span id="cb227-42"><a href="#cb227-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Create demo with Gradio</span></span>
<span id="cb227-43"><a href="#cb227-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Create demo with Transformers.js, see: https://huggingface.co/docs/transformers.js/en/tutorials/vanilla-js</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="extensions-extra-curriculum" class="level2" data-number="18">
<h2 data-number="18" class="anchored" data-anchor-id="extensions-extra-curriculum"><span class="header-section-number">18</span> Extensions + Extra-Curriculum</h2>
<ul>
<li>Extension: possibly improve the model with synthetic data? e.g.&nbsp;on classes/bins not visible in the model</li>
<li>Extension: train the model for longer and see how it improves, this could be model v4
<ul>
<li>Baselines:
<ul>
<li>V1 = model no data augmentaiton</li>
<li>V2 = model with data augmentation</li>
<li>V3 = model with NMS (post processing)</li>
</ul></li>
<li>Extensions:
<ul>
<li>V4 = model trained for longer with NMS</li>
<li>V5 = synthetic data scaled upâ€¦?</li>
</ul></li>
</ul></li>
<li>Extension: Zero-shot object detection - but what if I donâ€™t have labels?
<ul>
<li>This could discuss the use of zero-shot object detection models such as GroundingDINO and OmDet</li>
<li>See OmDet - https://huggingface.co/omlab/omdet-turbo-swin-tiny-hf</li>
<li>See GroundingDINO - https://huggingface.co/docs/transformers/en/model_doc/grounding-dino</li>
</ul></li>
<li>Extension: Try to repeat the workflow weâ€™ve gone through with another model such as https://huggingface.co/IDEA-Research/dab-detr-resnet-50-dc5-pat3 (apparently it is slightly better performing on COCO too)</li>
</ul>
</section>
<section id="summary" class="level2" data-number="19">
<h2 data-number="19" class="anchored" data-anchor-id="summary"><span class="header-section-number">19</span> Summary</h2>
<ul>
<li>Bounding box formats: An important step in any object detection project is to figure out what format your bounding boxes are in.</li>
</ul>
</section>
<section id="extra-resources" class="level2" data-number="20">
<h2 data-number="20" class="anchored" data-anchor-id="extra-resources"><span class="header-section-number">20</span> Extra resources</h2>
<ul>
<li><a href="https://www.learnml.io/posts/a-guide-to-bounding-box-formats/">A Guide to Bounding Box Formats and How to Draw Them</a> by Daniel Bourke.</li>
</ul>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.learnhuggingface\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mrdbourke/learn-huggingface/issues" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.mrdbourke.com">
      <i class="bi bi-globe" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mrdbourke/learn-huggingface">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/channel/UCr8O8l5cCX85Oem1d18EezQ">
      <i class="bi bi-youtube" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/mrdbourke">
      <i class="bi bi-twitter-x" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/mrdbourke/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>