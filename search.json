[
  {
    "objectID": "extras/setup.html",
    "href": "extras/setup.html",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "The following steps are to help you get started with the Hugging Face ecosystem.\nBest to follow the ‚ÄúStart here‚Äù steps and then go through the other setup steps as necessary.\n\n\n\nCreate a free Hugging Face account at https://huggingface.co/join.\nCreate a Hugging Face access token with read and write access at https://huggingface.co/settings/tokens.\n\nYou can create a read/write token using the fine-grained settings and selecting all the appropriate options.\nRead more on Hugging Face access tokens at https://huggingface.co/docs/hub/en/security-tokens.\n\n\n\n\n\n\nTo read from and write to your Hugging Face Hub account, you‚Äôll need to set up an access token. You can have one token for reading and one for writing. However, I personally use a single token for reading and writing.\n\n\nNote: Do not share your token with others. Always keep it private and avoid saving it in raw text format.\n\n\n\nNote: If you‚Äôre unfamiliar with Google Colab, I‚Äôd recommend going through Sam Witteveen‚Äôs video Colab 101 and then Advanced Colab to learn more.\n\nFollow the steps in Start here.\nAdd your Hugging Face read/write token as a Secret in Google Colab.\n\nNaming this Secret HF_TOKEN will mean that Hugging Face libraries automatically recognize your token for future use.\n\n\n\n\n\n\nFor accessing models and datasets from the Hugging Face Hub (both read and write) inside Google Colab, you‚Äôll need to add your Hugging Face token as a Secret in Google Colab. Once you give your Google Colab notebook access to the token, it can be used by Hugging Face libraries to interact with the Hugging Face Hub.\n\n\nAlternatively, if you need to force relogin for a notebook session, you can run:\nimport huggingface_hub # requires !pip install huggingface_hub\n\n# Login to Hugging Face\nhuggingface_hub.login()\nAnd enter your token in the box that appears (note: this token will only be active for the current notebook session and will delete when your Google Colab instance terminates).\n\n\n\n\nFollow the steps in Start here.\nInstall the Hugging Face CLI with pip install -U \"huggingface_hub[cli]\".\nFollow the setup steps mentioned in https://huggingface.co/docs/huggingface_hub/en/guides/cli.\n\n\n\n\nWe‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\nevaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\naccelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio."
  },
  {
    "objectID": "extras/setup.html#start-here-universal-steps",
    "href": "extras/setup.html#start-here-universal-steps",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "Create a free Hugging Face account at https://huggingface.co/join.\nCreate a Hugging Face access token with read and write access at https://huggingface.co/settings/tokens.\n\nYou can create a read/write token using the fine-grained settings and selecting all the appropriate options.\nRead more on Hugging Face access tokens at https://huggingface.co/docs/hub/en/security-tokens.\n\n\n\n\n\n\nTo read from and write to your Hugging Face Hub account, you‚Äôll need to set up an access token. You can have one token for reading and one for writing. However, I personally use a single token for reading and writing.\n\n\nNote: Do not share your token with others. Always keep it private and avoid saving it in raw text format."
  },
  {
    "objectID": "extras/setup.html#getting-setup-on-google-colab",
    "href": "extras/setup.html#getting-setup-on-google-colab",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "Note: If you‚Äôre unfamiliar with Google Colab, I‚Äôd recommend going through Sam Witteveen‚Äôs video Colab 101 and then Advanced Colab to learn more.\n\nFollow the steps in Start here.\nAdd your Hugging Face read/write token as a Secret in Google Colab.\n\nNaming this Secret HF_TOKEN will mean that Hugging Face libraries automatically recognize your token for future use.\n\n\n\n\n\n\nFor accessing models and datasets from the Hugging Face Hub (both read and write) inside Google Colab, you‚Äôll need to add your Hugging Face token as a Secret in Google Colab. Once you give your Google Colab notebook access to the token, it can be used by Hugging Face libraries to interact with the Hugging Face Hub.\n\n\nAlternatively, if you need to force relogin for a notebook session, you can run:\nimport huggingface_hub # requires !pip install huggingface_hub\n\n# Login to Hugging Face\nhuggingface_hub.login()\nAnd enter your token in the box that appears (note: this token will only be active for the current notebook session and will delete when your Google Colab instance terminates)."
  },
  {
    "objectID": "extras/setup.html#tk---getting-started-locally",
    "href": "extras/setup.html#tk---getting-started-locally",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "Follow the steps in Start here.\nInstall the Hugging Face CLI with pip install -U \"huggingface_hub[cli]\".\nFollow the setup steps mentioned in https://huggingface.co/docs/huggingface_hub/en/guides/cli."
  },
  {
    "objectID": "extras/setup.html#installing-hugging-face-libraries",
    "href": "extras/setup.html#installing-hugging-face-libraries",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "We‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\nevaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\naccelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio."
  },
  {
    "objectID": "extras/resources.html",
    "href": "extras/resources.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "See the Pytorch extra resources for some ideas: https://www.learnpytorch.io/pytorch_extra_resources/\nHugging Face NLP course: https://huggingface.co/learn/nlp-course/chapter0/1\nHugging Face forum: https://discuss.huggingface.co/\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html",
    "href": "notebooks/hugging_face_object_detection_tutorial.html",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "",
    "text": "Note: If you‚Äôre running in Google Colab, make sure to enable GPU usage by going to Runtime -&gt; Change runtime type -&gt; select GPU.\nSource code on GitHub | Online book version | Setup guide | Video Course (coming soon)",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#overview",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#overview",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "1 Overview",
    "text": "1 Overview\nWelcome to the Learn Hugging Face Object Detection project!\nInside this project, we‚Äôll learn bits and pieces about the Hugging Face ecosystem as well as how to build our own custom object detection model.\nWe‚Äôll start with a collection of images with bounding box files as our dataset, fine-tune an existing computer vision model to detect items in an image and then share our model as a demo others can use.\n\n\n\nWe‚Äôre going to put on our startup hats and build a Trashify object detection model using tools from the Hugging Face ecosystem.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFeel to keep reading through the notebook but if you‚Äôd like to run the code yourself, be sure to go through the setup guide first.\n\n\n\n1.1 What we‚Äôre going to build\nWe‚Äôre going to be bulding Trashify üöÆ, an object detection model which incentivises people to pick up trash in their local area by detecting bin, trash, hand.\nIf all three items are detected, a person gets +1 point!\nFor example, say you were going for a walk around your neighbourhood and took a photo of yourself picking up a piece (with your hand or trash arm) of trash and putting it in the bin, you would get a point.\nWith this object detection model, you could deploy it to an application which would automatically detect the target classes and then save the result to an online leaderboard.\nThe incentive would be to score the most points, in turn, picking up the most piecces of trash, in a given area.\nMore specifically, we‚Äôre going to follow the following steps:\n\nData: Problem defintion and dataset preparation - Getting a dataset/setting up the problem space.\nModel: Finding, training and evaluating a model - Finding an object detection model suitable for our problem on Hugging Face and customizing it to our own dataset.\nDemo: Creating a demo and put our model into the real world - Sharing our trained model in a way others can access and use.\n\nBy the end of this project, you‚Äôll have a trained model and demo on Hugging Face you can share with others:\n\nfrom IPython.display import HTML \n\nHTML(\"\"\"\n&lt;iframe\n    src=\"https://mrdbourke-trashify-demo-v4.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"1150\"\n&gt;&lt;/iframe&gt;\n\"\"\")\n\n\n\n\n\n\n\n1.2 What is object detection?\nObject detection is the process of identifying and locating an item in an image.\nWhere item can mean almost anything.\nFor example:\n\nDetecting car licence plates in a video feed (videos are a series of images) for a parking lot entrance.\nDetecting delivery people walking towards your front door on a security camera.\nDetecting defects on a manufacturing line.\nDetecting pot holes in the road so repair works can automatically be scheduled.\nDetecting small pests (Varroa Mite) on the bodies of bees.\nDetecting weeds in a field so you know what to remove and what to keep.\n\nAnd for some trash identification examples:\n\nGoogle open-sourcing CircularNet for helping to detect and identify different kinds of trash in waste management facilities.\nA machine learning paper for using a computer vision model on a Raspberry Pi (a small computer) for waste identification.\nAmeru is a company building a trash-identifying bin to sort automatically sort trash as people put it in, see the case study of hose they created their own custom dataset using Label Studio.\n\n\n\n\nDifferent problems where object detection can be used: license plate detection, trash type identification, pest detection on bees and pothole detection.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: Object detection is also sometimes referred to as image localization or object localization. For consistency, I will use the term object detection, however, either of these terms could substitute.\n\n\nImage classification deals with classifying an image as a whole into a single class, object detection endeavours to find the specific target item and where it is in an image.\nOne of the most common ways of showing where an item is in an image is by displaying a bounding box (a rectangle-like box around the target item).\nAn object detection model will often take an input image tensor in the shape [3, 640, 640] ([colour_channels, height, width]) and output a tensor in the form [class_name, x_min, y_min, x_max, y_max] or [class_name, x1, y1, x2, y2] (this is two ways to write the same example format, there are more formats, we‚Äôll see these below in Table¬†1).\nWhere:\n\nclass_name = The classification of the target item (e.g.¬†\"car\", \"person\", \"banana\", \"piece_of_trash\", this could be almost anything).\nx_min = The x value of the top left corner of the box.\ny_min = The y value of the top left corner of the box.\nx_max = The x value of the bottom right corner of the box.\ny_max = The y value of the bottom right corner of the box.\n\n\n\n\nNormalized bounding box coordinates in CXCYWH format as well as an absolute bounding in XYXY format.\n\n\n\n\n\n\n\n\nObject detection bounding box formats\n\n\n\nWhen you get into the world of object detection, you will find that there are several different bounding box formats.\nThere are three major formats you should be familiar with: XYXY, XYWH, CXCYWH (there are more but these are the most common).\nKnowing which bounding box format you‚Äôre working with can be the difference between a good model and a very poor model (wrong bounding boxes = wrong outcome).\nWe‚Äôll get hands-on with a couple of these in this project.\nBut for an in-depth example of all three, I created a guide on different bounding box formats and how to draw them, reading this should give a good intuition behind each style of bounding box.\n\n\n\n\n1.3 Why train your own object detection models?\nYou can customize pre-trained models for object detection as well as API-powered models and LLMs such as Gemini, LandingAI and DINO-X.\nDepending on your requirements, there are several pros and cons for using your own model versus using an API.\nTraining/fine-tuning your own model:\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nControl: Full control over model lifecycle.\nCan be complex to get setup.\n\n\nNo usage limits (aside from compute constraints).\nRequires dedicated compute resources for training/inference.\n\n\nCan train once and deploy everywhere/whenever you want (for example, Tesla deploying a model to all self-driving cars).\nRequires maintenance over time to ensure performance remains up to par.\n\n\nPrivacy: Data can be kept in-house/app and doesn‚Äôt need to go to a third party.\nCan require longer development cycles compared to using existing APIs.\n\n\nSpeed: Customizing a small model for a specific use case often means it runs much faster on local hardware, for example, modern object detection models can achieve 70-100+ FPS (frames per second) on modern GPU hardware.\n\n\n\n\nUsing a pre-built model API:\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEase of use: often can be setup within a few lines of code.\nIf the model API goes down, your service goes down.\n\n\nNo maintenance of compute resources.\nData is required to be sent to a third-party for processing.\n\n\nAccess to the most advanced models.\nThe API may have usage limits per day/time period.\n\n\nCan scale if usage increases.\nCan be much slower than using dedicated models due to requiring an API call.\n\n\n\nFor this project, we‚Äôre going to focus on fine-tuning our own model.\n\n\n1.4 Workflow we‚Äôre going to follow\nThe good news for us is that the Hugging Face ecosystem makes working on custom machine learning projects an absolute blast.\nAnd workflow is reproducible across several kinds of projects.\nStart with data (or skip this step and go straight to a model) -&gt; get/customize a model -&gt; build and share a demo.\nWith this in mind, our motto is data, model, demo!\nMore specifically, we‚Äôre going to follow the rough workflow of:\n\nCreate, preprocess and load data using Hugging Face Datasets.\nDefine the model we‚Äôd like use with transformers.AutoModelForObjectDetection (or another similar model class).\nDefine training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nI say rough because machine learning projects are often non-linear in nature.\nAs in, because machine learning projects involve many experiments, they can kind of be all over the place.\nBut this worfklow will give us some good guidelines to follow.\n\n\n\n\nA general Hugging Face workflow from idea to shared model and demo using tools from the Hugging Face ecosystem. You‚Äôll notice some of the steps don‚Äôt match with our workflow outline above. This is because the text-based workflow outline above breaks some of the steps down for educational purposes. These kind of workflows are not set in stone and are more of guide than specific directions. See information on each of the tools in the Hugging Face documentation.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#importing-necessary-libraries",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#importing-necessary-libraries",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "2 Importing necessary libraries",
    "text": "2 Importing necessary libraries\nLet‚Äôs get started!\nFirst, we‚Äôll import the required libraries.\nIf you‚Äôre running on your local computer, be sure to check out the getting setup guide to make sure you have everything you need.\nIf you‚Äôre using Google Colab, many of them the following libraries will be installed by default.\nHowever, we‚Äôll have to install a few extras to get everything working.\n\n\n\n\n\n\nNote\n\n\n\nIf you‚Äôre running on Google Colab, this notebook will work best with access to a GPU. To enable a GPU, go to Runtime ‚û°Ô∏è Change runtime type ‚û°Ô∏è Hardware accelerator ‚û°Ô∏è GPU.\n\n\nWe‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio.\nOptional: evaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\nOptional: accelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\n\nAnd the following library is not part of the Hugging Face ecosystem but it is helpful for evaluating our models:\n\ntorchmetrics - a library containing many evaluation metrics compatible with PyTorch/Transformers, you can install it via pip install torchmetrics.\n\nWe can also check the versions of our software with package_name.__version__.\n\n# Install/import dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)\ntry:\n  import datasets\n  import gradio as gr\n  import torchmetrics\n  import pycocotools \nexcept ModuleNotFoundError:\n  \n  # If a module isn't found, install it \n  !pip install -U datasets gradio # -U stands for \"upgrade\" so we'll get the latest version by default\n  !pip install -U torchmetrics[detection]\n\n  import datasets\n  import gradio as gr\n\n  # Required for evalation\n  import torchmetrics\n  import pycocotools # make sure we have this for torchmetrics\n\nimport random\n\nimport numpy as np\n\nimport torch\nimport transformers\n\n# Check versions (as long as you've got the following versions or higher, you should be good)\nprint(f\"Using transformers version: {transformers.__version__}\")\nprint(f\"Using datasets version: {datasets.__version__}\")\nprint(f\"Using torch version: {torch.__version__}\")\nprint(f\"Using torchmetrics version: {torchmetrics.__version__}\")\n\nUsing transformers version: 4.53.0\nUsing datasets version: 3.6.0\nUsing torch version: 2.7.0+cu126\nUsing torchmetrics version: 1.7.1\n\n\nWonderful, as long as your versions are the same or higher to the versions above, you should be able to run the code below.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#getting-a-dataset",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#getting-a-dataset",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "3 Getting a dataset",
    "text": "3 Getting a dataset\nOkay, now we‚Äôre got the required libraries, let‚Äôs get a dataset.\nGetting a dataset is one of the most important things a machine learning project.\nThe dataset you often determines the type of model you use as well as the quality of the outputs of that model.\nMeaning, if you have a high quality dataset, chances are, your future model could also have high quality outputs.\nIt also means if your dataset is of poor quality, your model will likely also have poor quality outputs.\nFor an object detection problem, your dataset will likely come in the form of a group of images as well as a file with annotations belonging to those images.\nFor example, you might have the following setup:\nfolder_of_images/\n    image_1.jpeg\n    image_2.jpeg\n    image_3.jpeg\nannotations.json\nWhere the annotations.json contains details about the contains of each image:\n\n\nannotations.json\n\n[\n    {\n        'image_path': 'image_1.jpeg', \n        'image_id': 42,\n        'annotations': \n            {\n                'file_name': ['image_1.jpeg'],\n                'image_id': [42],\n                'category_id': [1],\n                'bbox': [\n                            [360.20001220703125, 528.5, 177.1999969482422, 261.79998779296875],\n                        ],\n                'area': [46390.9609375]\n            },\n        'label_source': 'manual_prodigy_label',\n        'image_source': 'manual_taken_photo'\n    },\n\n    ...(more labels down here)\n]\n\nDon‚Äôt worry too much about the exact meaning of everything in the above annotations.json file for now (this is only one example, there are many different ways object detection information could be displayed).\nThe main point is that each target image is paired with an assosciated label.\nNow like all good machine learning cooking shows, I‚Äôve prepared a dataset from earlier.\n\n  \n\nOur Trashify dataset is available on Hugging Face. These images have been labelled manually with bounding boxes for different classes.\n\n\nIt‚Äôs stored on Hugging Face Datasets (also called the Hugging Face Hub) under the name mrdbourke/trashify_manual_labelled_images.\nThis is a dataset I‚Äôve collected manually by hand (yes, by picking up 1000+ pieces of trash and photographing it) as well as labelled by hand (by drawing boxes on each image with a labelling tool called Prodigy).\n\n3.1 Loading the dataset\nTo load a dataset stored on the Hugging Face Hub we can use the datasets.load_dataset(path=NAME_OR_PATH_OF_DATASET) function and pass it the name/path of the dataset we want to load.\nIn our case, our dataset name is mrdbourke/trashify_manual_labelled_images (you can also change this for your own dataset).\nAnd since our dataset is hosted on Hugging Face, when we run the following code for the first time, it will download it.\nIf your target dataset is quite large, this download may take a while.\nHowever, once the dataset is downloaded, subsequent reloads will be mush faster.\n\n\n\n\n\n\nGetting information about a function/method\n\n\n\nOne way to find out what a function or method does is to lookup the documentation.\nAnother way is to write the function/method name with a question mark afterwards.\nFor example:\nfrom datasets import load_dataset\n\nload_dataset?\nGive it a try.\nYou should see some helpful information about what inputs the method takes and how they are used.\n\n\nLet‚Äôs load our dataset and check it out.\n\nfrom datasets import load_dataset\n\n# Load our Trashify dataset\ndataset = load_dataset(path=\"mrdbourke/trashify_manual_labelled_images\")\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n        num_rows: 1128\n    })\n})\n\n\nBeautiful!\nWe can see that there is a train split of the dataset already which currently contains all of the samples (1128 in total).\nThere are also some features that come with our dataset which are related to our object detection goal.\n\nprint(f\"[INFO] Length of original dataset: {len(dataset['train'])}\")\nprint(f\"[INFO] Dataset features:\") \n\nfrom pprint import pprint\n\npprint(dataset['train'].features)\n\n[INFO] Length of original dataset: 1128\n[INFO] Dataset features:\n{'annotations': Sequence(feature={'area': Value(dtype='float32', id=None),\n                                  'bbox': Sequence(feature=Value(dtype='float32',\n                                                                 id=None),\n                                                   length=4,\n                                                   id=None),\n                                  'category_id': ClassLabel(names=['bin',\n                                                                   'hand',\n                                                                   'not_bin',\n                                                                   'not_hand',\n                                                                   'not_trash',\n                                                                   'trash',\n                                                                   'trash_arm'],\n                                                            id=None),\n                                  'file_name': Value(dtype='string', id=None),\n                                  'image_id': Value(dtype='int64', id=None),\n                                  'iscrowd': Value(dtype='int64', id=None)},\n                         length=-1,\n                         id=None),\n 'image': Image(mode=None, decode=True, id=None),\n 'image_id': Value(dtype='int64', id=None),\n 'image_source': Value(dtype='string', id=None),\n 'label_source': Value(dtype='string', id=None)}\n\n\nNice!\nWe can see our dataset features contain the following fields:\n\nannotations - A sequence of values including a bbox field (short for bounding box) as well as category_id field which contains the target objects we‚Äôd like to identify in our images (['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']).\nimage - This contains the target image assosciated with a given set of annotations (in our case, images and annotations have been uploaded to the Hugging Face Hub together).\nimage_id - A unique ID assigned to a given sample.\nimage_source - Where the image came from (all of our images have been manually collected).\nlabel_source - Where the image label came from (all of our images have been manually labelled).\n\n\n\n3.2 Viewing a single sample from our data\nNow we‚Äôve seen the features, let‚Äôs check out a single sample from our dataset.\nWe can index on a single sample of the \"train\" set just like indexing on a Python list.\n\n# View a single sample of the dataset\ndataset[\"train\"][42]\n\n{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n 'image_id': 745,\n 'annotations': {'file_name': ['094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg',\n   '094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg',\n   '094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg'],\n  'image_id': [745, 745, 745],\n  'category_id': [5, 1, 0],\n  'bbox': [[333.1000061035156,\n    611.2000122070312,\n    244.89999389648438,\n    321.29998779296875],\n   [504.0, 612.9000244140625, 451.29998779296875, 650.7999877929688],\n   [202.8000030517578,\n    366.20001220703125,\n    532.9000244140625,\n    555.4000244140625]],\n  'iscrowd': [0, 0, 0],\n  'area': [78686.3671875, 293706.03125, 295972.65625]},\n 'label_source': 'manual_prodigy_label',\n 'image_source': 'manual_taken_photo'}\n\n\nWe see a few more details here compared to just looking at the features.\nWe notice the image is a PIL.Image with size 960x1280 (width x height).\nAnd the file_name is a UUID (Universially Unique Identifier, made with uuid.uuid4()).\nThe bbox field in the annotations key contains a list of bounding boxes assosciated with the image.\nIn this case, there are 3 different bounding boxes.\nWith the category_id values of 5, 1, 0 (we‚Äôll map these to class names shortly).\nLet‚Äôs inspect a single bounding box.\n\ndataset[\"train\"][42][\"annotations\"][\"bbox\"][0]\n\n[333.1000061035156, 611.2000122070312, 244.89999389648438, 321.29998779296875]\n\n\nThis array gives us the coordinates of a single bounding box in the format XYWH.\nWhere:\n\nX is the x-coordinate of the top left corner of the box (333.1).\nY is the y-coordinate of the top left corner of the box (611.2).\nW is the width of the box (244.9).\nH is the height of the box (321.3).\n\nAll of these values are in absolute pixel values (meaning an x-coordinate of 333.1 is 333.1 pixels across on the x-axis).\nHow do I know this?\nI know this because I created the box labels and this is the default value Prodigy (the labelling tool I used) outputs boxes.\nHowever, if you were to come across another bouding box dataset, one of the first steps would be to figure out what format your bounding boxes are in.\nWe‚Äôll see more on bounding box formats shortly.\n\n\n3.3 Extracting the category names from our data\nBefore we start to visualize our sample image and bounding boxes, let‚Äôs extract the category names from our dataset.\nWe can do so by accessing the features attribute our of dataset and then following it through to find the category_id feature, this contains a list of our text-based class names.\n\n\n\n\n\n\nNote\n\n\n\nWhen working with different categories, it‚Äôs good practice to get a list or mapping (e.g.¬†a Python dictionary) from category name to ID and vice versa.\nFor example:\n# Category to ID\n{\"class_name\": 0}\n\n# ID to Category\n{0: \"class_name\"}\nNot all datasets will have this implemented in an easy to access way, so it might take a bit of research to get it created.\n\n\nLet‚Äôs access the class names in our dataset and save them to a variable categories.\n\n# Get the categories from the dataset\n# Note: This requires the dataset to have been uploaded with this information setup, not all datasets will have this available.\ncategories = dataset[\"train\"].features[\"annotations\"][\"category_id\"]\n\n# Get the names attribute\ncategories.feature.names\n\n['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']\n\n\nBeautiful!\nWe get the following class names:\n\nbin - A rubbish bin or trash can.\nhand - A person‚Äôs hand.\nnot_bin - Negative version of bin for items that look like a bin but shouldn‚Äôt be identified as one.\nnot_hand - Negative version of hand for items that look like a hand but shouldn‚Äôt be identified as one.\nnot_trash - Negative version of trash for items that look like trash but shouldn‚Äôt be identified as it.\ntrash - An item of trash you might find on a walk such as an old plastic bottle, food wrapper, cigarette butt or used coffee cup.\ntrash_arm - A mechanical arm used for picking up trash.\n\nThe goal of our computer vision model will be: given an image, detect items belonging to these target classes if they are present.\n\n\n3.4 Creating a mapping from numbers to labels\nNow we‚Äôve got our text-based class names, let‚Äôs create a mapping from label to ID and ID to label.\nFor each of these, Hugging Face use the terminology label2id and id2label respectively.\n\n# Map ID's to class names and vice versa\nid2label = {i: class_name for i, class_name in enumerate(categories.feature.names)}\nlabel2id = {value: key for key, value in id2label.items()}\n\nprint(f\"Label to ID mapping:\\n{label2id}\\n\")\nprint(f\"ID to label mapping:\\n{id2label}\")\n# id2label, label2id\n\nLabel to ID mapping:\n{'bin': 0, 'hand': 1, 'not_bin': 2, 'not_hand': 3, 'not_trash': 4, 'trash': 5, 'trash_arm': 6}\n\nID to label mapping:\n{0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash', 6: 'trash_arm'}\n\n\n\n\n3.5 Creating a colour palette\nOk we know which class name matches to which ID, now let‚Äôs create a dictionary of different colours we can use to display our bounding boxes.\nIt‚Äôs one thing to plot bounding boxes, it‚Äôs another thing to make them look nice.\nAnd we always want our plots looking nice!\nWe‚Äôll colour the positive classes bin, hand, trash, trash_arm in nice bright colours.\nAnd the negative classes not_bin, not_hand, not_trash in a light red colour to indicate they‚Äôre the negative versions.\nOur colour dictionary will map class_name -&gt; (red, green, blue) (or RGB) colour values.\n\n# Make colour dictionary\ncolour_palette = {\n    'bin': (0, 0, 224),         # Bright Blue (High contrast with greenery) in format (red, green, blue)\n    'not_bin': (255, 80, 80),   # Light Red to indicate negative class\n\n    'hand': (148, 0, 211),      # Dark Purple (Contrasts well with skin tones)\n    'not_hand': (255, 80, 80),  # Light Red to indicate negative class\n\n    'trash': (0, 255, 0),       # Bright Green (For trash-related items)\n    'not_trash': (255, 80, 80), # Light Red to indicate negative class\n\n    'trash_arm': (255, 140, 0), # Deep Orange (Highly visible)\n}\n\nLet‚Äôs check out what these colours look like!\nIt‚Äôs the ABV motto: Always Be Visualizing!\nWe can plot our colours with matplotlib.\nWe‚Äôll just have to write a small function to normalize our colour values from [0, 255] to [0, 1] (matplotlib expects our colour values to be between 0 and 1).\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Normalize RGB values to 0-1 range\ndef normalize_rgb(rgb_tuple):\n    return tuple(x/255 for x in rgb_tuple)\n\n# Turn colors into normalized RGB values for matplotlib\ncolors_and_labels_rgb = [(key, normalize_rgb(value)) for key, value in colour_palette.items()]\n\n# Create figure and axis\nfig, ax = plt.subplots(1, 7, figsize=(8, 1))\n\n# Flatten the axis array for easier iteration\nax = ax.flatten()\n\n# Plot each color square\nfor idx, (label, color) in enumerate(colors_and_labels_rgb):\n    ax[idx].add_patch(plt.Rectangle(xy=(0, 0), \n                                    width=1, \n                                    height=1, \n                                    facecolor=color))\n    ax[idx].set_title(label)\n    ax[idx].set_xlim(0, 1)\n    ax[idx].set_ylim(0, 1)\n    ax[idx].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSensational!\nNow we know what colours to look out for when we visualize our bounding boxes.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#plotting-a-single-image-and-visualizing-the-boxes",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#plotting-a-single-image-and-visualizing-the-boxes",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "4 Plotting a single image and visualizing the boxes",
    "text": "4 Plotting a single image and visualizing the boxes\nOkay, okay, finally time to plot an image!\nLet‚Äôs take a random sample from our dataset and plot the image as well as the box on it.\n\n4.1 Creating helper functions to help with visualization\nTo save some space in our notebook (plotting many images can increase the size of our notebook dramatically), we‚Äôll create two small helper functions:\n\nhalf_image - Halves the size of a given image.\nhalf_boxes - Divides the input coordinates of a given input box by 2.\n\nThese functions aren‚Äôt 100% necessary in our workflow.\nThey‚Äôre just to make the images slightly smaller so they fit better in the notebook.\n\nimport PIL\n\ndef half_image(image: PIL.Image) -&gt; PIL.Image:\n    \"\"\"\n    Resizes a given input image by half and returns the smaller version.\n    \"\"\"\n    return image.resize(size=(image.size[0] // 2, image.size[1] // 2))\n\ndef half_boxes(boxes):\n    \"\"\"\n    Halves an array/tensor of input boxes and returns them. Necessary for plotting them on a half-sized image.\n\n    For example:\n\n    boxes = [100, 100, 100, 100]\n    half_boxes = half_boxes(boxes)\n    print(half_boxes)\n\n    &gt;&gt;&gt; [50, 50, 50, 50]\n    \"\"\"\n    if isinstance(boxes, list):\n        # If boxes are list of lists, then we have multiple boxes\n        for box in boxes:\n            if isinstance(box, list):\n                return [[coordinate // 2 for coordinate in box] for box in boxes]\n            else:\n                return [coordinate // 2 for coordinate in boxes]         \n    \n    if isinstance(boxes, np.ndarray):\n        return (boxes // 2)\n    \n    if isinstance(boxes, torch.Tensor):\n        return (boxes // 2)\n\n# Test the functions \nimage_test = dataset[\"train\"][42][\"image\"]\nimage_test_half = half_image(image_test)\nprint(f\"[INFO] Original image size: {image_test.size} | Half image size: {image_test_half.size}\")\n\nboxes_test_list = [100, 100, 100, 100]\nprint(f\"[INFO] Original boxes: {boxes_test_list} | Half boxes: {half_boxes(boxes_test_list)}\")\n\nboxes_test_torch = torch.tensor([100.0, 100.0, 100.0, 100.0])\nprint(f\"[INFO] Original boxes: {boxes_test_torch} | Half boxes: {half_boxes(boxes_test_torch)}\")\n\n[INFO] Original image size: (960, 1280) | Half image size: (480, 640)\n[INFO] Original boxes: [100, 100, 100, 100] | Half boxes: [50, 50, 50, 50]\n[INFO] Original boxes: tensor([100., 100., 100., 100.]) | Half boxes: tensor([50., 50., 50., 50.])\n\n\n\n\n4.2 Plotting boxes on a single image step by step\nTo plot an image and its assosciated boxes, we‚Äôll do the following steps:\n\nSelect a random sample from the dataset.\nExtract the \"image\" (our image is in PIL format) and \"bbox\" keys from the random sample.\n\nWe can also optionally halve the size of our image/boxes to save space. In our case, we will halve our image and boxes.\n\nTurn the box coordinates into a torch.tensor (we‚Äôll be using torchvision utilities to plot the image and boxes).\nConvert the box format from XYXY to XYWH using torchvision.ops.box_convert (we do this because torchvision.utils.draw_bounding_boxes requires XYXY format as input).\nGet a list of label names (e.g.¬†\"bin\", \"trash\", etc) assosciated with each of the boxes as well as a list of colours to match (these will be from our colour_palette).\nDraw the boxes on the target image by:\n\nTurning the image into a tensor with torchvision.transforms.functional.pil_to_tensor.\nDraw the bounding boxes on our image tensor with torchvision.utils.draw_bounding_boxes.\nTurn the image and bounding box tensors back into a PIL image with torchvision.transforms.functional.to_pil_image.\n\n\nPhew!\nA fair few steps‚Ä¶\nBut we‚Äôve got this!\n\n\n\n\n\n\nNote\n\n\n\nIf the terms XYXY or XYWH or all of the drawing methods sound a bit confusing or intimidating, don‚Äôt worry, there‚Äôs a fair bit going on here.\nWe‚Äôll cover bounding box formats, such as XYXY shortly.\nIn the meantime, if you want to learn more about different bounding box formats and how to draw them, I wrote A Guide to Bounding Box Formats and How to Draw Them which you might find helpful.\n\n\n\n# Plotting a bounding box on a single image\nimport random\n\nimport torch\n\nfrom torchvision.ops import box_convert\nfrom torchvision.utils import draw_bounding_boxes\n\nfrom torchvision.transforms.functional import pil_to_tensor, to_pil_image \n\n# 1. Select a random sample from our dataset\nrandom_index = random.randint(0, len(dataset[\"train\"]))\nprint(f\"[INFO] Showing training sample from index: {random_index}\")\nrandom_sample = dataset[\"train\"][random_index]\n\n# 2. Get image and boxes from random sample\nrandom_sample_image = random_sample[\"image\"]\nrandom_sample_boxes = random_sample[\"annotations\"][\"bbox\"]\n\n# Optional: Half the image and boxes for space saving (all of the following code will work with/without half size images)\nhalf_random_sample_image = half_image(random_sample_image)\nhalf_random_sample_boxes = half_boxes(random_sample_boxes)\n\n# 3. Turn box coordinates in a tensor\nboxes_xywh = torch.tensor(half_random_sample_boxes)\nprint(f\"Boxes in XYWH format: {boxes_xywh}\")\n\n# 4. Convert boxes from XYWH -&gt; XYXY \n# torchvision.utils.draw_bounding_boxes requires input boxes in XYXY format (X_min, y_min, X_max, y_max)\nboxes_xyxy = box_convert(boxes=boxes_xywh,\n                         in_fmt=\"xywh\",\n                         out_fmt=\"xyxy\")\nprint(f\"Boxes XYXY: {boxes_xyxy}\")\n\n# 5. Get label names of target boxes and colours to match\nrandom_sample_label_names = [categories.int2str(x) for x in random_sample[\"annotations\"][\"category_id\"]]\nrandom_sample_colours = [colour_palette[label_name] for label_name in random_sample_label_names]\nprint(f\"Label names: {random_sample_label_names}\")\nprint(f\"Colour codes: {random_sample_colours}\")\n\n# 6. Draw the boxes on the image as a tensor and then turn it into a PIL image\nto_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=half_random_sample_image),\n        boxes=boxes_xyxy,\n        colors=random_sample_colours,\n        labels=random_sample_label_names,\n        width=2,\n        label_colors=random_sample_colours\n    )\n)\n\n[INFO] Showing training sample from index: 264\nBoxes in XYWH format: tensor([[ 64., 256., 308., 304.],\n        [267., 344.,  92., 121.],\n        [149., 256., 175., 186.]])\nBoxes XYXY: tensor([[ 64., 256., 372., 560.],\n        [267., 344., 359., 465.],\n        [149., 256., 324., 442.]])\nLabel names: ['bin', 'hand', 'trash']\nColour codes: [(0, 0, 224), (148, 0, 211), (0, 255, 0)]\n\n\n\n\n\n\n\n\n\nOutstanding!\nOur first official bounding boxes plotted on an image!\nNow the idea of Trashify üöÆ is coming to life.\nDepending on the random sample you‚Äôre looking at, you should see some combination of ['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm'].\nOur goal will be to build an object detection model to replicate these boxes on a given image.\n\n\n\n\n\n\nGetting familiar with a dataset: viewing 100 random samples\n\n\n\nWhenever working with a new dataset, I find it good practice to view 100+ random samples of the data.\nIn our case, this would mean viewing 100 random images with their bounding boxes drawn on them.\nDoing so starts to build your own intuition of the data.\nUsing this intuition, along with evaluation metrics, you can start to get a better idea of how your model might be performing later on.\nKeep this in mind for any new dataset or problem space you‚Äôre working on.\nStart by looking at 100+ random samples.\nAnd yes, generally more is better.\nSo you can practice by running the code cell above a number of times to see the different kinds of images and boxes in the dataset.\nCan you think of any scenarios which the dataset might be missing?",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#different-bounding-box-formats",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#different-bounding-box-formats",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "5 Different bounding box formats",
    "text": "5 Different bounding box formats\nWhen drawing our bounding box, we discussed the terms XYXY and XYWH.\nWell, we didn‚Äôt really discuss these at all‚Ä¶\nBut that‚Äôs why we‚Äôre here.\nOne of the most confusing things in the world of object detection is the different formats bounding boxes come in.\nAre your boxes in XYXY, XYWH or CXCYWH?\nAre they in absolute format?\nOr normalized format?\nPerhaps a table will help us.\nThe following table contains a non-exhaustive list of some of the most common bounding box formats you‚Äôll come across in the wild.\n\n\n\nTable¬†1: Different bounding box formats\n\n\n\n\n\nBox format\nDescription\nAbsolute Example\nNormalized Example\nSource\n\n\n\n\nXYXY\nDescribes the top left corner coordinates (x1, y1) as well as the bottom right corner coordinates of a box.  Also referred to as:  [x1, y1, x2, y2]  or  [x_min, y_min, x_max, y_max]\n[8.9, 275.3, 867.5, 964.0]\n[0.009, 0.215, 0.904, 0.753]\nPASCAL VOC Dataset uses the absolute version of this format, torchvision.utils.draw_bounding_boxes defaults to the absolute version of this format.\n\n\nXYWH\nDescribes the top left corner coordinates (x1, y1) as well as the width (box_width) and height (box_height) of the target box. The bottom right corners (x2, y2) are found by adding the width and height to the top left corner coordinates (x1 + box_width, y1 + box_height).  Also referred to as:  [x1, y1, box_width, box_height]  or  [x_min, y_min, box_width, box_height]\n[8.9, 275.3, 858.6, 688.7]\n[0.009, 0.215, 0.894, 0.538]\nThe COCO (Common Objects in Context) dataset uses the absolute version of this format, see the section under ‚Äúbbox‚Äù.\n\n\nCXCYWH\nDescribes the center coordinates of the bounding box (center_x, center_y) as well as the width (box_width) and height (box_height) of the target box.  Also referred to as:  [center_x, center_y, box_width, box_height]\n[438.2, 619.65, 858.6, 688.7]\n[0.456, 0.484, 0.894, 0.538]\nNormalized version introduced in the YOLOv3 (You Only Look Once) paper and is used by many later forms of YOLO.\n\n\n\n\n\n\n\n5.1 Absolute or normalized format?\nIn absolute coordinate form, bounding box values are in the same format as the width and height dimensions (e.g.¬†our image is 960x1280 pixels).\nFor example in XYXY format: [\"bin\", 8.9, 275.3, 867.5, 964.0]\nAn (x1, y1) (or (x_min, y_min)) coordinate of (8.9, 275.3) means the top left corner is 8.9 pixels in on the x-axis, and 275.3 pixels down on the y-axis.\nIn normalized coordinate form, values are between [0, 1] and are proportions of the image width and height.\nFor example in XYXY format: [\"bin\", 0.009, 0.215, 0.904, 0.753]\nA normalized (x1, y1) (or (x_min, y_min)) coordinate of (0.009, 0.215) means the top left corner is 0.009 * image_width pixels in on the x-axis and 0.215 * image_height down on the y-axis.\nTo convert absolute coordinates to normalized, you can divide x-axis values by the image width and y-axis values by the image height.\n\\[\nx_{\\text{normalized}} = \\frac{x_{\\text{absolute}}}{\\text{image\\_width}} \\quad y_{\\text{normalized}} = \\frac{y_{\\text{absolute}}}{\\text{image\\_height}}\n\\]\n\n\n5.2 Which bounding box format should you use?\nThe bounding box format you use will depend on the framework, model and existing data you‚Äôre trying to use.\nFor example, the take the following frameworks:\n\nPyTorch - If you‚Äôre using PyTorch pre-trained models, for example, torchvision.models.detection.fasterrcnn_resnet50_fpn, you‚Äôll want absolute XYXY ([x1, y1, x2, y2]) format.\nHugging Face Transformers - If you‚Äôre using a Hugging Face Transformers model such as RT-DETRv2, you‚Äôll want to take note that outputs from the model can be of one type (e.g.¬†CXCYWH) but they can be post-processed into another type (e.g.¬†absolute XYXY).\nUltralytics YOLO - If you‚Äôre using a YOLO-like model such as Ultralytics YOLO, you‚Äôll want normalized CXCYWH ([center_x, center_y, width, height]) format.\nGoogle Gemini - If you‚Äôre using Google Gemini to predict bounding boxes on your images, then you‚Äôll want to pay attention to the special [y_min, x_min, y_max, x_max] (YXYX) normalized coordinates.\n\nOr if you note that someone has said their model is pre-trained on the COCO dataset, chances are the data has been formatted in XYWH format (see Table¬†1).\n\n\n\n\n\n\nNote\n\n\n\nFor more on different bounding box formats and how to draw them, see A Guide to Bounding Box Formats and How to Draw Them.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#getting-an-object-detection-model",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#getting-an-object-detection-model",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "6 Getting an object detection model",
    "text": "6 Getting an object detection model\nThere are two main ways of getting an object detection model:\n\nBuilding it yourself. For example, constructing it layer by layer, testing it and training it on your target problem.\nUsing an existing one. For example, find an existing model on a problem space similar to your own and then adapt it via transfer learning to your own task.\n\nIn our case, we‚Äôre going to focus on the latter.\nWe‚Äôll be taking a pre-trained object detection model and fine-tuning it on our Trashify üöÆ dataset so it outputs the boxes and labels we‚Äôre after.\n\n6.1 Places to get object detection models\nInstead of building your own machine learning model from scratch, it‚Äôs common practice to take an existing model that works on similar problem space to yours and then fine-tune it to your own use case.\nThere are several places to get object detection models:\n\n\n\nTable¬†2: Places to get pre-trained object detection models\n\n\n\n\n\nLocation\nDescription\n\n\n\n\nHugging Face Hub\nOne the best places on the internet to find open-source machine learning models of nearly any kind. You can find pre-trained object detection models here such as facebook/detr-resnet-50, a model from Facebook (Meta) and microsoft/conditional-detr-resnet-50, a model from Microsoft. And RT-DETRv2, the model we‚Äôre going to use as our base model. Many of the models are permissively licensed, meaning you can use them for your own projects.\n\n\nApache 2.0 Object Detection Models\nA list collected by myself of open-source, permissively licenced and high performing object detection models.\n\n\ntorchvision\nPyTorch‚Äôs built-in domain library for computer vision has several pre-trained object detection models which you can use in your own workflows.\n\n\npaperswithcode.com/task/object-detection\nWhilst not a direct place to download object detection models from, paperswithcode contains benchmarks for many machine learning tasks (including object detection) which shows the current state of the art (best performing) models and usually includes links to where to get the code.\n\n\nDetectron2\nDetectron2 is an open-source library to help with many of the tasks in detecting items in images. Inside you‚Äôll find several pre-trained and adaptable models as well as utilities such as data loaders for object detection and segmentation tasks.\n\n\nYOLO Series\nA running series of ‚ÄúYou Only Look Once‚Äù models. Usually, the higher the number, the better performing. For example, YOLOv11 by Ultralytics should outperform YOLOv10, however, this often requires testing on your own dataset. Beware of the license, it is under the AGPL-3.0 license which may cause issues in some organizations.\n\n\nmmdetection library\nAn open-source library from the OpenMMLab which contains many different open-source models as well as detection-specific utilties.\n\n\n\n\n\n\nWhen you find a pre-trained object detection model, you‚Äôll often see statements such as:\n\nConditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images).\nSource: https://huggingface.co/microsoft/conditional-detr-resnet-50\n\nThis means the model has already been trained on the COCO object detection dataset which contains 118,000 images and 80 classes such as [\"cake\", \"person\", \"skateboard\"...].\nThis is a good thing.\nIt means that the model should have a fairly good starting point when we try to adapt it to our own project.\n\n\n6.2 Downloading our model from Hugging Face\nFor our Trashify üöÆ project we‚Äôre going to be using the pre-trained object detection model PekingU/rtdetr_v2_r50vd which was originally introduced in the paper RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer.\n\n\n\n\n\n\nNote\n\n\n\nThe term ‚ÄúDETR‚Äù stands for ‚ÄúDEtection TRansformer‚Äù.\nWhere ‚ÄúTransformer‚Äù refers to the Transformer neural network architecture, specifically the Vision Transformer (or ViT) rather than the Hugging Face transformers library (quite confusing, yes).\nSo DETR means ‚Äúperforming detection with the Transformer architecture‚Äù.\nAnd the ‚ÄúRT‚Äù part stands for ‚ÄúReal Time‚Äù as in, the model is capable at performing predictions at over 30 FPS (a standard rate for video feeds).\n\n\nTo use this model, there are some helpful documentation resources we should be aware of:\n\n\n\nTable¬†3: Model documentation resources\n\n\n\n\n\nResource\nDescription\n\n\n\n\nRT-DETRv2 documentation\nContains detailed information on each of the transformers.RTDetrV2 classes.\n\n\ntransformers.RTDetrV2Config\nContains the configuration settings for our model such as number of layers and other hyperparameters.\n\n\ntransformers.RTDetrImageProcessor\nContains several preprocessing on post processing functions and settings for data going into and out of our model. Here we can set values such as size in the preprocess method which will resize our images to a certain size. We can also use the post_process_object_detection method to process the raw outputs of our model into a more usable format. Note: Even though our model is RT-DETRv2, it uses the original RT-DETR processor.\n\n\ntransformers.RTDetrV2ForObjectDetection\nThis will enable us to load the RT-DETRv2 model weights and enable to pass data through them via the forward method.\n\n\ntransformers.AutoImageProcessor\nThis will enable us to create an instance of transformers.RTDetrImageProcessor by passing the model name PekingU/rtdetr_v2_r50vd to the from_pretrained method. Hugging Face Transformers uses several Auto Classes for various problem spaces and models.\n\n\ntransformers.AutoModelForObjectDetection\nEnables us to load the model architecture and weights for the RT-DETRv2 architecture by passing the model name PekingU/rtdetr_v2_r50vd to the from_pretrained method.\n\n\n\n\n\n\nWe‚Äôll get hands-on which each of these throughout the project.\nFor now, if you‚Äôd like to read up more on each, I‚Äôd highly recommend it.\nKnowing how to navigate and read through a framework‚Äôs documentation is a very helpful skill to have.\n\n\n\n\n\n\nNote\n\n\n\nThere are other object detection models we could try on the Hugging Face Hub such as facebook/detr-resnet-50 or IDEA-Research/dab-detr-resnet-50-dc5-pat3.\nFor now, we‚Äôll stick with PekingU/rtdetr_v2_r50vd.\nIt‚Äôs easy to get stuck figuring out which model to use instead of just trying one and seeing how it goes.\nBest to get something small working with one model and try another one later as part of a series of experiments to try and improve your results.\nSpoiler: After trying several different object detection models on our problem, I found PekingU/rtdetr_v2_r50vd to be one of the best. Perhaps the newer D-FINE model might do better but I leave this for exploration.\n\n\nWe can load our model with transformers.AutoModelForObjectDetection.from_pretrained and passing in the following parameters:\n\npretrained_model_name_or_path - Our target model, which can be a local path or Hugging Face model name (e.g.¬†PekingU/rtdetr_v2_r50vd).\nlabel2id - A dictionary mapping our class names/labels to their numerical ID, this is so our model will know how many classes to output.\nid2label - A dictionary mapping numerical IDs to our class names/labels, so our model will know how many classes we‚Äôre working with and what their IDs are.\nignore_mismatched_sizes=True (default) - We‚Äôll set this to True so that our model can be instatiated with a varying number of classes compared to what it may have been trained on (e.g.¬†if our model was trained on the 91 classes from COCO, we only need 7).\n\nSee the full documentation for a full list of parameters we can use.\nLet‚Äôs create a model!\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.module\") # turn off warnings for loading the model (feel free to comment this if you want to see the warnings)\n\nfrom transformers import AutoModelForObjectDetection\n\nMODEL_NAME = \"PekingU/rtdetr_v2_r50vd\"\n\nmodel = AutoModelForObjectDetection.from_pretrained(\n    pretrained_model_name_or_path=MODEL_NAME,\n    label2id=label2id,\n    id2label=id2label,\n    # Original model was trained with a different number of output classes to ours\n    # So we'll ignore any mismatched sizes (e.g. 91 vs. 7)\n    # Try turning this to False and see what happens\n    ignore_mismatched_sizes=True, \n)\n\n# Uncomment to see full model architecture\n# model\n\nSome weights of RTDetrV2ForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_v2_r50vd and are newly initialized because the shapes did not match:\n- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([8, 256]) in the model instantiated\n- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBeautiful!\nWe‚Äôve got a model ready.\nYou might‚Äôve noticed a warning about the model needing to be trained on a down-stream task:\n\nSome weights of RTDetrV2ForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_v2_r50vd and are newly initialized because the shapes did not match: ‚Ä¶ - model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated - model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nThis is because our model has a different number of target classes (7 in total) comapred to the original model (91 in total, from the COCO dataset).\nSo in order to get this pretrained model to work on our dataset, we‚Äôll need to fine-tune it.\nYou might also notice that if you set ignore_mismatched_sizes=False, you‚Äôll get an error:\n\nRuntimeError: Error(s) in loading state_dict for Linear: size mismatch for bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([7]).\n\nThis is a similar warning to the one above.\nKeep this is mind for when you‚Äôre working with pretrained models.\nIf you are using data slightly different to what the model was trained on, you may need to alter the setup hyperparameters as well as fine-tune it on your own data.\n\n\n6.3 Inspecting our model‚Äôs layers\nWe can inspect the full model architecture by running print(model) (I‚Äôve commented this out for brevity).\nAnd if you do so, you‚Äôll see a large list of layers which combine to contribute to make the overall model.\nThe following subset of layers has been truncated for brevity.\n# Shortened version of the model architecture, print the full model to see all layers\nRTDetrV2ForObjectDetection(\n  (model): RTDetrV2Model(\n    (backbone): RTDetrV2ConvEncoder(\n      (model): RTDetrResNetBackbone(\n        (embedder): RTDetrResNetEmbeddings(\n          (embedder): Sequential(\n            (0): RTDetrResNetConvLayer(\n              (convolution): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n              (normalization): RTDetrV2FrozenBatchNorm2d()\n              (activation): ReLU()\n            )\n            ... [Many layers here] ...\n    )\n  )\n  (class_embed): ModuleList(\n    (0-5): 6 x Linear(in_features=256, out_features=7, bias=True)\n  )\n  (bbox_embed): ModuleList(\n    (0-5): 6 x RTDetrV2MLPPredictionHead(\n      (layers): ModuleList(\n        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        (2): Linear(in_features=256, out_features=4, bias=True)\n      )\n    )\n  )\n)\nIf we check out a few of our model‚Äôs layers, we can see that it is a combination of convolutional, attention, MLP (multi-layer perceptron) and linear layers.\nI‚Äôll leave exploring each of these layer types for extra-curriculum, you can see the source code for the model in the modeling_rt_detr_v2.py file on the transformers GitHub.\nFor now, think of them as progressively pattern extractors.\nWe‚Äôll feed our input image into our model and layer by layer it will manipulate the pixel values to try and extract patterns in a way so that its internal parameters matches the image to its input annotations.\nMore specifically, if we dive into the final two layer sections:\n\nclass_embed = classification head with out_features=7 (one for each of our labels, 'bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']).\nbbox_embed = regression head with out_features=4 (one for each of our bbox coordinates, e.g.¬†[center_x, center_y, width, height]).\n\n\nprint(f\"[INFO] Final classification layer: {model.class_embed}\\n\")\nprint(f\"[INFO] Final box regression layer: {model.bbox_embed}\")\n\n[INFO] Final classification layer: ModuleList(\n  (0-5): 6 x Linear(in_features=256, out_features=7, bias=True)\n)\n\n[INFO] Final box regression layer: ModuleList(\n  (0-5): 6 x RTDetrV2MLPPredictionHead(\n    (layers): ModuleList(\n      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n      (2): Linear(in_features=256, out_features=4, bias=True)\n    )\n  )\n)\n\n\nThese two layers are what are going to output the final predictions of our model in structure similar to our annotations.\nThe class_embed will output the predicted class label of a given bounding box output from bbox_predictor.\nIn essence, we are trying to get all of the pretrained patterns (also called parameters/weights & biases) of the previous layers to conform to the ideal outputs we‚Äôd like at the end.\n\n\n6.4 Counting the number of parameters in our model\nParameters are individual values which contribute to a model‚Äôs final output.\nParameters are also referred to as weights and biases.\nYou can think of these individual weights as small pushes and pulls on the input data to get it to match the input annotations.\nIf our weights were perfect, we could input an image and always get back the correct bounding boxes and class labels.\nIt‚Äôs very unlikely to ever have perfect weights (unless your dataset is very small) but we can make them quite good (and useful).\nWhen you have a good set of weights, this is known as a good representation.\nRight now, our weights have been trained on COCO, a collection of 91 different common objects.\nSo they have a fairly good representation of detecting general common objects, however, we‚Äôd like to fine-tune these weights to detect our target objects.\nImportantly, our model will not be starting from scratch when it begins to train.\nIt will instead take off from its existing knowledge of detecting common objects in images and try to adhere to our task.\nWhen it comes to parameters and weights, generally, more is better.\nMeaning the more parameters your model has, the better representation it can learn.\nFor example, ResNet50 (our computer vision backbone) has ~25 million parameters, about 100 MB in float32 precision or 50MB in float16 precision.\nWhereas a model such as Llama-3.1-405B has ~405 billion parameters, about 1.45 TB in float32 precision or 740 GB in float16 precision, about 16,000x more than ResNet50.\nHowever, as we can see having more parameters comes with the tradeoff of size and latency.\nFor each new parameter requires to be stored and it also adds an extra computation unit to your model.\nIn the case of Trashify, since we‚Äôd like our model to run on-device (e.g.¬†make predictions live on an iPhone), we‚Äôd opt for the smallest number of parameters we could get acceptable results from.\nIf performance is your number 1 criteria and size and latency don‚Äôt matter, then you‚Äôd likely opt for the model with the largest number of parameters (though always evaluate these models on your own data, larger models are generally better, not always better).\nSince our model is built using PyTorch, let‚Äôs write a small function to count the number of:\n\nTrainable parameters (parameters which will be tweaked during training)\nNon-trainable parameters (parameters which will not be tweaked during training)\nTotal parameters (trainable parameters + non-trainable parameters)\n\n\n# Count the number of parameters in the model\ndef count_parameters(model):\n    \"\"\"Takes in a PyTorch model and returns the number of parameters.\"\"\"\n    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    non_trainable_parameters = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n    total_parameters = sum(p.numel() for p in model.parameters())\n    print(f\"Total parameters: {total_parameters:,}\")\n    print(f\"Trainable parameters (will be updated): {trainable_parameters:,}\")\n    print(f\"Non-trainable parameters (will not be updated): {non_trainable_parameters:,}\")\n\ncount_parameters(model)\n\nTotal parameters: 42,741,357\nTrainable parameters (will be updated): 42,741,357\nNon-trainable parameters (will not be updated): 0\n\n\nCool!\nIt looks like our model has a total of 42,741,357 parameters, of which, most of them are trainable.\nThis means that when we fine-tune our model later on, we‚Äôll be tweaking the majority of the parameters to try and represent our data.\nIn practice, this is known as full fine-tuning, trying to fine-tune a large portion of the model to our data.\nThere are other methods for fine-tuning, such as feature extraction (where you only fine-tune the final layers of the model) and partial fine-tuning (where you fine-tune a portion of the model).\nAnd even methods such as LoRA (Low-Rank Adaptation) which fine-tunes an adaptor matrix as a compliment to the model‚Äôs parameters.\n\n\n6.5 Creating a function to build our model\nSince machine learning is very experimental, we may want to create multiple instances of our model to test various things.\nSo let‚Äôs functionize the creation of a new model with parameters for our target model name, id2label and label2id dictionaries.\n\nfrom transformers import AutoModelForObjectDetection\n\n# Setup the model\ndef create_model(pretrained_model_name_or_path: str = MODEL_NAME, \n                 label2id: dict = label2id, \n                 id2label: dict = id2label):\n    \"\"\"Creates and returns an instance of AutoModelForObjectDetection.\n    \n    Args: \n        pretrained_model_name_or_path (str): The name or path of the pretrained model to load. \n            Defaults to MODEL_NAME.\n        label2id (dict): A dictionary mapping class labels to IDs. Defaults to label2id.\n        id2label (dict): A dictionary mapping class IDs to labels. Defaults to id2label.\n    \n    Returns:\n        AutoModelForObjectDetection: A pretrained model for object detection with number of output\n            classes equivalent to len(label2id).\n    \"\"\"\n    model = AutoModelForObjectDetection.from_pretrained(\n        pretrained_model_name_or_path=pretrained_model_name_or_path,\n        label2id=label2id,\n        id2label=id2label,\n        ignore_mismatched_sizes=True, # default\n    )\n    return model\n\nPerfect!\nAnd to make sure our function works‚Ä¶\n\n# Create a new model instance\nmodel = create_model()\n# model\n\nSome weights of RTDetrV2ForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_v2_r50vd and are newly initialized because the shapes did not match:\n- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([8, 256]) in the model instantiated\n- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n\n6.6 Trying to pass a single sample through our model (part 1)\nOkay, now we‚Äôve got a model, let‚Äôs put some data through it!\nWhen we call our model, because it‚Äôs a PyTorch Module (torch.nn.Module) it will by default run the forward method.\nIn PyTorch, forward overrides the special __call__ method on functions.\nSo we can pass data into our model by running:\nmodel(input_data)\nWhich is equivalent to running:\nmodel.forward(input_data)\nTo see what happens when we call our model, let‚Äôs inspect the forward method‚Äôs docstring with model.forward?.\n\n# What happens when we call our model?\n# Note: for PyTorch modules, `forward` overrides the __call__ method, \n# so calling the model is equivalent to calling the forward method.\nmodel.forward?\n\n\n\nOutput of model.forward?\n\nSignature:\nmodel.forward(\n    pixel_values: torch.FloatTensor,\n    pixel_mask: Optional[torch.LongTensor] = None,\n    encoder_outputs: Optional[torch.FloatTensor] = None,\n    inputs_embeds: Optional[torch.FloatTensor] = None,\n    decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n    labels: Optional[List[dict]] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n    **loss_kwargs,\n) -&gt; Union[Tuple[torch.FloatTensor], transformers.models.rt_detr_v2.modeling_rt_detr_v2.RTDetrV2ObjectDetectionOutput]\nDocstring:\nThe [`RTDetrV2ForObjectDetection`] forward method, overrides the `__call__` special method.\n\n&lt;Tip&gt;\n\nAlthough the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\ninstance afterwards instead of this since the former takes care of running the pre and post processing steps while\nthe latter silently ignores them.\n\n&lt;/Tip&gt;\n\nArgs:\n    pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n        The tensors corresponding to the input images. Pixel values can be obtained using\n        [`{image_processor_class}`]. See [`{image_processor_class}.__call__`] for details ([`{processor_class}`] uses\n        [`{image_processor_class}`] for processing images).\n    pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n        Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:\n\n        - 1 for pixels that are real (i.e. **not masked**),\n        - 0 for pixels that are padding (i.e. **masked**).\n\n        [What are attention masks?](../glossary#attention-mask)\n    encoder_outputs (`torch.FloatTensor`, *optional*):\n        Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n        `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n        hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n        Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n        can choose to directly pass a flattened representation of an image.\n    decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n        Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n        embedded representation.\n    labels (`List[dict]` of len `(batch_size,)`, *optional*):\n        Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the\n        following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch\n        respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes\n        in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.\n    output_attentions (`bool`, *optional*):\n        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n        tensors for more detail.\n    output_hidden_states (`bool`, *optional*):\n        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n        more detail.\n    return_dict (`bool`, *optional*):\n        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\nReturns:\n    [`transformers.models.rt_detr_v2.modeling_rt_detr_v2.RTDetrV2ObjectDetectionOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.models.rt_detr_v2.modeling_rt_detr_v2.RTDetrV2ObjectDetectionOutput`] or a tuple of\n    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n    elements depending on the configuration ([`RTDetrV2Config`]) and inputs.\n\n    - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)) -- Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n      bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n      scale-invariant IoU loss.\n    - **loss_dict** (`Dict`, *optional*) -- A dictionary containing the individual losses. Useful for logging.\n    - **logits** (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`) -- Classification logits (including no-object) for all queries.\n    - **pred_boxes** (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`) -- Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n      values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n      possible padding). You can use [`~RTDetrV2ImageProcessor.post_process_object_detection`] to retrieve the\n      unnormalized (absolute) bounding boxes.\n    - **auxiliary_outputs** (`list[Dict]`, *optional*) -- Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n      and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n      `pred_boxes`) for each decoder layer.\n    - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`) -- Sequence of hidden-states at the output of the last layer of the decoder of the model.\n    - **intermediate_hidden_states** (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`) -- Stacked intermediate hidden states (output of each layer of the decoder).\n    - **intermediate_logits** (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, config.num_labels)`) -- Stacked intermediate logits (logits of each layer of the decoder).\n    - **intermediate_reference_points** (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`) -- Stacked intermediate reference points (reference points of each layer of the decoder).\n    - **intermediate_predicted_corners** (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`) -- Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n    - **initial_reference_points** (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`) -- Stacked initial reference points (initial reference points of each layer of the decoder).\n    - **decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n      shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n      plus the initial embedding outputs.\n    - **decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, num_queries,\n      num_queries)`. Attentions weights of the decoder, after the attention softmax, used to compute the weighted\n      average in the self-attention heads.\n    - **cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n      Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n      weighted average in the cross-attention heads.\n    - **encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) -- Sequence of hidden-states at the output of the last layer of the encoder of the model.\n    - **encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n      shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n      layer plus the initial embedding outputs.\n    - **encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n      Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n      self-attention heads.\n    - **init_reference_points** (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`) -- Initial reference points sent through the Transformer decoder.\n    - **enc_topk_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`) -- Logits of predicted bounding boxes coordinates in the encoder.\n    - **enc_topk_bboxes** (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`) -- Logits of predicted bounding boxes coordinates in the encoder.\n    - **enc_outputs_class** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`) -- Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n      picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n      foreground and background).\n    - **enc_outputs_coord_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`) -- Logits of predicted bounding boxes coordinates in the first stage.\n    - **denoising_meta_values** (`dict`) -- Extra dictionary for the denoising related values\n\nExamples:\n\n```python\n&gt;&gt;&gt; from transformers import RTDetrV2ImageProcessor, RTDetrV2ForObjectDetection\n&gt;&gt;&gt; from PIL import Image\n&gt;&gt;&gt; import requests\n&gt;&gt;&gt; import torch\n\n&gt;&gt;&gt; url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n&gt;&gt;&gt; image = Image.open(requests.get(url, stream=True).raw)\n\n&gt;&gt;&gt; image_processor = RTDetrV2ImageProcessor.from_pretrained(\"PekingU/RTDetrV2_r50vd\")\n&gt;&gt;&gt; model = RTDetrV2ForObjectDetection.from_pretrained(\"PekingU/RTDetrV2_r50vd\")\n\n&gt;&gt;&gt; # prepare image for the model\n&gt;&gt;&gt; inputs = image_processor(images=image, return_tensors=\"pt\")\n\n&gt;&gt;&gt; # forward pass\n&gt;&gt;&gt; outputs = model(**inputs)\n\n&gt;&gt;&gt; logits = outputs.logits\n&gt;&gt;&gt; list(logits.shape)\n[1, 300, 80]\n\n&gt;&gt;&gt; boxes = outputs.pred_boxes\n&gt;&gt;&gt; list(boxes.shape)\n[1, 300, 4]\n\n&gt;&gt;&gt; # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n&gt;&gt;&gt; target_sizes = torch.tensor([image.size[::-1]])\n&gt;&gt;&gt; results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[\n...     0\n... ]\n\n&gt;&gt;&gt; for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(\n...         f\"Detected {model.config.id2label[label.item()]} with confidence \"\n...         f\"{round(score.item(), 3)} at location {box}\"\n...     )\nDetected sofa with confidence 0.97 at location [0.14, 0.38, 640.13, 476.21]\nDetected cat with confidence 0.96 at location [343.38, 24.28, 640.14, 371.5]\nDetected cat with confidence 0.958 at location [13.23, 54.18, 318.98, 472.22]\nDetected remote with confidence 0.951 at location [40.11, 73.44, 175.96, 118.48]\nDetected remote with confidence 0.924 at location [333.73, 76.58, 369.97, 186.99]\nFile: ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py Type: method\n\n\nRunning model.forward? we can see that our model wants to take in pixel_values as well as a pixel_mask as arguments.\nWhat happens if we try to pass in a single image from our random_sample?\nLet‚Äôs try!\n\n\n\n\n\n\nNote\n\n\n\nIt‚Äôs good practice to try and pass a single sample through your model as soon as possible to see what happens.\nIf we‚Äôre lucky, it‚Äôll work.\nIf we‚Äôre really lucky, we‚Äôll get an error message saying why it didn‚Äôt work (this is usually the case because rarely does raw data flow through a model without being preprocessed first).\n\n\nWe‚Äôll do so by setting pixel_values to our random_sample[\"image\"] and pixel_mask=None.\n\n# Do a single forward pass with the model\nrandom_sample_outputs = model(pixel_values=random_sample[\"image\"],\n                              pixel_mask=None)\nrandom_sample_outputs\n\n\n\nOutput of random_sample_outputs\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[21], line 3\n      1 #| output: false\n      2 # Do a single forward pass with the model\n----&gt; 3 random_sample_outputs = model(pixel_values=random_sample[\"image\"],\n      4                               pixel_mask=None)\n      5 random_sample_outputs\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1751, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1749     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1750 else:\n-&gt; 1751     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1762, in Module._call_impl(self, *args, **kwargs)\n   1757 # If we don't have any hooks, we want to skip the rest of the logic in\n   1758 # this function, and just call forward.\n   1759 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1760         or _global_backward_pre_hooks or _global_backward_hooks\n   1761         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1762     return forward_call(*args, **kwargs)\n   1764 result = None\n   1765 called_always_called_hooks = set()\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py:1967, in RTDetrV2ForObjectDetection.forward(self, pixel_values, pixel_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **loss_kwargs)\n   1962 output_hidden_states = (\n   1963     output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n   1964 )\n   1965 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-&gt; 1967 outputs = self.model(\n   1968     pixel_values,\n   1969     pixel_mask=pixel_mask,\n   1970     encoder_outputs=encoder_outputs,\n   1971     inputs_embeds=inputs_embeds,\n   1972     decoder_inputs_embeds=decoder_inputs_embeds,\n   1973     labels=labels,\n   1974     output_attentions=output_attentions,\n   1975     output_hidden_states=output_hidden_states,\n   1976     return_dict=return_dict,\n   1977 )\n   1979 denoising_meta_values = (\n   1980     outputs.denoising_meta_values if return_dict else outputs[-1] if self.training else None\n   1981 )\n   1983 outputs_class = outputs.intermediate_logits if return_dict else outputs[2]\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1751, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1749     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1750 else:\n-&gt; 1751     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1762, in Module._call_impl(self, *args, **kwargs)\n   1757 # If we don't have any hooks, we want to skip the rest of the logic in\n   1758 # this function, and just call forward.\n   1759 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1760         or _global_backward_pre_hooks or _global_backward_hooks\n   1761         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1762     return forward_call(*args, **kwargs)\n   1764 result = None\n   1765 called_always_called_hooks = set()\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py:1658, in RTDetrV2Model.forward(self, pixel_values, pixel_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\n   1653 output_hidden_states = (\n   1654     output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n   1655 )\n   1656 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-&gt; 1658 batch_size, num_channels, height, width = pixel_values.shape\n   1659 device = pixel_values.device\n   1661 if pixel_mask is None:\n\nAttributeError: 'Image' object has no attribute 'shape'\n\nOh no!‚Ä¶ I mean‚Ä¶ Oh, yes!\nWe get an error:\n\nAttributeError: ‚ÄòImage‚Äô object has no attribute ‚Äòshape‚Äô\n\nHmmm‚Ä¶ it seems we‚Äôve tried to pass a PIL.Image to our model rather than a torch.FloatTensor of shape (batch_size, num_channels, height, width).\nIt looks like our input data might require some preprocessing before we can pass it to our model.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#aside-processor-to-model-pattern",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#aside-processor-to-model-pattern",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "7 Aside: Processor to Model Pattern",
    "text": "7 Aside: Processor to Model Pattern\nMany Hugging Face data loading and modelling workflows as well as machine learning workflows in general follow the pattern of:\n\nData -&gt; Preprocessor -&gt; Model\n\n\n\n\nWorkflow we‚Äôll follow to create our own custom object detection model. We‚Äôll start with images labelled with boxes of trash, bin and hand (and other classes), preprocess the process to be ready for use with a model and then we‚Äôll train the model on our preprocessed custom data.\n\n\nMeaning, the raw input data gets preprocessed or transformed in some way before being passed to a model.\nPreprocessors and models are often loaded with an Auto Class.\nAn Auto Class pairs a preprocessor and model based on their model name or key.\nFor example:\nfrom transformers import AutoProcessor, AutoModel\n\n# Load raw data\nraw_data = load_data()\n\n# Define target model name\nMODEL_NAME = \"...\"\n\n# Load preprocessor and model (these two are often paired)\npreprocessor = AutoProcessor.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\n\n# Preprocess data\npreprocessed_data = preprocessor.preprocess(raw_data)\n\n# Pass preprocessed data to model\noutput = model(preprocessed_data)\nThis is the same for our Trashify üöÆ project.\nWe‚Äôve got our raw data (images and bounding boxes), however, they need to be preprocessed in order for our model to be able to handle them.\nPreviously we tried to pass a sample of raw data to our model and this errored.\nWe can fix this by first preprocessing our raw data with our model‚Äôs pair preprocessor and then passing to our model again.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#loading-our-models-processor",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#loading-our-models-processor",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "8 Loading our model‚Äôs processor",
    "text": "8 Loading our model‚Äôs processor\nTime to get our raw data ready for our model!\nTo begin, let‚Äôs load our model‚Äôs processor.\nWe‚Äôll use this to prepare our input images for the model.\nTo do so, we‚Äôll use transformers.AutoImageProcessor and pass our target model name to the from_pretrained method.\nWe can set use_fast=True so the fast version of the processor is loaded (see more in the docs under transformers.RTDetrImageProcessorFast).\n\nfrom transformers import AutoImageProcessor\n\nMODEL_NAME = \"PekingU/rtdetr_v2_r50vd\"\n# MODEL_NAME = \"facebook/detr-resnet-50\" # Could also use this model as an another experiment\n\n# Load the image processor\nimage_processor = AutoImageProcessor.from_pretrained(pretrained_model_name_or_path=MODEL_NAME,\n                                                     use_fast=True) # load the fast version of the processor\n\n# Check out the image processor\nimage_processor\n\nRTDetrImageProcessorFast {\n  \"crop_size\": null,\n  \"data_format\": \"channels_first\",\n  \"default_to_square\": false,\n  \"device\": null,\n  \"disable_grouping\": null,\n  \"do_center_crop\": null,\n  \"do_convert_annotations\": true,\n  \"do_convert_rgb\": null,\n  \"do_normalize\": false,\n  \"do_pad\": false,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"format\": \"coco_detection\",\n  \"image_mean\": [\n    0.485,\n    0.456,\n    0.406\n  ],\n  \"image_processor_type\": \"RTDetrImageProcessorFast\",\n  \"image_std\": [\n    0.229,\n    0.224,\n    0.225\n  ],\n  \"input_data_format\": null,\n  \"pad_size\": null,\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"return_segmentation_masks\": null,\n  \"return_tensors\": null,\n  \"size\": {\n    \"height\": 640,\n    \"width\": 640\n  }\n}\n\n\nOk, a few things going on here.\nThese parameters will transform our input images before we pass them to our model.\nOne of the first things to see is the image_processor is expecting our bounding boxes to be in COCO format (see the \"format\": coco_detection field, this is the default).\nWe‚Äôll see what this looks like later on but our processor wants this format because that‚Äôs the format our model has been trained on (it‚Äôs generally best practice to input data to a model in the same way its been trained on, otherwise you‚Äôll likely get poor results).\nAnother thing to notice is that our input images will be resized to the values of the size parameter.\nIn our case, it‚Äôs currently:\n\"size\": {\n    \"height\": 640,\n    \"width\": 640\n}\nWhich means that input image will get resized to be 640x640 pixels (height x width).\nWe‚Äôll keep these dimensions but we‚Äôll update it to use \"longest_edge\": 640 and \"shortest_edge: 640\" (this will maintain aspect ratio).\nYou can read more about what each of these does in the transformers.RTDetrImageProcessor documentation.\nLet‚Äôs update our instance of transformers.RTDetrImageProcessor with a few custom parameters:\n\ndo_convert_annotations=True - This is the default and it will convert our boxes to the format CXCYWH or (center_x, center_y, width, height) (see Table¬†1) in the range [0, 1].\nsize - We‚Äôll update the size dictionary so all of our images have \"longest_edge\": 640 and \"shortest_edge: 640\". We‚Äôll use a value of 640 which is a common size in world of object detection. But there are also other sizes such as 300x300, 480x480, 512x512, 800x800 and more.\ndo_pad=True - We‚Äôll make sure to pad our images with 0 pixels so that all images in the same batch have the same size. For example if an image has the dimensions image_1 = (640, 480) but the largest image in the batch is (640, 640), image_1 will be padded with 0 to have its dimensions equal (640, 640). This will only happen during batch processing rather than individual image processing. See the do_pad parameter documentation for more.\n\n\n\n\n\n\n\nNote\n\n\n\nDepending on what task you‚Äôre working on, you might want to tweak the image resolution you‚Äôre working with.\nFor example, I like this quote from Lucas Beyer, a former research scientist at DeepMind and current researcher at Meta:\n\nMy conservative claim is that you can always stretch to a square, and for:\nnatural images, meaning most photos, 224px¬≤ is enough; text in photos, phone screens, diagrams and charts, 448px¬≤ is enough; desktop screens and single-page documents, 896px¬≤ is enough.\n\nTypically, in the case of object detection, you‚Äôll want to use a higher value.\nBut this is another thing that is open to experimentation.\n\n\n\n# Set image size\nIMAGE_SIZE = 640 # we could try other sizes here: 300x300, 480x480, 512x512, 640x640, 800x800 (best to experiment and see which works best)\n\n# Create a new instance of the image processor with the desired image size\nimage_processor = AutoImageProcessor.from_pretrained(\n    pretrained_model_name_or_path=MODEL_NAME,\n    use_fast=True, # use the fast preprocessor\n    format=\"coco_detection\", # this is the default\n    do_convert_annotations=True, # defaults to True, converts boxes to (center_x, center_y, width, height) in range [0, 1]\n    size={\"shortest_edge\": IMAGE_SIZE, \n          \"longest_edge\": IMAGE_SIZE},\n    return_segmentation_masks=True,\n    do_pad=True # make sure all images have 640x640 size thanks to padding\n)\n\n# Optional: View the docstring of our image_processor.preprocess function\n# image_processor.preprocess?\n\n# Check out our new image processor size\nimage_processor.size\n\n{'shortest_edge': 640, 'longest_edge': 640}\n\n\nBeautiful!\nNow our images will be resized to a square of size 640x640 when we pass them to our model.\nHow about we try to preprocess our random_sample?\nTo do so, we can pass its \"image\" key and \"annotations\" key to our image_processor‚Äôs preprocess method (we can also just called image_processor directly as it will call preprocess via the __call__ method).\nLet‚Äôs try!\n\nimage_processor\n\nRTDetrImageProcessorFast {\n  \"crop_size\": null,\n  \"data_format\": \"channels_first\",\n  \"default_to_square\": false,\n  \"device\": null,\n  \"disable_grouping\": null,\n  \"do_center_crop\": null,\n  \"do_convert_annotations\": true,\n  \"do_convert_rgb\": null,\n  \"do_normalize\": false,\n  \"do_pad\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"format\": \"coco_detection\",\n  \"image_mean\": [\n    0.485,\n    0.456,\n    0.406\n  ],\n  \"image_processor_type\": \"RTDetrImageProcessorFast\",\n  \"image_std\": [\n    0.229,\n    0.224,\n    0.225\n  ],\n  \"input_data_format\": null,\n  \"pad_size\": null,\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"return_segmentation_masks\": true,\n  \"return_tensors\": null,\n  \"size\": {\n    \"longest_edge\": 640,\n    \"shortest_edge\": 640\n  }\n}\n\n\n\n# Try to process a single image and annotation pair (spoiler: this will error)\nrandom_sample_preprocessed = image_processor.preprocess(images=random_sample[\"image\"],\n                                                        annotations=random_sample[\"annotations\"])\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[22], line 2\n      1 # Try to process a single image and annotation pair (spoiler: this will error)\n----&gt; 2 random_sample_preprocessed = image_processor.preprocess(images=random_sample[\"image\"],\n      3                                                         annotations=random_sample[\"annotations\"])\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr/image_processing_rt_detr_fast.py:403, in RTDetrImageProcessorFast.preprocess(self, images, annotations, masks_path, **kwargs)\n    380 @auto_docstring\n    381 def preprocess(\n    382     self,\n   (...)\n    386     **kwargs: Unpack[RTDetrFastImageProcessorKwargs],\n    387 ) -&gt; BatchFeature:\n    388     r\"\"\"\n    389     annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n    390         List of annotations associated with the image or batch of images. If annotation is for object\n   (...)\n    401         Path to the directory containing the segmentation masks.\n    402     \"\"\"\n--&gt; 403     return super().preprocess(images, annotations, masks_path, **kwargs)\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/image_processing_utils_fast.py:654, in BaseImageProcessorFast.preprocess(self, images, *args, **kwargs)\n    651 kwargs.pop(\"default_to_square\")\n    652 kwargs.pop(\"data_format\")\n--&gt; 654 return self._preprocess(images, *args, **kwargs)\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr/image_processing_rt_detr_fast.py:440, in RTDetrImageProcessorFast._preprocess(self, images, annotations, masks_path, return_segmentation_masks, do_resize, size, interpolation, do_rescale, rescale_factor, do_normalize, do_convert_annotations, image_mean, image_std, do_pad, pad_size, format, return_tensors, **kwargs)\n    438 format = AnnotationFormat(format)\n    439 if annotations is not None:\n--&gt; 440     validate_annotations(format, SUPPORTED_ANNOTATION_FORMATS, annotations)\n    442 data = {}\n    443 processed_images = []\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/image_utils.py:919, in validate_annotations(annotation_format, supported_annotation_formats, annotations)\n    917 if annotation_format is AnnotationFormat.COCO_DETECTION:\n    918     if not valid_coco_detection_annotations(annotations):\n--&gt; 919         raise ValueError(\n    920             \"Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts \"\n    921             \"(batch of images) with the following keys: `image_id` and `annotations`, with the latter \"\n    922             \"being a list of annotations in the COCO format.\"\n    923         )\n    925 if annotation_format is AnnotationFormat.COCO_PANOPTIC:\n    926     if not valid_coco_panoptic_annotations(annotations):\n\nValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: `image_id` and `annotations`, with the latter being a list of annotations in the COCO format.\n\n\n\nOh no!\nWe get an error:\n\nValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: image_id and annotations, with the latter being a list of annotations in the COCO format.\n\n\n8.1 Preprocessing a single image\nOkay so it turns out that our annotations aren‚Äôt in the format that the preprocess method was expecting.\nSince our pre-trained model was trained on the COCO dataset, the preprocess method expects input data to be in line with the COCO format.\nWe can fix this later on by adjusting our annotations.\nHow about we try to preprocess just a single image instead?\n\n# Preprocess our target sample\nrandom_sample_preprocessed_image_only = image_processor(images=random_sample[\"image\"],\n                                                        annotations=None, # no annotations this time \n                                                        masks_path=None, # no masks inputs\n                                                        return_tensors=\"pt\") # return as PyTorch tensors\n\n# Uncomment to see the full output\n# print(random_sample_preprocessed_image_only)\n\n# Print out the keys of the preprocessed image\nprint(random_sample_preprocessed_image_only.keys())\n\nKeysView({'pixel_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]]]), 'pixel_values': tensor([[[[0.9176, 0.9137, 0.9176,  ..., 0.7020, 0.7020, 0.7020],\n          [0.9098, 0.8941, 0.8863,  ..., 0.7098, 0.7098, 0.7137],\n          [0.8941, 0.8745, 0.8627,  ..., 0.6980, 0.6980, 0.6980],\n          ...,\n          [0.4039, 0.3608, 0.3882,  ..., 0.8235, 0.8392, 0.8549],\n          [0.3725, 0.4000, 0.4902,  ..., 0.8275, 0.8392, 0.8510],\n          [0.3686, 0.4667, 0.5176,  ..., 0.8275, 0.8314, 0.8353]],\n\n         [[0.9647, 0.9608, 0.9608,  ..., 0.7882, 0.7882, 0.7882],\n          [0.9569, 0.9412, 0.9294,  ..., 0.7961, 0.8000, 0.8000],\n          [0.9451, 0.9294, 0.9176,  ..., 0.7961, 0.7961, 0.7922],\n          ...,\n          [0.4196, 0.3765, 0.4000,  ..., 0.6157, 0.6314, 0.6431],\n          [0.3882, 0.4157, 0.4980,  ..., 0.6196, 0.6314, 0.6431],\n          [0.3843, 0.4784, 0.5255,  ..., 0.6196, 0.6235, 0.6275]],\n\n         [[0.9490, 0.9569, 0.9647,  ..., 0.8706, 0.8706, 0.8706],\n          [0.9490, 0.9412, 0.9412,  ..., 0.8784, 0.8824, 0.8824],\n          [0.9373, 0.9333, 0.9294,  ..., 0.8745, 0.8745, 0.8745],\n          ...,\n          [0.4157, 0.3725, 0.3961,  ..., 0.5451, 0.5608, 0.5725],\n          [0.3843, 0.4118, 0.4941,  ..., 0.5490, 0.5608, 0.5725],\n          [0.3804, 0.4745, 0.5216,  ..., 0.5490, 0.5529, 0.5569]]]])})\n\n\nNice! It looks like the preprocess method works on a single image.\nAnd it seems like we get a dictionary output with the following keys:\n\npixel_values - the processed pixel values of the input image.\n(Optional) pixel_mask - a mask multiplier for the pixel values as to whether they should be paid attention to or not (a value of 0 means the pixel value should be ignored by the model and a value of 1 means the pixel value should be paid attention to by the model).\n\nBeautiful!\nNow how about we inspect our processed image‚Äôs shape?\n\n# Uncomment to inspect all preprocessed pixel values\n# print(random_sample_preprocessed_image_only[\"pixel_values\"][0])\n\nprint(f\"[INFO] Original image shape: {random_sample['image'].size} -&gt; [width, height]\")\nprint(f\"[INFO] Preprocessed image shape: {random_sample_preprocessed_image_only['pixel_values'].shape} -&gt; [batch_size, colour_channles, height, width]\")\n\n[INFO] Original image shape: (960, 1280) -&gt; [width, height]\n[INFO] Preprocessed image shape: torch.Size([1, 3, 640, 480]) -&gt; [batch_size, colour_channles, height, width]\n\n\nOk wonderful, it looks like our image has been downsized to [3, 640, 480] (1 item in the batch, 3 colour channels, 640 pixels high, 480 pixels wide).\nThis is down from its original size of [960, 1280] (1280 pixels high, 960 pixels wide).\n\n\n\n\n\n\nNote\n\n\n\nThe order of image dimensions can differ between libraries and frameworks.\nFor example, image tensors in PyTorch typically follow the format [colour_channels, height, width] whereas in TensorFlow they follow [height, width, colour_channels].\nAnd in PIL, the format is [width, height].\nAs you can imagine, this can get confusing.\nHowever, with some practice, you‚Äôll be able to decipher which is which.\nAnd if your images and bounding boxes start looking strange, perhaps checking the image dimension and format can help.\n\n\n\n\n8.2 Trying to pass a single sample through our model (part 2)\nThis is exciting!\nWe‚Äôve processed an image into the format our model is expecting.\nHow about we try another forward by calling model.forward(pixel_values, pixel_mask)?\nWhich is the same as calling model(pixel_values, pixel_mask).\n\n# Do a single forward pass with the model\nrandom_sample_outputs = model(\n    pixel_values=random_sample_preprocessed_image_only[\"pixel_values\"], # model expects input [batch_size, color_channels, height, width]\n    # pixel_mask=random_sample_preprocessed_image_only[\"pixel_mask\"], # some object detection models expect masks\n)\n\n# Inspect the outputs\n# random_sample_outputs # uncomment to see full outputs\n\n\n\nFull output of random_sample_outputs:\n\nRTDetrV2ObjectDetectionOutput(loss=None, loss_dict=None, logits=tensor([[[-2.4103, -3.2165, -2.1090,  ..., -2.4911, -1.6609, -2.6324],\n         [-2.5400, -3.6887, -1.6646,  ..., -2.1060, -2.1198, -2.6746],\n         [-2.4400, -3.6842, -1.4782,  ..., -2.0494, -2.0977, -2.6446],\n         ...,\n         [-2.7582, -3.9171, -1.3343,  ..., -2.2933, -2.4012, -2.4486],\n         [-2.5491, -3.2022, -1.5356,  ..., -2.5856, -2.1178, -2.7912],\n         [-2.6526, -3.0643, -1.8657,  ..., -2.4201, -2.7698, -2.2681]]],\n       grad_fn=&lt;SelectBackward0&gt;), pred_boxes=tensor([[[0.5707, 0.2961, 0.2293, 0.0371],\n         [0.3631, 0.4809, 0.4610, 0.1908],\n         [0.4239, 0.4808, 0.5838, 0.2076],\n         ...,\n         [0.3488, 0.4558, 0.3377, 0.1503],\n         [0.3869, 0.5929, 0.3553, 0.3231],\n         [0.6407, 0.7314, 0.1552, 0.1663]]], grad_fn=&lt;SelectBackward0&gt;), auxiliary_outputs=None, last_hidden_state=tensor([[[ 4.8664e-01,  8.1942e-01, -9.4307e-04,  ...,  2.0758e-01,\n          -1.8689e-01, -5.1746e-01],\n         [ 2.5495e-01,  9.9750e-01,  4.1035e-01,  ..., -2.5949e-02,\n          -4.6852e-02, -7.8246e-01],\n         [ 1.0380e-01,  8.7084e-01,  4.6921e-01,  ..., -1.8778e-01,\n           8.0271e-02, -6.9041e-01],\n         ...,\n         [ 1.2882e-01,  9.6441e-01,  2.7554e-01,  ..., -4.3895e-01,\n           1.3827e-01, -7.4690e-01],\n         [ 2.7624e-01,  7.5101e-01,  2.8540e-01,  ...,  1.8248e-01,\n           8.2285e-02, -5.2314e-01],\n         [ 2.9966e-01,  8.5921e-01, -6.3817e-02,  ...,  2.6486e-01,\n           8.5958e-02, -4.2009e-01]]], grad_fn=&lt;NativeLayerNormBackward0&gt;), intermediate_hidden_states=tensor([[[[-5.3139e-02,  5.8580e-01, -9.4774e-02,  ..., -1.0139e-01,\n            5.4970e-01,  7.9870e-01],\n          [-4.3604e-01,  4.7587e-01,  4.2217e-01,  ...,  2.4720e-01,\n            4.7449e-01,  4.3058e-01],\n          [-3.8533e-01,  3.1024e-01,  2.9360e-01,  ...,  3.8614e-01,\n            2.5190e-01,  5.6520e-01],\n          ...,\n          [-2.1117e-01,  2.5501e-01,  5.4156e-01,  ...,  4.3788e-01,\n            1.0951e-01,  3.3779e-01],\n          [-8.4637e-03,  2.5538e-01, -5.3647e-01,  ...,  1.1439e-01,\n           -1.5487e-01,  8.4961e-01],\n          [-6.5541e-01,  6.7535e-01, -1.4167e-01,  ...,  3.8774e-01,\n            1.6148e-01,  9.3250e-01]],\n\n         [[ 1.2810e-01,  8.7219e-01,  3.6511e-01,  ...,  2.4804e-01,\n           -5.8754e-01,  1.3489e-01],\n          [ 2.8370e-01,  1.5663e+00,  3.4950e-03,  ..., -2.2311e-01,\n            2.1730e-01, -3.5775e-01],\n          [ 2.2267e-01,  1.1197e+00,  3.1473e-02,  ..., -2.2521e-01,\n           -1.8248e-01, -2.1684e-01],\n          ...,\n          [ 2.1268e-01,  1.3836e+00,  3.6696e-01,  ..., -1.6308e-01,\n           -1.3671e-01, -5.9738e-01],\n          [ 8.2533e-01,  1.1259e+00, -9.0579e-02,  ..., -3.6533e-01,\n           -1.3390e-02, -7.2271e-01],\n          [ 5.1040e-01,  9.4933e-01, -1.5047e-01,  ..., -4.1083e-02,\n            3.6723e-01,  5.1494e-02]],\n\n         [[ 4.0047e-01,  5.0443e-01,  1.1916e-01,  ...,  2.1427e-01,\n           -4.9870e-01, -5.9084e-02],\n          [-5.3964e-01,  1.4135e+00,  3.8025e-01,  ..., -1.3144e-01,\n           -7.2814e-01, -7.3661e-01],\n          [-5.8102e-01,  1.2173e+00,  5.9914e-01,  ...,  1.2107e-01,\n           -8.4583e-01, -4.5118e-02],\n          ...,\n          [-4.3799e-01,  1.8912e+00,  6.2712e-01,  ..., -4.0048e-01,\n           -9.9042e-01, -6.5335e-01],\n          [-2.6272e-02,  9.8732e-01,  2.4686e-01,  ...,  5.3733e-01,\n           -6.8889e-01, -1.8957e-01],\n          [ 7.4871e-01,  1.0935e+00,  2.5242e-02,  ...,  2.0705e-01,\n           -4.3149e-01,  1.6533e-01]],\n\n         [[-9.6548e-02,  6.1364e-02, -3.5741e-01,  ...,  5.7603e-01,\n            1.6279e-01, -7.4688e-02],\n          [-2.9921e-01,  5.0191e-01,  3.2028e-01,  ..., -8.5623e-02,\n           -1.5414e-02, -8.6969e-01],\n          [-4.2423e-01,  2.0131e-01,  3.0605e-01,  ..., -9.9301e-02,\n           -9.4032e-03, -3.9313e-01],\n          ...,\n          [-3.3691e-01,  5.0171e-01,  8.0514e-02,  ..., -6.5877e-02,\n           -1.8204e-03, -1.4205e-01],\n          [ 3.2399e-01, -5.1599e-03, -1.0354e-01,  ...,  2.7421e-01,\n            2.0394e-01, -5.6927e-01],\n          [ 8.0265e-01,  1.9461e-01, -4.2067e-01,  ...,  2.3415e-02,\n            6.6626e-01, -4.5957e-01]],\n\n         [[ 1.4321e-01,  1.7849e-01, -6.8985e-01,  ...,  6.2630e-01,\n            1.2153e-01, -5.3756e-01],\n          [-3.5460e-01,  6.1744e-01,  7.0757e-02,  ...,  3.0924e-02,\n            7.0767e-02, -1.1299e+00],\n          [-3.5211e-01,  5.7647e-01,  5.0576e-01,  ..., -1.6950e-01,\n            1.4924e-01, -6.5683e-01],\n          ...,\n          [ 4.7732e-03,  4.8165e-01, -2.9804e-01,  ...,  1.2332e-01,\n            5.9123e-01, -6.4708e-01],\n          [ 1.1597e-01,  4.8908e-01, -2.6656e-01,  ...,  1.7284e-01,\n            5.8165e-01, -7.6454e-01],\n          [ 5.7671e-01,  3.1484e-01, -6.6855e-01,  ...,  3.9596e-01,\n            9.3491e-01, -3.5171e-01]],\n\n         [[ 4.8664e-01,  8.1942e-01, -9.4307e-04,  ...,  2.0758e-01,\n           -1.8689e-01, -5.1746e-01],\n          [ 2.5495e-01,  9.9750e-01,  4.1035e-01,  ..., -2.5949e-02,\n           -4.6852e-02, -7.8246e-01],\n          [ 1.0380e-01,  8.7084e-01,  4.6921e-01,  ..., -1.8778e-01,\n            8.0271e-02, -6.9041e-01],\n          ...,\n          [ 1.2882e-01,  9.6441e-01,  2.7554e-01,  ..., -4.3895e-01,\n            1.3827e-01, -7.4690e-01],\n          [ 2.7624e-01,  7.5101e-01,  2.8540e-01,  ...,  1.8248e-01,\n            8.2285e-02, -5.2314e-01],\n          [ 2.9966e-01,  8.5921e-01, -6.3817e-02,  ...,  2.6486e-01,\n            8.5958e-02, -4.2009e-01]]]], grad_fn=&lt;StackBackward0&gt;), intermediate_logits=tensor([[[[-2.7654, -1.9506, -3.2306,  ..., -1.7228, -5.0831, -3.3259],\n          [-1.6720, -2.0784, -3.3905,  ..., -1.8552, -4.7686, -2.3647],\n          [-1.6152, -1.7790, -3.3754,  ..., -1.7405, -4.9992, -2.6163],\n          ...,\n          [-1.7987, -1.3256, -3.1915,  ..., -1.8915, -4.8887, -2.5755],\n          [-1.8172, -1.1075, -3.1850,  ..., -1.5766, -4.7429, -2.9463],\n          [-2.3645, -1.5377, -3.2648,  ..., -0.9642, -3.8302, -2.9143]],\n\n         [[-2.2888, -0.3772, -3.3768,  ..., -2.0233, -1.4014, -2.1638],\n          [-2.3195,  0.1030, -2.7420,  ..., -1.5070, -1.5560, -1.8782],\n          [-2.1245, -0.0459, -2.9056,  ..., -1.8131, -2.3000, -1.5002],\n          ...,\n          [-1.6669, -0.5204, -2.4404,  ..., -1.5310, -2.6033, -1.8718],\n          [-2.1209, -0.0206, -2.9078,  ..., -2.4905, -1.4664, -1.9780],\n          [-2.8389, -0.9289, -1.7524,  ..., -1.9419, -2.0081, -2.5840]],\n\n         [[-1.4762, -1.8236, -2.0330,  ..., -1.9085, -3.0767, -1.0480],\n          [-1.3000, -1.9365, -1.8160,  ..., -3.6340, -2.9030,  0.1608],\n          [-1.8684, -1.3785, -2.5306,  ..., -3.2591, -3.6757,  0.0854],\n          ...,\n          [-2.1450, -2.0016, -1.9492,  ..., -3.1256, -2.6620,  0.3906],\n          [-1.2368, -1.5141, -2.7120,  ..., -3.1521, -3.3444,  0.1144],\n          [-1.2681, -1.5955, -1.4615,  ..., -2.9850, -2.3149,  0.0573]],\n\n         [[ 0.2567, -3.0313, -1.2494,  ..., -0.4495, -1.7004, -2.2419],\n          [-0.9745, -1.9402, -1.3785,  ..., -0.7464, -0.7797, -2.6964],\n          [-0.8782, -2.3249, -1.3109,  ..., -0.4191, -0.9425, -2.6211],\n          ...,\n          [-0.0977, -2.1923, -1.3370,  ..., -1.5180, -1.6462, -2.4973],\n          [-1.4765, -2.7589, -1.4049,  ..., -0.8494, -0.8584, -2.7061],\n          [-1.5082, -2.3043, -1.5386,  ..., -1.7372, -1.5837, -3.2329]],\n\n         [[-2.0384, -2.6995, -1.7593,  ..., -2.6025, -2.1955, -1.0745],\n          [-1.6715, -3.3748, -1.4403,  ..., -2.0172, -2.1580, -1.1449],\n          [-1.3890, -2.7188, -1.4331,  ..., -2.0791, -2.3328, -1.2735],\n          ...,\n          [-1.1386, -2.7756, -1.2096,  ..., -1.8302, -3.0670, -1.4466],\n          [-1.5671, -2.8469, -1.7781,  ..., -1.9640, -2.5537, -0.8333],\n          [-1.6719, -4.0084, -1.9040,  ..., -2.5117, -3.6465, -0.9080]],\n\n         [[-2.4103, -3.2165, -2.1090,  ..., -2.4911, -1.6609, -2.6324],\n          [-2.5400, -3.6887, -1.6646,  ..., -2.1060, -2.1198, -2.6746],\n          [-2.4400, -3.6842, -1.4782,  ..., -2.0494, -2.0977, -2.6446],\n          ...,\n          [-2.7582, -3.9171, -1.3343,  ..., -2.2933, -2.4012, -2.4486],\n          [-2.5491, -3.2022, -1.5356,  ..., -2.5856, -2.1178, -2.7912],\n          [-2.6526, -3.0643, -1.8657,  ..., -2.4201, -2.7698, -2.2681]]]],\n       grad_fn=&lt;StackBackward0&gt;), intermediate_reference_points=tensor([[[[0.4246, 0.3450, 0.6868, 0.0952],\n          [0.3522, 0.5075, 0.4007, 0.2292],\n          [0.4169, 0.5195, 0.5569, 0.2495],\n          ...,\n          [0.3323, 0.4641, 0.4027, 0.1624],\n          [0.3743, 0.5587, 0.4831, 0.2996],\n          [0.6440, 0.7286, 0.1657, 0.1577]],\n\n         [[0.4947, 0.3540, 0.8195, 0.0886],\n          [0.3510, 0.4977, 0.4081, 0.2191],\n          [0.4038, 0.5018, 0.5501, 0.2451],\n          ...,\n          [0.3365, 0.4615, 0.3876, 0.1573],\n          [0.3595, 0.5509, 0.4855, 0.3372],\n          [0.6372, 0.7305, 0.1418, 0.1217]],\n\n         [[0.5216, 0.3579, 0.8873, 0.0982],\n          [0.3686, 0.4920, 0.4201, 0.2134],\n          [0.4142, 0.4943, 0.5637, 0.2140],\n          ...,\n          [0.3453, 0.4565, 0.3535, 0.1386],\n          [0.3782, 0.5932, 0.4974, 0.4147],\n          [0.6417, 0.7370, 0.1343, 0.1304]],\n\n         [[0.5531, 0.3710, 0.7984, 0.0757],\n          [0.3646, 0.4909, 0.4443, 0.2195],\n          [0.4210, 0.4847, 0.5764, 0.2169],\n          ...,\n          [0.3475, 0.4538, 0.3316, 0.1413],\n          [0.3795, 0.6289, 0.3801, 0.4177],\n          [0.6398, 0.7362, 0.1482, 0.1549]],\n\n         [[0.5707, 0.2961, 0.2293, 0.0371],\n          [0.3631, 0.4809, 0.4610, 0.1908],\n          [0.4239, 0.4808, 0.5838, 0.2075],\n          ...,\n          [0.3488, 0.4558, 0.3377, 0.1503],\n          [0.3869, 0.5929, 0.3553, 0.3231],\n          [0.6407, 0.7314, 0.1552, 0.1663]],\n\n         [[0.5707, 0.2961, 0.2293, 0.0371],\n          [0.3631, 0.4809, 0.4610, 0.1908],\n          [0.4239, 0.4808, 0.5838, 0.2076],\n          ...,\n          [0.3488, 0.4558, 0.3377, 0.1503],\n          [0.3869, 0.5929, 0.3553, 0.3231],\n          [0.6407, 0.7314, 0.1552, 0.1663]]]], grad_fn=&lt;StackBackward0&gt;), intermediate_predicted_corners=None, initial_reference_points=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=[tensor([[[[ 1.3938e+00,  3.6899e+00,  3.9381e+00,  ...,  2.0755e+00,\n            1.8361e+00,  1.3195e+00],\n          [ 1.3440e+00,  3.2245e+00,  1.4568e+00,  ...,  1.9746e-01,\n            2.1494e-01,  2.6737e-01],\n          [ 2.7528e-01,  2.2857e-01,  1.9231e-02,  ...,  1.8635e-01,\n           -1.4560e-01, -2.7372e-02],\n          ...,\n          [-4.3640e-01,  4.9777e-01,  3.4311e-01,  ...,  7.7798e-02,\n           -2.6776e-02,  5.2855e-02],\n          [-4.1876e-01,  1.3459e+00, -2.7879e-01,  ..., -5.2242e-01,\n           -2.5501e-01,  1.2173e-01],\n          [-5.0070e-01, -4.2214e-01, -5.4279e-01,  ..., -4.6641e-01,\n           -5.0324e-01, -3.3993e-01]],\n\n         [[ 2.4725e+00,  2.0903e-01,  2.6478e+00,  ...,  3.4342e+00,\n            9.2424e-01,  5.1411e+00],\n          [-2.3204e-01, -5.1897e-02,  1.2316e+00,  ...,  1.6147e+00,\n            1.1504e-01,  2.9225e+00],\n          [ 1.6521e+00,  1.0391e+00,  7.7571e-01,  ...,  2.1104e+00,\n            2.7364e-01,  3.4205e+00],\n          ...,\n          [ 1.5187e+00,  1.9785e-02,  3.1015e-01,  ...,  1.4698e-02,\n           -2.7163e-01,  2.8728e+00],\n          [-3.2695e-01, -4.2197e-01, -3.7899e-01,  ..., -4.1741e-01,\n           -3.8424e-01,  9.2021e-02],\n          [ 1.9287e+00, -3.8777e-02,  9.3081e-01,  ...,  2.9011e-01,\n           -4.3217e-01,  1.4951e+00]],\n\n         [[ 1.2350e+00,  4.6030e-01,  2.8556e-01,  ...,  1.9445e-01,\n            1.6401e-01,  8.9029e-01],\n          [ 3.6415e-01, -2.8295e-01, -2.6529e-01,  ..., -1.7090e-01,\n            1.7011e-03, -4.6186e-01],\n          [ 7.8289e-01,  1.4218e-01,  7.1477e-01,  ...,  1.3742e-01,\n            7.8084e-01,  7.7748e-01],\n          ...,\n          [ 8.6720e-01,  1.5243e+00, -4.3439e-01,  ...,  5.5073e-01,\n            5.3333e-01, -1.0822e-01],\n          [-3.4549e-01, -4.2964e-01, -5.3637e-01,  ...,  6.4962e-01,\n            5.0146e-01, -2.1609e-01],\n          [ 8.9138e-01,  9.1170e-02, -4.0709e-01,  ...,  5.9540e-01,\n            3.3999e-01,  1.8298e-01]],\n\n         ...,\n\n         [[ 2.1335e+00,  1.8645e+00,  6.3417e-01,  ...,  3.1492e-01,\n            8.0026e-01,  1.7273e+00],\n          [ 1.4950e+00,  3.3948e+00,  1.5539e+00,  ...,  2.7506e+00,\n            1.6872e+00,  1.6127e+00],\n          [ 1.6123e+00,  2.7481e+00,  1.6800e+00,  ...,  1.5145e+00,\n            4.7902e-01,  1.3030e+00],\n          ...,\n          [ 6.9388e-01,  7.5684e-02,  4.9832e-01,  ...,  5.0049e-01,\n            3.6597e-01,  6.8521e-01],\n          [ 1.3686e+00,  6.8740e-01,  2.7520e-01,  ...,  1.1264e-01,\n            9.8072e-01,  1.2247e+00],\n          [ 2.0563e+00,  7.3219e-01,  7.6325e-01,  ...,  1.0054e-01,\n            1.2625e+00,  1.5637e+00]],\n\n         [[-5.0865e-01, -2.1889e-01, -5.0291e-01,  ..., -4.8922e-01,\n            1.0710e-01, -1.3777e-01],\n          [-2.1542e-01,  3.0400e-02, -4.8931e-01,  ..., -4.8805e-01,\n            6.0389e-01, -2.9731e-01],\n          [-4.8478e-01, -5.2478e-01, -5.2429e-01,  ..., -3.8690e-01,\n           -3.9214e-01, -5.1093e-01],\n          ...,\n          [-3.3657e-01, -3.8390e-01, -2.0802e-01,  ...,  6.8628e-02,\n           -1.4215e-01, -4.1823e-01],\n          [ 6.0165e-01,  4.8848e-01, -2.1377e-01,  ..., -4.1432e-01,\n           -4.2646e-02, -4.9816e-01],\n          [-2.2064e-01,  1.3190e-01, -3.2296e-01,  ..., -5.4226e-01,\n           -4.9149e-01, -5.2242e-01]],\n\n         [[ 6.8631e+00,  2.0115e+00,  1.0507e+00,  ...,  7.6572e-01,\n            2.8275e+00,  4.7968e+00],\n          [ 6.0376e+00,  2.6104e+00,  1.0852e+00,  ...,  8.7101e-01,\n            3.6189e+00,  5.2539e+00],\n          [ 4.4559e+00,  1.1176e+00,  6.1536e-01,  ...,  5.0730e-01,\n            1.7848e+00,  3.4097e+00],\n          ...,\n          [ 2.7294e+00,  2.6141e+00,  2.0362e-01,  ..., -4.6068e-02,\n            7.6516e-01,  7.5494e-01],\n          [ 3.6208e+00,  2.3895e+00,  1.2078e+00,  ..., -1.6173e-01,\n            1.1155e+00,  1.2592e+00],\n          [ 4.0734e+00,  2.5129e+00,  1.4954e+00,  ..., -1.4671e-01,\n            7.0241e-01,  1.2234e+00]]]], grad_fn=&lt;AddBackward0&gt;), tensor([[[[ 0.5185,  0.3819, -0.1690,  ...,  0.1612,  0.3752,  0.8158],\n          [-0.3562, -0.1270, -0.2667,  ...,  0.0401,  0.4915, -0.2151],\n          [-0.4916, -0.5365, -0.5014,  ..., -0.3831, -0.1949, -0.1640],\n          ...,\n          [-0.1386,  0.0442, -0.1045,  ..., -0.2343, -0.3740, -0.4187],\n          [ 0.3513, -0.0980,  0.0411,  ..., -0.3251, -0.0708,  0.0967],\n          [ 0.3068,  0.0513, -0.2880,  ..., -0.5223, -0.3471,  0.1323]],\n\n         [[ 5.2011,  2.3660,  1.2325,  ...,  0.8635,  1.1357,  2.8331],\n          [ 0.2637,  0.3067,  0.3858,  ...,  1.0561,  1.7242,  0.2823],\n          [ 0.9179,  0.4715,  0.1141,  ...,  1.1161,  1.5353,  0.0766],\n          ...,\n          [-0.0186, -0.2360, -0.1069,  ...,  0.6026,  1.6307,  0.4062],\n          [ 0.2480,  0.2849,  0.9233,  ...,  1.4712,  2.0401,  0.4068],\n          [ 0.9946,  0.0223,  0.7812,  ...,  1.2851,  1.5783,  1.0995]],\n\n         [[-0.1503, -0.3740,  0.1545,  ..., -0.1221, -0.3535,  0.2091],\n          [ 0.1572, -0.3622, -0.0614,  ..., -0.3433, -0.2081, -0.3249],\n          [ 0.3192, -0.1503,  0.6414,  ..., -0.3637, -0.1499, -0.2254],\n          ...,\n          [ 0.0378,  0.5036,  0.1861,  ..., -0.5171, -0.5046, -0.5475],\n          [-0.3393, -0.4130, -0.1570,  ..., -0.3578, -0.3516, -0.4207],\n          [-0.4436, -0.1539, -0.3768,  ..., -0.5277, -0.4855, -0.4495]],\n\n         ...,\n\n         [[-0.4967, -0.3191, -0.5172,  ..., -0.3178, -0.0690, -0.5089],\n          [-0.2761,  0.0149, -0.4904,  ..., -0.2543,  0.0177, -0.2294],\n          [-0.4058, -0.4162, -0.2881,  ...,  0.0443,  0.4478, -0.4462],\n          ...,\n          [-0.4004, -0.1296,  0.1152,  ...,  0.4313,  0.6645,  0.2798],\n          [-0.4441,  0.1218, -0.4305,  ...,  0.4615,  0.6798, -0.1293],\n          [-0.5465, -0.3989, -0.5344,  ..., -0.0198,  0.0151, -0.4183]],\n\n         [[-0.3388, -0.5053, -0.5295,  ..., -0.4755, -0.4938, -0.5397],\n          [-0.4959, -0.5068, -0.5260,  ..., -0.4077, -0.4669,  0.1614],\n          [ 0.7145, -0.1875, -0.1235,  ...,  0.2665,  0.0499,  1.1588],\n          ...,\n          [-0.4128, -0.3582, -0.5506,  ...,  0.2992, -0.2863,  0.2803],\n          [ 1.0126, -0.5243,  0.2794,  ...,  1.5115,  1.1862,  1.6769],\n          [-0.5020, -0.5326, -0.5383,  ..., -0.0564,  0.1121,  0.0871]],\n\n         [[-0.5202, -0.1108, -0.1819,  ..., -0.4941, -0.4913, -0.5165],\n          [-0.2305,  0.1010, -0.2430,  ...,  0.5093,  0.1895,  0.1037],\n          [-0.4895, -0.3958, -0.3056,  ..., -0.2141, -0.0102,  0.5653],\n          ...,\n          [ 1.0310, -0.5228, -0.1168,  ..., -0.5437, -0.4989,  0.2949],\n          [-0.0247, -0.3842, -0.1510,  ..., -0.5504, -0.5146, -0.0977],\n          [-0.5410, -0.5339, -0.4973,  ..., -0.5472, -0.4962, -0.3349]]]],\n       grad_fn=&lt;AddBackward0&gt;), tensor([[[[ 1.7336e-01, -2.6222e-01, -4.2793e-01,  ..., -1.1853e-01,\n           -2.9695e-01, -2.0832e-01],\n          [-3.4296e-03, -5.1239e-01, -5.2320e-01,  ..., -4.6905e-01,\n           -5.4388e-01, -1.7963e-01],\n          [ 6.9119e-02, -4.7162e-01, -4.7037e-01,  ..., -2.3459e-01,\n           -3.6831e-01, -2.2590e-02],\n          ...,\n          [ 8.7253e-01,  7.6361e-01,  5.9890e-01,  ...,  8.0291e-01,\n            1.0609e+00,  1.1305e+00],\n          [ 6.8713e-01,  6.3124e-01,  6.0094e-01,  ...,  8.2307e-01,\n            1.2763e+00,  4.9848e-01],\n          [ 3.0797e-01,  1.7016e-01,  8.4849e-01,  ...,  3.9683e-02,\n            1.4594e-01, -7.6637e-02]],\n\n         [[-4.7298e-01, -4.7068e-01, -4.7336e-01,  ..., -4.0120e-01,\n           -3.2050e-01, -2.8249e-01],\n          [-4.7759e-01, -5.4533e-01, -4.9800e-01,  ..., -5.0616e-01,\n           -4.6732e-01, -3.0524e-01],\n          [-4.9285e-01, -5.3249e-01, -5.3137e-01,  ..., -5.2591e-01,\n           -5.4686e-01, -4.9311e-01],\n          ...,\n          [-2.4863e-02,  5.6615e-01,  1.3635e+00,  ...,  1.5708e+00,\n            9.7564e-01,  8.4283e-01],\n          [-3.4610e-02,  9.4266e-01,  9.5997e-01,  ...,  1.2739e+00,\n            7.4010e-01,  7.9288e-01],\n          [-6.2025e-02, -2.2215e-01,  4.5640e-02,  ...,  1.0250e+00,\n            1.0097e+00,  3.7875e-01]],\n\n         [[ 8.6356e-01,  9.1551e-01,  8.9298e-01,  ...,  7.6591e-01,\n            7.6748e-01,  1.3054e+00],\n          [ 5.0886e-01,  4.1404e-01,  2.8045e-01,  ...,  7.0586e-01,\n            4.6785e-01,  1.3641e+00],\n          [ 2.3436e-01,  1.8401e-01,  6.3883e-01,  ...,  6.3147e-01,\n            3.4000e-01,  3.0140e-01],\n          ...,\n          [-3.2019e-01, -2.9413e-01, -1.6662e-01,  ...,  1.5093e-02,\n            2.1418e-01,  3.8254e-01],\n          [-2.4847e-01, -2.5539e-01, -3.1744e-01,  ..., -2.7485e-01,\n           -3.5164e-01, -3.4260e-02],\n          [-2.0574e-01, -1.8172e-01, -1.6656e-01,  ..., -1.1945e-01,\n           -3.5600e-02,  1.7422e-02]],\n\n         ...,\n\n         [[ 1.0281e-01, -1.7983e-01, -2.6104e-01,  ...,  2.9685e-01,\n            2.9522e-02, -1.5922e-01],\n          [ 6.4848e-01,  2.1403e-02, -2.9381e-02,  ...,  3.1224e-01,\n            4.1161e-01,  7.7101e-01],\n          [ 1.5228e-01,  8.2648e-02, -2.4173e-02,  ...,  8.9092e-01,\n            6.4181e-01,  4.0476e-01],\n          ...,\n          [ 4.2079e-01, -3.6907e-02, -1.4251e-01,  ..., -2.9826e-01,\n           -6.7151e-02, -3.6289e-02],\n          [ 1.2386e-01, -4.1844e-01, -3.9618e-01,  ..., -5.0743e-01,\n           -3.4355e-01, -1.8816e-01],\n          [-4.4068e-01, -5.4903e-01, -4.7067e-01,  ..., -6.2028e-02,\n           -5.0896e-01, -2.2307e-01]],\n\n         [[ 9.8333e-01, -2.7065e-01, -3.3590e-01,  ..., -3.3124e-01,\n           -1.4504e-01,  7.5079e-01],\n          [ 4.1522e-01, -3.4168e-01, -3.9592e-01,  ..., -4.3886e-01,\n           -3.1252e-01,  2.9204e-01],\n          [ 5.3854e-01, -9.4371e-02, -2.8638e-01,  ..., -3.7553e-01,\n           -2.0117e-01,  5.2053e-01],\n          ...,\n          [ 7.5336e-01,  4.2127e-02, -4.7527e-01,  ..., -2.3514e-01,\n            3.8577e-01,  1.0483e+00],\n          [ 7.9049e-01,  2.1774e-02, -3.5424e-01,  ..., -2.3879e-01,\n            1.9274e-01,  7.7723e-01],\n          [ 8.9491e-01,  3.2616e-01, -1.3408e-01,  ...,  1.5268e-01,\n            5.6745e-01,  1.0702e+00]],\n\n         [[ 1.7084e-01, -3.4757e-02, -1.7290e-01,  ..., -8.5381e-02,\n            1.6041e-01,  4.4312e-01],\n          [ 3.7726e-01, -2.6368e-02, -3.0844e-01,  ...,  5.2936e-02,\n            6.6930e-03,  3.1330e-01],\n          [ 5.3114e-01, -2.7307e-02, -4.2941e-01,  ...,  3.0857e-01,\n            6.0580e-01,  1.5468e-01],\n          ...,\n          [ 4.8378e-01, -1.1162e-03, -1.8971e-01,  ..., -3.5753e-01,\n           -2.4050e-01, -2.1233e-01],\n          [ 6.9566e-01,  6.6009e-01,  7.9304e-02,  ..., -3.6931e-01,\n           -1.2620e-01, -4.8648e-02],\n          [ 4.7603e-01,  3.2894e-01, -2.1638e-01,  ..., -4.6596e-01,\n           -2.9300e-01, -7.0762e-02]]]], grad_fn=&lt;AddBackward0&gt;)], encoder_hidden_states=None, encoder_attentions=None, init_reference_points=tensor([[[-0.4862, -0.7119, -0.1336, -2.2152],\n         [-0.6809,  0.0766, -0.6053, -1.3876],\n         [-0.6570,  0.1424, -0.2582, -1.2478],\n         ...,\n         [-0.7187, -0.1965, -0.4079, -1.5115],\n         [-0.7981,  0.2656,  0.4589, -1.1378],\n         [ 0.6217,  1.1485, -1.5680, -1.7015]]]), enc_topk_logits=tensor([[[-2.3160, -1.3959, -2.2753,  ..., -1.0056, -2.1959, -2.4759],\n         [-1.8523, -2.0708, -2.8310,  ..., -0.8968, -2.3933, -2.8444],\n         [-1.8766, -1.7784, -2.8721,  ..., -0.8604, -2.3539, -3.0173],\n         ...,\n         [-2.7151, -2.0619, -2.1254,  ..., -1.1068, -2.5075, -3.0717],\n         [-2.3066, -0.9151, -1.8485,  ..., -0.6808, -2.1900, -2.4505],\n         [-0.6756, -1.9183, -0.7957,  ..., -0.7819, -1.3664, -1.9971]]],\n       grad_fn=&lt;GatherBackward0&gt;), enc_topk_bboxes=tensor([[[0.3808, 0.3292, 0.4666, 0.0984],\n         [0.3361, 0.5191, 0.3531, 0.1998],\n         [0.3414, 0.5355, 0.4358, 0.2231],\n         ...,\n         [0.3277, 0.4510, 0.3994, 0.1807],\n         [0.3104, 0.5660, 0.6128, 0.2427],\n         [0.6506, 0.7592, 0.1725, 0.1543]]], grad_fn=&lt;SigmoidBackward0&gt;), enc_outputs_class=tensor([[[-1.8213, -0.6814, -1.5843,  ..., -1.2911, -1.7899, -3.3334],\n         [-1.8213, -0.6814, -1.5843,  ..., -1.2911, -1.7899, -3.3334],\n         [-1.8213, -0.6814, -1.5843,  ..., -1.2911, -1.7899, -3.3334],\n         ...,\n         [-1.4263, -0.7852, -3.2138,  ..., -1.3979, -2.2594, -0.6975],\n         [-1.8004, -0.7145, -2.7931,  ..., -1.6721, -2.0172, -0.9661],\n         [-1.8367, -0.4701, -3.1283,  ..., -1.8585, -1.4448, -1.6497]]],\n       grad_fn=&lt;ViewBackward0&gt;), enc_outputs_coord_logits=tensor([[[ 3.4028e+38,  3.4028e+38,  3.4028e+38,  3.4028e+38],\n         [ 3.4028e+38,  3.4028e+38,  3.4028e+38,  3.4028e+38],\n         [ 3.4028e+38,  3.4028e+38,  3.4028e+38,  3.4028e+38],\n         ...,\n         [ 1.3017e+00,  1.7718e+00, -5.1475e-01, -6.0635e-01],\n         [ 1.8548e+00,  2.4244e+00, -1.1509e+00, -1.5645e+00],\n         [ 2.9150e+00,  2.5202e+00, -2.4418e+00, -1.7639e+00]]],\n       grad_fn=&lt;AddBackward0&gt;), denoising_meta_values=None)\n\nNice!\nIt looks like it worked!\nOur model processed our random_sample_preprocessed_image_only[\"pixel_values\"] and returned a RTDetrV2ObjectDetectionOutput object as output.\nLet‚Äôs inspect the keys() method of this output and see what they are.\n\n# Check the keys of the output\nrandom_sample_outputs.keys()\n\nodict_keys(['logits', 'pred_boxes', 'last_hidden_state', 'intermediate_hidden_states', 'intermediate_logits', 'intermediate_reference_points', 'encoder_last_hidden_state', 'init_reference_points', 'enc_topk_logits', 'enc_topk_bboxes', 'enc_outputs_class', 'enc_outputs_coord_logits'])\n\n\nBreaking these down:\n\nlogits - The raw outputs from the model, these are the classification logits we can later apply a softmax function/sigmoid function to to get prediction probabilties.\npred_boxes - Normalized box coordinates in CXCYWH ((center_x, center_y, width, height)) format.\nlast_hidden_state - Last hidden state of the last decoder layer of the model.\nencoder_last_hidden_state - Last hidden state of the last encoder layer of the model.\n\nHow about we inspect the shape attribute of the logits?\n\n# Inspect logits output shape\noutput_logits = random_sample_outputs.logits\nprint(f\"[INFO] Output logits shape: {output_logits.shape} -&gt; [1 image, 300 boxes, 7 classes]\")\n\n[INFO] Output logits shape: torch.Size([1, 300, 7]) -&gt; [1 image, 300 boxes, 7 classes]\n\n\nNice!\nWe get an output from our model that coincides with the shape of our data.\nThe final value of 7 in the output_logits tensor is equivalent to the number of classes we have.\nAnd the 300 is the number of boxes our model predicts for each image (this is defined by the num_queries parameter of the transformers.RTDetrV2Config, where num_queries=300 is the default).\n\n# Inspect predicted boxes output shape\noutput_pred_boxes = random_sample_outputs.pred_boxes\nprint(f\"[INFO] Output predicted boxes shape: {output_pred_boxes.shape} -&gt; [1 image, 300 boxes, 4 coordinates (center_x, center_y, width, height)]\")\n\n[INFO] Output predicted boxes shape: torch.Size([1, 300, 4]) -&gt; [1 image, 300 boxes, 4 coordinates (center_x, center_y, width, height)]\n\n\nReading the documentation for the forward method, we can determine the output format of our models predicted boxes:\n\nReturns:\npred_boxes (torch.FloatTensor of shape (batch_size, num_queries, 4)) ‚Äî Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding). You can use post_process_object_detection() to retrieve the unnormalized bounding boxes.\n\nThis is good to know!\nIt means that the raw output boxes from our model come in normalized CXCYWH format (see Table¬†1 for more).\nHow about we inspect a single box?\n\n# Single example predicted bounding box coordinates\nprint(f\"[INFO] Example output box: {output_pred_boxes[:, 0, :][0].detach()} -&gt; (center_x, center_y, width, height)\")\n\n[INFO] Example output box: tensor([0.3726, 0.2132, 0.1820, 0.0800]) -&gt; (center_x, center_y, width, height)\n\n\nExcellent!\nWe can process these boxes and logits later on into different formats using the transformers.RTDetrImageProcessor.post_process_object_detection method.\nFor now, let‚Äôs figure out how to preprocess our annotations.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#preprocessing-our-annotations",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#preprocessing-our-annotations",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "9 Preprocessing our annotations",
    "text": "9 Preprocessing our annotations\nOne of the most tricky parts of any machine learning problem is getting your data in the right format.\nWe‚Äôve done it for our images.\nNow let‚Äôs do it for our annotations.\n\n9.1 Trying to preprocess a single annotation\nRecall in a previous section we tried to preprocess a single image and its annotation.\nAnd we got an error.\nLet‚Äôs make sure we‚Äôre not crazy and this is still the case.\n\n# Preprocess a single image and annotation pair\nimage_processor.preprocess(\n    images=random_sample[\"image\"], \n    annotations=random_sample[\"annotations\"]\n)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[30], line 2\n      1 # Preprocess a single image and annotation pair\n----&gt; 2 image_processor.preprocess(\n      3     images=random_sample[\"image\"], \n      4     annotations=random_sample[\"annotations\"]\n      5 )\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr/image_processing_rt_detr_fast.py:403, in RTDetrImageProcessorFast.preprocess(self, images, annotations, masks_path, **kwargs)\n    380 @auto_docstring\n    381 def preprocess(\n    382     self,\n   (...)\n    386     **kwargs: Unpack[RTDetrFastImageProcessorKwargs],\n    387 ) -&gt; BatchFeature:\n    388     r\"\"\"\n    389     annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n    390         List of annotations associated with the image or batch of images. If annotation is for object\n   (...)\n    401         Path to the directory containing the segmentation masks.\n    402     \"\"\"\n--&gt; 403     return super().preprocess(images, annotations, masks_path, **kwargs)\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/image_processing_utils_fast.py:654, in BaseImageProcessorFast.preprocess(self, images, *args, **kwargs)\n    651 kwargs.pop(\"default_to_square\")\n    652 kwargs.pop(\"data_format\")\n--&gt; 654 return self._preprocess(images, *args, **kwargs)\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr/image_processing_rt_detr_fast.py:440, in RTDetrImageProcessorFast._preprocess(self, images, annotations, masks_path, return_segmentation_masks, do_resize, size, interpolation, do_rescale, rescale_factor, do_normalize, do_convert_annotations, image_mean, image_std, do_pad, pad_size, format, return_tensors, **kwargs)\n    438 format = AnnotationFormat(format)\n    439 if annotations is not None:\n--&gt; 440     validate_annotations(format, SUPPORTED_ANNOTATION_FORMATS, annotations)\n    442 data = {}\n    443 processed_images = []\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/image_utils.py:919, in validate_annotations(annotation_format, supported_annotation_formats, annotations)\n    917 if annotation_format is AnnotationFormat.COCO_DETECTION:\n    918     if not valid_coco_detection_annotations(annotations):\n--&gt; 919         raise ValueError(\n    920             \"Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts \"\n    921             \"(batch of images) with the following keys: `image_id` and `annotations`, with the latter \"\n    922             \"being a list of annotations in the COCO format.\"\n    923         )\n    925 if annotation_format is AnnotationFormat.COCO_PANOPTIC:\n    926     if not valid_coco_panoptic_annotations(annotations):\n\nValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: `image_id` and `annotations`, with the latter being a list of annotations in the COCO format.\n\n\n\nWonderful!\nWe‚Äôre not crazy‚Ä¶\nBut we still get an error:\n\nValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: image_id and annotations, with the latter being a list of annotations in the COCO format.\n\nIn this section, we‚Äôre going to fix it.\n\n\n9.2 Discussing the format our annotations need to be in\nAccording the error we got in the previous segment, the transformers.RTDetrImageProcessor.preprocess method expects input annotations in COCO format.\nIn the documentation we can read that the annotations parameter taks in a list of dictionaries with the following keys:\n\n\"image_id\" (int): The image id.\n\"annotations\" (List[Dict]): List of annotations for an image. Each annotation should be a dictionary. An image can have no annotations, in which case the list should be empty.\n\nAs for the \"annotations\" field, this should be a list of dictionaries containing individual annotations in COCO format:\n# COCO format, see: https://cocodataset.org/#format-data  \n[{\n    \"image_id\": 42,\n    \"annotations\": [{\n        \"id\": 123456,\n        \"category_id\": 1,\n        \"iscrowd\": 0,\n        \"segmentation\": [\n            [42.0, 55.6, ... 99.3, 102.3]\n        ],\n        \"image_id\": 42, # this matches the 'image_id' field above\n        \"area\": 135381.07,\n        \"bbox\": [523.70,\n                 545.09,\n                 402.79,\n                 336.11]\n    },\n    # Next annotation in the same format as the previous one (one annotation per dict).\n    # For example, if an image had 4 bounding boxes, there would be a list of 4 dictionaries\n    # each containing a single annotation.\n    ...]\n}]\nLet‚Äôs breakdown each of the fields in the COCO annotation:\n\n\n\nTable¬†4: COCO data format keys breakdown\n\n\n\n\n\n\n\n\n\n\n\nField\nRequirement\nData Type\nDescription\n\n\n\n\nimage_id (top-level)\nRequired\nInteger\nID of the target image.\n\n\nannotations\nRequired\nList[Dict]\nList of dictionaries with one box annotation per dict. Can be empty if there are no boxes.\n\n\nid\nNot required\nInteger\nID of the particular annotation.\n\n\ncategory_id\nRequired\nInteger\nID of the class the box relates to (e.g.¬†{0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash'}).\n\n\nsegmentation\nNot required\nList or None\nSegmentation mask related to an annotation instance. Focus is on boxes, not segmentation.\n\n\nimage_id (inside annotations field)\nRequired\nInteger\nID of the target image the particular box relates to, should match image_id on the top-level field.\n\n\narea\nNot required\nFloat\nArea of the target bounding box (e.g.¬†box height * width).\n\n\nbbox\nRequired\nList[Float]\nCoordinates of the target bounding box in XYWH ([x, y, width, height]) format. (x, y) are the top left corner coordinates, width and height are dimensions.\n\n\nis_crowd\nNot required\nInt\nBoolean flag (0 or 1) to indicate whether or not an object is multiple (a crowd) of the same thing. For example, a crowd of ‚Äúpeople‚Äù or a group of ‚Äúapples‚Äù rather than a single apple.\n\n\n\n\n\n\nAnd now our annotation data comes in the format:\n{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n 'image_id': 292,\n 'annotations': {'file_name': ['00347467-13f1-4cb9-94aa-4e4369457e0c.jpeg',\n   '00347467-13f1-4cb9-94aa-4e4369457e0c.jpeg'],\n  'image_id': [292, 292],\n  'category_id': [1, 0],\n  'bbox': [[523.7000122070312,\n    545.0999755859375,\n    402.79998779296875,\n    336.1000061035156],\n   [10.399999618530273,\n    163.6999969482422,\n    943.4000244140625,\n    1101.9000244140625]],\n  'iscrowd': [0, 0],\n  'area': [135381.078125, 1039532.4375]},\n 'label_source': 'manual_prodigy_label',\n 'image_source': 'manual_taken_photo'}\nHow about we write some code to convert our current annotation format to COCO format?\n\n\n\n\n\n\nNote\n\n\n\nIt‚Äôs common practice to get a dataset in a certain format and then have to preprocess it into another format before you can use it with a model.\nWe‚Äôre getting hands-on and practicing here so when it comes to working on converting another dataset, you‚Äôve already had some practice.\n\n\n\n\n9.3 Creating dataclasses to represent the COCO bounding box format\nLet‚Äôs write some code to transform our existing annotation data into the format required by transformers.RTDetrImageProcessor.preprocess.\nWe‚Äôll start by creating two Python dataclasses to house our desired COCO annotation format.\nTo do this we‚Äôll:\n\nCreate SingleCOCOAnnotation which contains the format structure of a single COCO annotation.\nCreate ImageCOCOAnnotations which contains all of the annotations for a given image in COCO format. This may be a single instance of SingleCOCOAnnotation or multiple.\n\nWe‚Äôll decorate both of these with the @dataclass decorator.\nUsing a @dataclass gives several benefits:\n\nType hints - we can define the types of objects we want in the class definition, for example, we want image_id to be an int.\nHelpful built-in methods - we can use methods such as asdict to convert our @dataclass into a dictionary (COCO wants lists of dictionaries).\nData validation - we can use methods such as __post_init__ to run checks on our @dataclass as it‚Äôs initialized, for example, we always want the length of bbox to be 4 (bounding box coordinates in XYWH format).\n\n\nfrom dataclasses import dataclass, asdict\nfrom typing import List, Tuple\n\n# 1. Create a dataclass for a single COCO annotation\n@dataclass\nclass SingleCOCOAnnotation:\n    \"\"\"An instance of a single COCO annotation. \n    \n    Represent a COCO-formatted (see: https://cocodataset.org/#format-data) single instance of an object \n    in an image. \n\n    Attributes:\n        image_id: Unique integer identifier for the image which the annotation belongs to.\n        category_id: Integer identifier for the target object label/category (e.g. \"0\" for \"bin\").\n        bbox: List of floats containing target bounding box coordinates in absolute XYWH format ([x_top_left, y_top_left, width, height]).\n        area: Area of the target bounding box. Defaults to 0.0.\n        iscrowd: Boolean flag (0 or 1) indicating whether the target is a crowd of objects, for example, a group of \n            apples rather than a single apple. Defaults to 0.\n    \"\"\"\n    image_id: int\n    category_id: int\n    bbox: List[float] # bboxes in XYWH format ([x_top_left, y_top_left, width, height])\n    area: float = 0.0\n    iscrowd: int = 0\n\n    # Make sure the bbox is always a list of 4 values (XYWH format)\n    def __post_init__(self):\n        if len(self.bbox) != 4:\n            raise ValueError(f\"bbox must contain exactly 4 values, current length: {len(self.bbox)}\")\n\n\n# 2. Create a dataclass for a collection of COCO annotations for a single image\n@dataclass\nclass ImageCOCOAnnotations:\n    \"\"\"A collection of COCO annotations for a single image_id.\n\n    Attributes:\n        image_id: Unique integer identifier for the image which the annotations belong to.\n        annotations: List of SingleCOCOAnnotation instances.\n    \"\"\"\n    image_id: int\n    annotations: List[SingleCOCOAnnotation]\n\nBeautiful!\nLet‚Äôs now inspect our SingleCOCOAnnotation dataclass.\nWe can use the SingleCOCOAnnotation? syntax to view the docstring of the class.\n\n# One of the benefits of using a dataclass is that we can inspect the attributes with the `?` syntax\nSingleCOCOAnnotation?\n\n\nInit signature:\n\nSingleCOCOAnnotation(\n\n    image_id: int,\n\n    category_id: int,\n\n    bbox: List[float],\n\n    area: float = 0.0,\n\n    iscrowd: int = 0,\n\n) -&gt; None\n\nDocstring:     \n\nAn instance of a single COCO annotation. \n\n\n\nRepresent a COCO-formatted (see: https://cocodataset.org/#format-data) single instance of an object \n\nin an image. \n\n\n\nAttributes:\n\n    image_id: Unique integer identifier for the image which the annotation belongs to.\n\n    category_id: Integer identifier for the target object label/category (e.g. \"0\" for \"bin\").\n\n    bbox: List of floats containing target bounding box coordinates in absolute XYWH format ([x_top_left, y_top_left, width, height]).\n\n    area: Area of the target bounding box. Defaults to 0.0.\n\n    iscrowd: Boolean flag (0 or 1) indicating whether the target is a crowd of objects, for example, a group of \n\n        apples rather than a single apple. Defaults to 0.\n\nType:           type\n\nSubclasses:     \n\n\n\nWe can also see the error handling of our __post_init__ method in action by trying to create an instance of SingleCOCOAnnotation with an incorrect number of bbox values.\n\n# Let's try our SingleCOCOAnnotation dataclass (this will error since the bbox doesn't have 4 values)\nSingleCOCOAnnotation(image_id=42, \n                     category_id=0, \n                     bbox=[100, 100, 100]) # missing a 4th value\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[33], line 2\n      1 # Let's try our SingleCOCOAnnotation dataclass (this will error since the bbox doesn't have 4 values)\n----&gt; 2 SingleCOCOAnnotation(image_id=42, \n      3                      category_id=0, \n      4                      bbox=[100, 100, 100]) # missing a 4th value\n\nFile &lt;string&gt;:8, in __init__(self, image_id, category_id, bbox, area, iscrowd)\n\nCell In[31], line 29, in SingleCOCOAnnotation.__post_init__(self)\n     27 def __post_init__(self):\n     28     if len(self.bbox) != 4:\n---&gt; 29         raise ValueError(f\"bbox must contain exactly 4 values, current length: {len(self.bbox)}\")\n\nValueError: bbox must contain exactly 4 values, current length: 3\n\n\n\nAnd now if we pass the correct number of values to our SingleCOCOAnnotation, it should work.\n\nSingleCOCOAnnotation(image_id=42, \n                     category_id=0, \n                     bbox=[100, 100, 100, 100]) # correct number of values\n\nSingleCOCOAnnotation(image_id=42, category_id=0, bbox=[100, 100, 100, 100], area=0.0, iscrowd=0)\n\n\n\n\n9.4 Creating a function to format our annotations as COCO format\nNow we‚Äôve got the COCO data format in our SingleCOCOAnnotation and ImageCOCOAnnotation dataclasses, let‚Äôs write a function to take our existing image annotations and format them in COCO style.\nOur format_image_annotations_as_coco function will:\n\nTake in an image_id to represent a unique identifier for the image as well as lists of category integers, area values and bounding box coordinates.\nPerform a list comprehension on a zipped version of each category, area and bounding box coordinate value in the input lists creating an instance of SingleCOCOAnnotation as a dictionary (using the asdict method) each time, this will give us a list of SingleCOCOAnnotation formatted dictionaries.\nReturn a dictionary version of ImageCOCOAnnotations using asdict passing it the image_id as well as list of SingleCOCOAnnotation dictionaries from 2.\n\nWhy does our function take in lists of categories, areas and bounding boxes?\nBecause that‚Äôs the current format our existing annotations are in (how we downloaded them from Hugging Face in the beginning).\nLet‚Äôs do it!\n\n# 1. Take in a unique image_id as well as lists of categories, areas, and bounding boxes\ndef format_image_annotations_as_coco(\n        image_id: int,\n        categories: List[int],\n        areas: List[float],\n        bboxes: List[Tuple[float, float, float, float]] # bboxes in XYWH format ([x_top_left, y_top_left, width, height])\n) -&gt; dict:\n    \"\"\"Formats lists of image annotations into COCO format.\n    \n    Takes in parallel lists of categories, areas, and bounding boxes and\n    then formats them into a COCO-style dictionary of annotations.\n\n    Args:\n        image_id: Unique integer identifier for an image.\n        categories: List of integer category IDs for each annotation.\n        areas: List of float areas for each annotation.\n        bboxes: List of tuples containing bounding box coordinates in XYWH format \n            ([x_top_left, y_top_left, width, height]).\n    \n    Returns:\n        A dictionary of image annotations in COCO format with the following structure:\n        {\n            \"image_id\": int,\n            \"annotations\": [\n                {\n                    \"image_id\": int,\n                    \"category_id\": int,\n                    \"bbox\": List[float],\n                    \"area\": float\n                },\n                ...more annotations here\n            ]\n        }\n    \n    Note:\n        All input lists much be the same length and in the same order.\n        Otherwise, there will be mismatched annotations.\n    \"\"\"\n    \n    # 2. Turn input lists into a list of dicts in SingleCOCOAnnotation format\n    coco_format_annotations = [\n        asdict(SingleCOCOAnnotation(\n            image_id=image_id,\n            category_id=category,\n            bbox=list(bbox),\n            area=area,\n        ))\n        for category, area, bbox in zip(categories, areas, bboxes)\n    ]\n\n    # 3. Return a of annotations with format {\"image_id\": ..., \"annotations\": [...]} (required COCO format)\n    return asdict(ImageCOCOAnnotations(image_id=image_id,\n                                       annotations=coco_format_annotations))\n\nNice!\nHaving those pre-built dataclasses makes everything else fall into place.\nNow let‚Äôs try our format_image_annotations_as_coco function on a new not so random_sample (we‚Äôll make a random_sample with a known index for reproducibility).\nFirst, we‚Äôll remind ourselves what our random_sample looks like.\n\n# Create a not so random sample and inspect it \nrandom_sample = dataset[\"train\"][77]\nrandom_sample\n\n{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n 'image_id': 558,\n 'annotations': {'file_name': ['13df7e4a-1a5c-4da9-a5d3-204b6119670c.jpeg',\n   '13df7e4a-1a5c-4da9-a5d3-204b6119670c.jpeg',\n   '13df7e4a-1a5c-4da9-a5d3-204b6119670c.jpeg'],\n  'image_id': [558, 558, 558],\n  'category_id': [5, 0, 1],\n  'bbox': [[261.8999938964844, 734.5, 181.8000030517578, 216.3000030517578],\n   [99.80000305175781, 215.1999969482422, 730.0, 685.7999877929688],\n   [0.0, 769.2999877929688, 367.8999938964844, 508.70001220703125]],\n  'iscrowd': [0, 0, 0],\n  'area': [39323.33984375, 500634.0, 187150.734375]},\n 'label_source': 'manual_prodigy_label',\n 'image_source': 'manual_taken_photo'}\n\n\nOk wonderful, looks like we can extract the image_id, category_id bbox and area fields from our random_sample to get the required inputs to our format_image_annotations_as_coco function.\nLet‚Äôs try it out.\n\n# Extract image_id, categories, areas, and bboxes from the random sample\nrandom_sample_image_id = random_sample[\"image_id\"]\nrandom_sample_categories = random_sample[\"annotations\"][\"category_id\"]\nrandom_sample_areas = random_sample[\"annotations\"][\"area\"]\nrandom_sample_bboxes = random_sample[\"annotations\"][\"bbox\"]\n\n# Format the random sample annotations as COCO format\nrandom_sample_coco_annotations = format_image_annotations_as_coco(image_id=random_sample_image_id,\n                                                                  categories=random_sample_categories,\n                                                                  areas=random_sample_areas,\n                                                                  bboxes=random_sample_bboxes)\nrandom_sample_coco_annotations\n\n{'image_id': 558,\n 'annotations': [{'image_id': 558,\n   'category_id': 5,\n   'bbox': [261.8999938964844, 734.5, 181.8000030517578, 216.3000030517578],\n   'area': 39323.33984375,\n   'iscrowd': 0},\n  {'image_id': 558,\n   'category_id': 0,\n   'bbox': [99.80000305175781, 215.1999969482422, 730.0, 685.7999877929688],\n   'area': 500634.0,\n   'iscrowd': 0},\n  {'image_id': 558,\n   'category_id': 1,\n   'bbox': [0.0, 769.2999877929688, 367.8999938964844, 508.70001220703125],\n   'area': 187150.734375,\n   'iscrowd': 0}]}\n\n\nWoohoo!\nLooks like we may have just fixed our ValueError from before:\n\nValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: image_id and annotations, with the latter being a list of annotations in the COCO format.\n\nOur COCO formatted annotations have the image_id and annotations keys and our annotations are a list of annotations in COCO format.\nPerfect!\n\n\n9.5 Preprocess a single image and set of COCO format annotations\nNow we‚Äôve preprocessed our annotations to be in COCO format, we can use them with transformers.RTDetrImageProcessor.preprocess.\nLet‚Äôs pass our random_sample image and COCO formatted annotations to the preprocess method.\n\n\n\n\n\n\nNote\n\n\n\nThe default value for the parameter do_convert_annotations of the preprocess method is True.\nThis means our boxes will go into the preprocess method in absolute XYWH format (the format we downloaded them in) and will be returned in normalized CXCYWH (or (center_x, center_y, width, height)) format.\nWhenever you perform adjustments or preprocessing steps on your annotations, it‚Äôs always good to keep track of the format that they are in, otherwise it can lead to unexpected bugs later on.\n\n\n\n# Preprocess random sample image and assosciated annotations\nrandom_sample_preprocessed = image_processor.preprocess(images=random_sample[\"image\"],\n                                                        annotations=random_sample_coco_annotations,\n                                                        do_convert_annotations=True, # defaults to True, this will convert our annotations to normalized CXCYWH format\n                                                        return_tensors=\"pt\" # can return as tensors or not, \"pt\" returns as PyTorch tensors\n                                                        ) \n\n\n\n\n\n\n\nNote\n\n\n\nWhen processing our single image and annotation, you may see a warning similar to the following:\n\nThe max_size parameter is deprecated and will be removed in v4.26. Please specify in size['longest_edge'] instead.\n\nIf you are not using the max_size parameter and are using a version of transformers &gt; 4.26, you can ignore this or disable it (as shown below).\n\n\n\n# Optional: Disable warnings about `max_size` parameter being deprecated\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", message=\"The `max_size` parameter is deprecated*\")\n\nExcellent!\nIt looks like the preprocess method worked on our single sample.\nLet‚Äôs inspect the keys() method of our random_sample_preprocessed.\n\n# Check the keys of our preprocessed example\nrandom_sample_preprocessed.keys()\n\nKeysView({'pixel_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]]]), 'pixel_values': tensor([[[[0.4980, 0.4314, 0.5333,  ..., 0.3020, 0.2902, 0.2941],\n          [0.5843, 0.4510, 0.4392,  ..., 0.2863, 0.2745, 0.2784],\n          [0.7020, 0.5686, 0.4471,  ..., 0.2588, 0.2588, 0.2627],\n          ...,\n          [0.2588, 0.2627, 0.2588,  ..., 0.5412, 0.6510, 0.6275],\n          [0.2706, 0.2706, 0.2667,  ..., 0.6235, 0.6235, 0.5882],\n          [0.2784, 0.2784, 0.2745,  ..., 0.7020, 0.6745, 0.5804]],\n\n         [[0.4510, 0.3843, 0.4941,  ..., 0.2863, 0.2745, 0.2784],\n          [0.5451, 0.4118, 0.4000,  ..., 0.2706, 0.2588, 0.2627],\n          [0.6706, 0.5373, 0.4157,  ..., 0.2431, 0.2431, 0.2471],\n          ...,\n          [0.2431, 0.2471, 0.2431,  ..., 0.5098, 0.6196, 0.5961],\n          [0.2549, 0.2549, 0.2510,  ..., 0.5922, 0.5922, 0.5569],\n          [0.2627, 0.2627, 0.2588,  ..., 0.6706, 0.6431, 0.5490]],\n\n         [[0.4196, 0.3529, 0.4549,  ..., 0.2392, 0.2275, 0.2314],\n          [0.5098, 0.3725, 0.3569,  ..., 0.2235, 0.2157, 0.2157],\n          [0.6275, 0.4902, 0.3686,  ..., 0.2000, 0.2000, 0.2000],\n          ...,\n          [0.3490, 0.3529, 0.3490,  ..., 0.4588, 0.5686, 0.5451],\n          [0.3608, 0.3608, 0.3569,  ..., 0.5412, 0.5412, 0.5059],\n          [0.3686, 0.3686, 0.3647,  ..., 0.6196, 0.5922, 0.4980]]]]), 'labels': [{'size': tensor([640, 480]), 'image_id': tensor([558]), 'class_labels': tensor([5, 0, 1]), 'boxes': tensor([[0.3675, 0.6583, 0.1894, 0.1690],\n        [0.4842, 0.4360, 0.7604, 0.5358],\n        [0.1916, 0.7997, 0.3832, 0.3974]]), 'area': tensor([  9830.8350, 125158.5000,  46787.6836]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}]})\n\n\nWonderful, we get a preprocessed image and labels:\n\npixel_values = preprocessed pixels (the preprocessed image).\n(Optional) pixel_mask = whether or not to mask the pixels (e.g.¬†0 = mask, 1 = no mask, in our case, all values will be 1 since we want the model to see all pixels).\nlabels = preprocessed labels (the preprocessed annotations).\n\n\n# Inspect preprocessed image shape\nprint(f\"[INFO] Preprocessed image shape: {random_sample_preprocessed['pixel_values'].shape} -&gt; [batch_size, colour_channels, height, width]\")\n\n[INFO] Preprocessed image shape: torch.Size([1, 3, 640, 480]) -&gt; [batch_size, colour_channels, height, width]\n\n\nSince we only passed a single sample to preprocess, we get back a batch size of 1.\nNow how do our labels look?\n\n# Inspect the preprocessed labels (our boxes and other metadata)\npprint(random_sample_preprocessed[\"labels\"])\n\n[{'area': tensor([  9830.8350, 125158.5000,  46787.6836]),\n  'boxes': tensor([[0.3675, 0.6583, 0.1894, 0.1690],\n        [0.4842, 0.4360, 0.7604, 0.5358],\n        [0.1916, 0.7997, 0.3832, 0.3974]]),\n  'class_labels': tensor([5, 0, 1]),\n  'image_id': tensor([558]),\n  'iscrowd': tensor([0, 0, 0]),\n  'orig_size': tensor([1280,  960]),\n  'size': tensor([640, 480])}]\n\n\nLet‚Äôs break this down:\n\narea - An array/tensor of floats containing the area (box_width * box_height) of our boxes.\nboxes - An array/tensor containing all of the bounding boxes for our image in normalized CXCYWH ((center_x, center_y, width, height)) format.\nclass_labels - An array/tensor of integer labels assosciated with each box (e.g.¬†tensor([5, 1, 0, 0, 4]) -&gt; ['trash', 'hand', 'bin', 'bin', 'not_trash']).\nimage_id - A unique integer identifier for our target image.\nis_crowd - An array/tensor of a boolean value (0 or 1) for whether an annotation is a group or not.\norig_size - An array/tensor containing the original size in (height, width) format (this is important for drawing conversion factors when using originally sized images).\nsize - An array/tensor with the current size in (height, width) format of the processed image tensor contained within random_sample_preprocessed[\"pixel_values\"].\n\nWoohoo!\nWe‚Äôve done it!\nWe‚Äôve officially preprocessed a single sample of our own data, both the image and its annotation pair.\nWe‚Äôll write some code later on to scale this up to our whole dataset.\nFor now, let‚Äôs see what it looks like postprocessing a single output.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#postprocessing-a-single-output",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#postprocessing-a-single-output",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "10 Postprocessing a single output",
    "text": "10 Postprocessing a single output\nWe‚Äôve got our inputs processed and successfully passed them through our model.\nHow about we postprocess the outputs of our model?\nDoing so will make our model‚Äôs outputs far more usable.\n\n\n\n\n\n\nGoing end-to-end on a single sample\n\n\n\nWhen working on a new problem or with a custom dataset and an existing model, it‚Äôs good practice to go end-to-end on a single sample.\nFor example, preprocess one of your samples, pass it through the model and then postprocess it (just like we‚Äôre in the middle of doing here).\nBeing able to go end-to-end on a single sample will help you see the overall process and discover any bugs that may hinder you later on.\n\n\nTo postprocess the outputs of our model we can use the transformers.RTDetrImageProcessor.post_process_object_detection() method (see the source code on GitHub, this is what we‚Äôll reproduce by hand).\nLet‚Äôs frist recompute the model‚Äôs outputs for our preprocessed single sample.\n\n# Recompute the random sample outputs with our preprocessed sample\nrandom_sample_outputs = model(\n    pixel_values=random_sample_preprocessed[\"pixel_values\"], # model expects input [batch_size, color_channels, height, width]\n    # pixel_mask=random_sample_preprocessed[\"pixel_mask\"], # optional: some models expect pixel_mask inputs\n)\n\n# Inspect the output type\ntype(random_sample_outputs)\n\ntransformers.models.rt_detr_v2.modeling_rt_detr_v2.RTDetrV2ObjectDetectionOutput\n\n\nWonderful!\nWe get the exact output our post_process_object_detection() method is looking for.\nNow we can fill in the following parameters:\n\noutputs - Raw outputs of the model (for us, this is random_sample_outputs).\nthreshold - A float score value to keep or discard boxes (e.g.¬†threshold=0.3 means all boxes under 0.3 will be discarded). This value can be adjusted as needed. A higher value means only the boxes the model is most confident on will be kept. A lower value means more boxes will be kept, however, these may be over lower quality. Best to be experimented with.\ntarget_sizes - Size of target image in (height, width) format for bounding boxes. For example, if our image is 960 pixels wide by 1280 high, we could pass in [1280, 960]. Number of target_sizes must match number of outputs. For example, if pass in 1 set of outputs, only 1 target_sizes is needed. If we pass in a batch of 32 outputs, 32 target_sizes are required, else it will error. If None, postprocessed outputs won‚Äôt be resized (this can be lead to poor looking boxes as the coordinates don‚Äôt match your image).\ntop_k - Integer defining the number of boxes you‚Äôd like to prepare for postprocessing before thresholding. Defaults to 100. For example, top_k=100 and threshold=0.3 means sample 100 boxes and then of those 100 boxes, only keep those with a score over 0.3.\n\nYou can see what happens behind the scenes of post_process_object_detection in the source code.\n\n\n\n\n\n\nNote\n\n\n\nDon‚Äôt worry too much if your boxes output different values to the below.\nThey likely will.\nThis is due to the inherit randomness in machine learning.\nSince we initialized our model with a random output head, it is currently outputting random predictions.\nThis means every time we run the following cell with a different image, we‚Äôll likely get different predictions.\nWe can make the random predictions ‚Äúless random‚Äù or ‚Äúconsistently random‚Äù using a random seed via transformers.set_seed.\n\n\n\n# Set the score threshold for postprocessing\nTHRESHOLD = 0.4 # adjust this where necessary to get a handful of outputs below (note: if it's too high, e.g. 0.5+, you might not see any outputs, try lowering to 0.3\n\n# Post process a single output from our model\nrandom_sample_outputs_post_processed = image_processor.post_process_object_detection(\n    outputs=random_sample_outputs,\n    threshold=THRESHOLD, # all boxes with scores under this value will be discarded (best to experiment with it)\n    target_sizes=random_sample_preprocessed[\"labels\"][0][\"orig_size\"].unsqueeze(0) # original input image size (or whichever target size you'd like), required to be same number of input items in a list\n)\n\nrandom_sample_outputs_post_processed\n\n[{'scores': tensor([0.5886, 0.5872, 0.5826, 0.5804, 0.5747, 0.5669, 0.5660, 0.5616, 0.5595,\n          0.5533, 0.5532, 0.5462, 0.5462, 0.5448, 0.5447, 0.5435, 0.5435, 0.5433,\n          0.5432, 0.5405, 0.5405, 0.5383, 0.5380, 0.5379, 0.5377, 0.5376, 0.5374,\n          0.5368, 0.5367, 0.5359, 0.5358, 0.5337, 0.5336, 0.5322, 0.5312, 0.5311,\n          0.5308, 0.5289, 0.5286, 0.5284, 0.5282, 0.5271, 0.5247, 0.5245, 0.5241,\n          0.5236, 0.5225, 0.5222, 0.5222, 0.5184, 0.5179, 0.5168, 0.5161, 0.5157,\n          0.5153, 0.5152, 0.5136, 0.5132, 0.5130, 0.5124, 0.5124, 0.5120, 0.5116,\n          0.5113, 0.5113, 0.5106, 0.5093, 0.5073, 0.5071, 0.5069, 0.5069, 0.5068,\n          0.5059, 0.5056, 0.5048, 0.5046, 0.5044, 0.5038, 0.5037, 0.5035, 0.5035,\n          0.5030, 0.5028, 0.5018, 0.5018, 0.5016, 0.5016, 0.5006, 0.5002, 0.5000,\n          0.4995, 0.4991, 0.4984, 0.4983, 0.4973, 0.4966, 0.4964, 0.4960, 0.4956,\n          0.4953, 0.4950, 0.4941, 0.4934, 0.4933, 0.4931, 0.4917, 0.4917, 0.4915,\n          0.4909, 0.4897, 0.4891, 0.4887, 0.4885, 0.4882, 0.4882, 0.4880, 0.4880,\n          0.4870, 0.4856, 0.4844, 0.4843, 0.4837, 0.4823, 0.4822, 0.4822, 0.4819,\n          0.4818, 0.4817, 0.4817, 0.4809, 0.4800, 0.4800, 0.4796, 0.4792, 0.4785,\n          0.4784, 0.4775, 0.4775, 0.4757, 0.4755, 0.4745, 0.4733, 0.4730, 0.4728,\n          0.4726, 0.4722, 0.4710, 0.4709, 0.4705, 0.4703, 0.4698, 0.4695, 0.4695,\n          0.4681, 0.4680, 0.4677, 0.4675, 0.4667, 0.4666, 0.4666, 0.4659, 0.4650,\n          0.4650, 0.4647, 0.4644, 0.4631, 0.4614, 0.4605, 0.4604, 0.4598, 0.4592,\n          0.4591, 0.4589, 0.4564, 0.4562, 0.4557, 0.4556, 0.4555, 0.4537, 0.4534,\n          0.4513, 0.4511, 0.4507, 0.4504, 0.4499, 0.4480, 0.4474, 0.4473, 0.4458,\n          0.4453, 0.4446, 0.4439, 0.4434, 0.4433, 0.4423, 0.4422, 0.4417, 0.4404,\n          0.4403, 0.4395, 0.4390, 0.4387, 0.4384, 0.4376, 0.4364, 0.4363, 0.4357,\n          0.4355, 0.4351, 0.4348, 0.4343, 0.4340, 0.4337, 0.4324, 0.4323, 0.4320,\n          0.4317, 0.4313, 0.4289, 0.4289, 0.4280, 0.4267, 0.4265, 0.4264, 0.4259,\n          0.4238, 0.4212, 0.4203, 0.4181, 0.4167, 0.4164, 0.4162, 0.4160, 0.4157,\n          0.4149, 0.4135, 0.4132, 0.4127, 0.4112, 0.4109, 0.4100, 0.4096, 0.4094,\n          0.4086, 0.4081, 0.4062, 0.4060, 0.4058, 0.4057, 0.4050, 0.4038, 0.4015],\n         grad_fn=&lt;IndexBackward0&gt;),\n  'labels': tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n          5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n          5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n          5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n          5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n          5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n          5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n          5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 2, 5, 5, 5, 5,\n          5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5,\n          5, 0, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 0,\n          5, 5, 0, 5, 0, 5, 5, 5, 0, 2, 0, 5]),\n  'boxes': tensor([[ 6.3129e+02,  5.2683e+02,  7.2593e+02,  7.3370e+02],\n          [ 6.1083e+02,  7.0313e+02,  7.0312e+02,  8.7310e+02],\n          [ 6.5854e+02,  5.9864e+02,  7.0919e+02,  7.1037e+02],\n          ...,\n          [-3.5870e-01,  7.1785e+02,  1.1115e+02,  9.0279e+02],\n          [ 3.9477e+02,  8.6496e+02,  6.3893e+02,  9.2062e+02],\n          [ 3.6671e+02,  7.5343e+02,  4.2857e+02,  8.8866e+02]],\n         grad_fn=&lt;IndexBackward0&gt;)}]\n\n\nPerfect!\nThis looks like something we can use.\nLet‚Äôs break down each of the keys in random_sample_outputs_post_processed.\nWe get three equal length tensors:\n\nscores - The prediction probabilities for each box, higher means the model is more confident in this prediction (though it doesn‚Äôt mean the prediction is correct). Notice how all the values in this tensor are over our threshold value. This value is acquired by applying torch.sigmoid() to the models raw output logits.\nlabels - The predicted classification label values for each box. These will be random as our model hasn‚Äôt been trained for our dataset. We can turn these into class names by mapping them to the id2label dictionary.\nboxes - The predicted bounding boxes whose scores are above the threshold parameter. These are normalized and in the format XYXY or (x_top_left, y_top_left, x_bottom_right, y_bottom_right).\n\nLet‚Äôs see what these predictions look like when plotted.\n\n# Get target image and boxes (half them for display purposes in the notebook)\nimage_to_plot = half_image(random_sample[\"image\"])\nboxes_to_plot = half_boxes(random_sample_outputs_post_processed[0][\"boxes\"])\n\n# Collect the boxes, scores and labels\nrandom_sample_label_names = []\nfor result in random_sample_outputs_post_processed:\n     for score, label_id, box in zip(result[\"scores\"], \n                                     result[\"labels\"], \n                                     result[\"boxes\"]):\n         score, label = score.item(), label_id.item()\n         box = [round(i, 2) for i in box.tolist()]\n         # Optionally print out each prediction \n         # print(f\"[INFO] {model.config.id2label[label]}: {score:.2f} {box}\")\n\n         random_sample_label_names.append(model.config.id2label[label])\n\n# Get the list of colours to plot\nrandom_sample_colours = [colour_palette[label_name] for label_name in random_sample_label_names]\n# print(f\"Label names: {random_sample_label_names}\")\n# print(f\"Colour codes: {random_sample_colours}\")\n\n# Create the output image with plotted boxes\noutput_image_with_boxes = to_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=image_to_plot),\n        boxes=boxes_to_plot,\n        labels=random_sample_label_names,\n        colors=random_sample_colours,\n        width=3,\n        label_colors=random_sample_colours\n    )\n)\noutput_image_with_boxes\n\n\n\n\n\n\n\n\nNice! We get some plotted boxes. But they look pretty poor.\nThis is because our model is currently predicting random outputs based on the new output layers we instantiated.\nWe‚Äôll be working towards making these boxes better later on.\n\n10.1 Reproducing our postprocessed box scores by hand\nWhen a raw prediction output from our model goes through the post_process_object_detection method, a few steps happen.\nOne of them is that the raw logits from our model get converted into prediction probabilities.\nThis happens by:\n\nApplying the torch.sigmoid() function to the logits to turn them into prediction probabilities. We‚Äôll also flatten them with torch.flatten(start_dim=1) to turn them into a single dimension tensor (e.g.¬†torch.Size([1, 300, 7]) -&gt; torch.Size([1, 2100])).\nGetting the top 100 (post_process_object_detection returns the top 100 values by default) prediction scores using torch.topk().\nFind the values above the target threshold by creating a mask.\nFilter the top 100 scores which are above the threshold (using our mask) and sort them in descending order and get the indices with torch.sort() (so the predictions with the highest prediction probability come first).\n\ntorch.topk() and torch.sort() will return both raw tensor values and the indices of where they occur in a tuple (values, indices).\nThese index values are the predicted label ID.\n\n\n\n\n\n\nNote\n\n\n\nWhen a prediction probability is assigned to a prediction, it usually falls between 0 and 1.\nWith 1 being the highest possible score.\nThe value with the highest prediction probability is the value the model is predicting to the be the most likely value.\nHowever, just because a prediction has being assigned a high prediction probability does not mean it is correct.\nA high prediction probability is essentially the model saying, ‚Äúbased on the training data I have and the patterns I‚Äôve learned, this is the most likely outcome‚Äù.\nA workflow using prediction probabilities could be to automatically send samples with low prediction probabilities for manual review.\nFor now, our model will likely assign close to random prediction probabilities as it has not been trained on our data.\n\n\nTo see this happen, let‚Äôs reproduce the \"scores\" key in random_sample_outputs_post_processed by hand.\n\n# Get the output scores from our post processed single output\noutput_scores = random_sample_outputs_post_processed[0][\"scores\"]\nlen(output_scores), output_scores\n\n(96,\n tensor([0.6736, 0.5951, 0.5918, 0.5854, 0.5757, 0.5403, 0.5364, 0.5363, 0.5268,\n         0.5200, 0.5069, 0.5045, 0.5005, 0.4951, 0.4890, 0.4884, 0.4857, 0.4854,\n         0.4828, 0.4814, 0.4808, 0.4788, 0.4780, 0.4746, 0.4729, 0.4649, 0.4629,\n         0.4608, 0.4608, 0.4606, 0.4598, 0.4580, 0.4577, 0.4506, 0.4504, 0.4500,\n         0.4498, 0.4478, 0.4476, 0.4469, 0.4465, 0.4448, 0.4432, 0.4431, 0.4418,\n         0.4417, 0.4393, 0.4378, 0.4377, 0.4374, 0.4372, 0.4365, 0.4348, 0.4332,\n         0.4316, 0.4316, 0.4290, 0.4287, 0.4270, 0.4257, 0.4256, 0.4249, 0.4239,\n         0.4238, 0.4232, 0.4229, 0.4227, 0.4226, 0.4224, 0.4202, 0.4192, 0.4178,\n         0.4178, 0.4160, 0.4154, 0.4151, 0.4147, 0.4145, 0.4122, 0.4107, 0.4101,\n         0.4099, 0.4098, 0.4088, 0.4087, 0.4087, 0.4081, 0.4066, 0.4056, 0.4045,\n         0.4041, 0.4037, 0.4016, 0.4015, 0.4013, 0.4003],\n        grad_fn=&lt;IndexBackward0&gt;))\n\n\nAnd we can reproduce these scores by following the steps outlined above.\n\nprint(f\"[INFO] Original input logits shape: {random_sample_outputs.logits.shape}\\n\") \n\n# 1. Perform sigmoid on the logits to get prediction probabilities \noutput_scores_manual = random_sample_outputs.logits.sigmoid().flatten(start_dim=1)\nprint(f\"[INFO] Manual output scores shape: {output_scores_manual.shape}\")\nprint(f\"[INFO] First 10 scores (these will be in random order):\\n{output_scores_manual[0][:10].detach().cpu()}\\n\")\n\n# 2. Get the top 100 scores (we can get any top amount but 100 will do for now) \noutput_scores_manual_top_100, output_scores_manual_top_100_indices = torch.topk(input=output_scores_manual,\n                                                                                k=100,\n                                                                                dim=-1)\nprint(f\"[INFO] Top 100 scores shape: {output_scores_manual_top_100.shape}\")\nprint(f\"[INFO] First top 100 score:\\n{output_scores_manual_top_100[0][0].item():.4f}\\n\")\n\n# 3. Find the values above the threshold and create a mask\noutput_scores_manual_mask = output_scores_manual_top_100 &gt; THRESHOLD\n\n# 4. Sort the top 100 scores which are above the threshold and sort them in descending order and get the indices\noutput_scores_manual_filtered, output_scores_manual_filtered_indices = torch.sort(input=output_scores_manual_top_100[output_scores_manual_mask], \n                                                                                  descending=True)\n\nprint(f\"[INFO] Filtered scores shape: {output_scores_manual_filtered.shape}\")\nprint(f\"[INFO] First filtered scores:\\n{output_scores_manual_filtered[0].detach().cpu():.4f}\")\n\n[INFO] Original input logits shape: torch.Size([1, 300, 7])\n\n[INFO] Manual output scores shape: torch.Size([1, 2100])\n[INFO] First 10 scores (these will be in random order):\ntensor([0.0792, 0.2543, 0.1580, 0.5757, 0.2654, 0.1171, 0.0330, 0.0410, 0.1540,\n        0.0951])\n\n[INFO] Top 100 scores shape: torch.Size([1, 100])\n[INFO] First top 100 score:\n0.6736\n\n[INFO] Filtered scores shape: torch.Size([96])\n[INFO] First filtered scores:\n0.6736\n\n\nNow we‚Äôve got our own output_scores_manual_filtered, how about we compare it to output_scores?\nWe can see if they‚Äôre close to each other with torch.isclose().\n\n# Compare the original output scores to our own manual version\ntorch.isclose(input=output_scores[:len(output_scores_manual_filtered)], \n              other=output_scores_manual_filtered, \n              atol=1e-2)\n\ntensor([True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True])\n\n\nNice!\nWe managed to reproduce our postprocessed output scores values by hand.\nHow about the labels?\n\n\n10.2 Reproducing our postprocessed box labels by hand\nWe‚Äôve reproduce our postprocessed model prediction scores by hand.\nNow let‚Äôs do the same with the labels.\nFirst, we‚Äôll get the output labels from our postprocessed object.\n\n# Get the model's predicted labels \noutput_labels = random_sample_outputs_post_processed[0][\"labels\"]\nprint(f\"[INFO] Output labels shape: {len(output_labels)}\")\nprint(f\"[INFO] Output labels:\\n{output_labels}\")\n\n[INFO] Output labels shape: 96\n[INFO] Output labels:\ntensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n\n\nWonderful!\nNow to reproduce these values, we can:\n\nGet the number of classes we have using random_sample_outputs.logits.shape[2], this way we can normalize the flattened indices we created in the previous section.\nModulo the output_scores_manual_top_100_indices by the number of classes to get the predicted labels (e.g.¬†index % num_classes = predicted label -&gt; 747 % 7 = 5).\nFilter the remaining labels for predictions which pass the threshold using output_scores_manual_filtered_indices.\n\n\n# 1. Get the number of classes\nnum_classes = random_sample_outputs.logits.shape[2]\nprint(f\"[INFO] Found total number of classes: {num_classes}\")\n\n# 2. Modulo the output_scores_manual_top_100_indices by the number of classes to get the predicted class (this is because we flattened our outputs above with .flatten(1))\noutput_labels_manual = output_scores_manual_top_100_indices % num_classes\n\n# 3. Find the top labels which pass our score threshold\noutput_labels_manual_filtered = output_labels_manual[0][output_scores_manual_filtered_indices]\n\noutput_labels.shape, output_labels_manual_filtered.shape\n\n[INFO] Found total number of classes: 7\n\n\n(torch.Size([96]), torch.Size([96]))\n\n\nExcellent, now let‚Äôs make sure these labels are equivalent to the postprocessed labels.\n\noutput_labels[:len(output_labels_manual_filtered)] == output_labels_manual_filtered\n\ntensor([True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True])\n\n\nPerfect!\nHow about we repeat the same for our model‚Äôs postprocessed predicted boxes?\n\n\n10.3 Reproducing our postprocessed box coordinates by hand\nOur postprocessed boxes are in absolute XYXY format.\nAnd the raw boxes out of the model are in normalized CXCYWH format.\nFirst, let‚Äôs get the postprocessed predicted boxes we‚Äôre trying to reproduce.\n\n# These are in absolute XYXY (x_top_left, y_top_left, x_bottom_right, y_bottom_right) format\noutput_boxes = random_sample_outputs_post_processed[0][\"boxes\"]\nprint(f\"[INFO] Output boxes shape: {output_boxes.shape}\")\nprint(f\"[INFO] Output boxes (absolute XYXY format), first 10:\\n{output_boxes[:10]}\")\n\n[INFO] Output boxes shape: torch.Size([96, 4])\n[INFO] Output boxes (absolute XYXY format), first 10:\ntensor([[ 5.0105e+02,  4.5203e-01,  7.3708e+02,  2.4250e+02],\n        [ 5.0412e+02,  2.0059e+00,  7.3278e+02,  1.8306e+02],\n        [ 6.1310e+02,  2.3096e+02,  6.6116e+02,  3.0177e+02],\n        [ 5.0306e+02,  1.2722e+00,  9.5733e+02,  2.2517e+02],\n        [ 2.4945e+01,  3.0436e+02,  5.1925e+01,  3.4591e+02],\n        [ 5.1260e+02,  6.8830e-01,  9.4725e+02,  6.6806e+01],\n        [ 6.1350e+02,  2.2984e+02,  7.0224e+02,  3.0797e+02],\n        [ 2.1152e+01, -3.2943e-01,  9.5351e+02,  1.4764e+02],\n        [ 1.2980e+01, -3.1096e+00,  9.5033e+02,  3.1315e+02],\n        [ 4.9968e+02,  2.2245e+02,  6.2810e+02,  2.4664e+02]],\n       grad_fn=&lt;SliceBackward0&gt;)\n\n\nBeautiful!\nThese are the boxes we‚Äôd like to reproduce.\nLet‚Äôs now get the raw predicted box coordinates from our model.\n\n# Get model output raw boxes\n# These are in format: normalized CXCYWH (center_x, center_y, width, height) format\noutput_boxes_manual_cxcywh = random_sample_outputs.pred_boxes[0]\nprint(f\"[INFO] Output boxes manual shape: {output_boxes_manual_cxcywh.shape}\")\nprint(f\"[INFO] Output boxes manual (normalized CXCYWH format), first 10:\\n{output_boxes_manual_cxcywh[:10]}\")\n\n[INFO] Output boxes manual shape: torch.Size([300, 4])\n[INFO] Output boxes manual (normalized CXCYWH format), first 10:\ntensor([[0.0400, 0.2540, 0.0281, 0.0325],\n        [0.9611, 0.0529, 0.0756, 0.0779],\n        [0.4853, 0.4791, 0.1735, 0.0516],\n        [0.3670, 0.6512, 0.1584, 0.1370],\n        [0.4984, 0.6331, 0.4169, 0.1247],\n        [0.3549, 0.5278, 0.0505, 0.0488],\n        [0.7606, 0.0885, 0.4732, 0.1749],\n        [0.3979, 0.6548, 0.0929, 0.0897],\n        [0.9758, 0.7925, 0.0431, 0.0480],\n        [0.5076, 0.0575, 0.9712, 0.1156]], grad_fn=&lt;SliceBackward0&gt;)\n\n\nOk, so we‚Äôve got 300 boxes here, let‚Äôs filter these down to only the boxes which have a score above the target threshold using our output_scores_manual_mask tensor.\nIf we want to go from raw boxes out of the model to the same format as our postprocessed boxes or from normalized CXCYWH to absolute XYXY, we‚Äôll have to:\n\nNormalize the output_scores_manual_top_100_indices by dividing them by the number of classes, for example, top_100_index // num_classes = top_box_index -&gt; 763 // 7 = 109.\nFilter for the top 100 scoring boxes coordinates which make it over the threshold using the output_scores_manual_mask.\nConvert the filtered box coordinates from normalized CXCYWH to normalized XYXY using torchvision.ops.box_convert.\nGet the original input image size (required for box conversion).\nConvert the normalized XYXY coordinates to absolute XYXY coordinates by multiplying the x coordinates by the desired width and the y coordinates by the desired height. For example, if we want to plot our boxes on the original image, we‚Äôd use the original image dimensions of (1280, 960) (height, width).\nSort the bounding box coordinates in the same order as the scores (descending).\nCheck for equivalence between original postprocessed boxes and our manually processed boxes\n\n\n# 1. Normalize the indices by dividing by the number of classes (this is because we flattened our logits tensor in a previous step) \noutput_scores_manual_top_100_indicies_normalized = output_scores_manual_top_100_indices[0] // num_classes\noutput_scores_manual_top_100_indicies_normalized\n\ntensor([292,  56, 118,   6,   0,  50, 167,   9,  46, 111, 270,  63, 122, 182,\n         87, 243,  82, 103, 229, 208, 161, 170,  91, 179, 198, 149, 130, 160,\n        197, 259, 260, 254, 183, 195,  52, 136, 172,  69,  25, 285, 216, 156,\n        296, 215, 146,  30, 101, 154, 203, 181, 209,  14,  88, 107,  74,  83,\n        213, 106,  68,  18, 294, 237, 178, 248, 224, 109,  92,  23, 115, 284,\n        258, 175, 277,  62,  65, 212,  24, 247, 235, 249, 124, 255, 272, 273,\n        231, 276, 256,  95,  51,  38,  70, 286, 133, 265, 142, 287, 244, 217,\n         89, 139])\n\n\n\n# 2. Filter boxes for top 100 above the target threshold\noutput_boxes_manual_above_threshold_cxcywh = output_boxes_manual_cxcywh[output_scores_manual_top_100_indicies_normalized]\n\nprint(f\"[INFO] Output boxes manual above threshold shape: {output_boxes_manual_above_threshold_cxcywh.shape}\")\nprint(f\"[INFO] Output boxes manual above threshold (normalized CXCYWH format), \\\nshowing first 10:\\n{output_boxes_manual_above_threshold_cxcywh[:10, :]}\")\n\n[INFO] Output boxes manual above threshold shape: torch.Size([100, 4])\n[INFO] Output boxes manual above threshold (normalized CXCYWH format), showing first 10:\ntensor([[0.6449, 0.0949, 0.2459, 0.1891],\n        [0.6442, 0.0723, 0.2382, 0.1414],\n        [0.6637, 0.2081, 0.0501, 0.0553],\n        [0.7606, 0.0885, 0.4732, 0.1749],\n        [0.0400, 0.2540, 0.0281, 0.0325],\n        [0.7603, 0.0264, 0.4528, 0.0517],\n        [0.6853, 0.2101, 0.0924, 0.0610],\n        [0.5076, 0.0575, 0.9712, 0.1156],\n        [0.5017, 0.1211, 0.9764, 0.2471],\n        [0.5874, 0.1832, 0.1338, 0.0189]], grad_fn=&lt;SliceBackward0&gt;)\n\n\nNice!\nOkay, now let‚Äôs convert the boxes from normalized CXCYWH to normalized XYXY format using torchvision.ops.box_convert.\n\nfrom torchvision.ops import box_convert\n\n# 3. Convert the model's predicted boxes from CXCYWH to XYXY format\noutput_boxes_manual_above_threshold_xyxy = box_convert(boxes=output_boxes_manual_above_threshold_cxcywh,\n                                                       in_fmt=\"cxcywh\",\n                                                       out_fmt=\"xyxy\")\nprint(f\"[INFO] Output boxes manual above threshold (absolute XYXY format):\\n{output_boxes_manual_above_threshold_xyxy[:10]}\")\n\n[INFO] Output boxes manual above threshold (absolute XYXY format):\ntensor([[ 5.2193e-01,  3.5315e-04,  7.6779e-01,  1.8945e-01],\n        [ 5.2513e-01,  1.5671e-03,  7.6331e-01,  1.4302e-01],\n        [ 6.3864e-01,  1.8044e-01,  6.8871e-01,  2.3576e-01],\n        [ 5.2402e-01,  9.9394e-04,  9.9722e-01,  1.7591e-01],\n        [ 2.5984e-02,  2.3778e-01,  5.4088e-02,  2.7024e-01],\n        [ 5.3396e-01,  5.3773e-04,  9.8672e-01,  5.2192e-02],\n        [ 6.3906e-01,  1.7956e-01,  7.3150e-01,  2.4060e-01],\n        [ 2.2033e-02, -2.5737e-04,  9.9324e-01,  1.1535e-01],\n        [ 1.3521e-02, -2.4294e-03,  9.8993e-01,  2.4465e-01],\n        [ 5.2050e-01,  1.7379e-01,  6.5427e-01,  1.9268e-01]],\n       grad_fn=&lt;SliceBackward0&gt;)\n\n\nExcellent, we‚Äôve got our box coordinates in normalized XYXY format.\nWe could keep them here.\nBut to fully replicate the outputs of our postprocessed boxes, we‚Äôll convert them to absolute format.\nAbsolute format conversion will depend on the target size of image we‚Äôd like to use.\nFor example, if we‚Äôd like to convert our boxes to the original dimensions of our input image so we can plot them on that image, we can use the image‚Äôs original dimensions.\nTo get the original dimensions of our image we can access the orig_size attribute of our preprocessed sample.\n\n# 4. Get the original input image size (required for box conversion)\nrandom_sample_image_original_size= random_sample_preprocessed[\"labels\"][0][\"orig_size\"]\nprint(f\"[INFO] Image original size: {random_sample_image_original_size} (height, width)\")\n\n[INFO] Image original size: tensor([1280,  960]) (height, width)\n\n\nNow to convert our normalized coordinates to absolute coordinates we can multiply x coordinates by the target width and y coordinates by the target height.\n\n# 5. Convert normalized box coordinates to absolute pixel values\n\n# Get image original height and width\noriginal_height, original_width = random_sample_image_original_size\n\n# Create an XYXY tensor to multiply by\noriginal_dimensions = torch.tensor([original_width,   # x1\n                                    original_height,  # y1 \n                                    original_width,   # x2\n                                    original_height]) # y2\n\n# Convert the boxes to absolute pixel values\noutput_boxes_manual_above_threshold_xyxy_absolute = output_boxes_manual_above_threshold_xyxy * original_dimensions\noutput_boxes_manual_above_threshold_xyxy_absolute[:10]\n\ntensor([[ 5.0105e+02,  4.5203e-01,  7.3708e+02,  2.4250e+02],\n        [ 5.0412e+02,  2.0059e+00,  7.3278e+02,  1.8306e+02],\n        [ 6.1310e+02,  2.3096e+02,  6.6116e+02,  3.0177e+02],\n        [ 5.0306e+02,  1.2722e+00,  9.5733e+02,  2.2517e+02],\n        [ 2.4945e+01,  3.0436e+02,  5.1925e+01,  3.4591e+02],\n        [ 5.1260e+02,  6.8830e-01,  9.4725e+02,  6.6806e+01],\n        [ 6.1350e+02,  2.2984e+02,  7.0224e+02,  3.0797e+02],\n        [ 2.1152e+01, -3.2943e-01,  9.5351e+02,  1.4764e+02],\n        [ 1.2980e+01, -3.1096e+00,  9.5033e+02,  3.1315e+02],\n        [ 4.9968e+02,  2.2245e+02,  6.2810e+02,  2.4664e+02]],\n       grad_fn=&lt;SliceBackward0&gt;)\n\n\nAbsolute XYXY coordinates acquired!\nTime to order them in the same order as our descending scores.\n\n# 6. Order boxes in same order as labels and scores (descending based on score)\noutput_boxes_manual_sorted = output_boxes_manual_above_threshold_xyxy_absolute[output_scores_manual_filtered_indices]\noutput_boxes_manual_sorted[:10]\n\ntensor([[ 5.0105e+02,  4.5203e-01,  7.3708e+02,  2.4250e+02],\n        [ 5.0412e+02,  2.0059e+00,  7.3278e+02,  1.8306e+02],\n        [ 6.1310e+02,  2.3096e+02,  6.6116e+02,  3.0177e+02],\n        [ 5.0306e+02,  1.2722e+00,  9.5733e+02,  2.2517e+02],\n        [ 2.4945e+01,  3.0436e+02,  5.1925e+01,  3.4591e+02],\n        [ 5.1260e+02,  6.8830e-01,  9.4725e+02,  6.6806e+01],\n        [ 6.1350e+02,  2.2984e+02,  7.0224e+02,  3.0797e+02],\n        [ 2.1152e+01, -3.2943e-01,  9.5351e+02,  1.4764e+02],\n        [ 1.2980e+01, -3.1096e+00,  9.5033e+02,  3.1315e+02],\n        [ 4.9968e+02,  2.2245e+02,  6.2810e+02,  2.4664e+02]],\n       grad_fn=&lt;SliceBackward0&gt;)\n\n\nFinally, we can check to see if our manually postprocessed boxes are equivalent to original post processed boxes.\nBecause we have 100 boxes, we‚Äôll use torch.all() which checks if all elements in an input evaluate to True to make sure they‚Äôre all the same.\n\n# 7. Check for equivalence between original postprocessed boxes and our manually processed boxes\ntorch.all(input=output_boxes[:100] == output_boxes_manual_sorted)\n\ntensor(True)\n\n\nExcellent!\nWe‚Äôve now successfully converted our model‚Äôs raw outputs to postprocessed usable outputs.\nTaking the time to do steps like this helps us understand the steps taken behind the scenes for in-built postprocessing methods.\nKnowing how to do these conversion steps can also help use troubleshoot errors we may come across in the future.\n\n\n10.4 Plotting our model‚Äôs first box predictions on an image\nWe‚Äôve got some predictions, time to follow the data explorer‚Äôs motto and visualize, visualize, visualize!\nTo do so we‚Äôll:\n\nExtract the scores, labels and boxes from our random_sample_outputs_post_processed.\nCreate a list of label names to plot by mapping label IDs to class names as well as a list of colours to colour our boxes with in accordance to our colour_palette.\nDraw boxes on the image with a combination of torchvision‚Äôs pil_to_tensor, draw_bounding_boxes and to_pil_image.\n\nWe‚Äôll halve the image as well as the box coordinates using half_image and half_boxes to save space in our notebook (this is not 100% necessary, just for convenience).\n\n# 1. Extract scores, labels and boxes\nrandom_sample_pred_scores = random_sample_outputs_post_processed[0][\"scores\"]\nrandom_sample_pred_labels = random_sample_outputs_post_processed[0][\"labels\"]\nrandom_sample_pred_boxes = half_boxes(random_sample_outputs_post_processed[0][\"boxes\"])\n\n# 2. Create a list of labels and colours to plot on the image/boxes\nrandom_sample_pred_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\" \n                  for label_pred, score_pred in zip(random_sample_pred_labels, random_sample_pred_scores)]\nrandom_sample_pred_colours = [colour_palette[id2label[label_pred.item()]] for label_pred in random_sample_pred_labels]\n\nprint(f\"[INFO] Labels with scores: {random_sample_pred_labels_to_plot[:3]}...\")\n\n# 3. Plot the random sample image with randomly predicted boxes \n# (these will be very poor since the model is not trained on our data yet)\nto_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=half_image(random_sample[\"image\"])),\n        boxes=random_sample_pred_boxes, # boxes are in XYXY format, which is required for draw_bounding_boxes\n        labels=random_sample_pred_labels_to_plot,\n        colors=random_sample_pred_colours,\n        width=3\n    )\n)\n\n[INFO] Labels with scores: ['Pred: not_hand (0.6736)', 'Pred: not_hand (0.5951)', 'Pred: not_hand (0.5918)']...\n\n\n\n\n\n\n\n\n\nWoah! Those boxes don‚Äôt look good at all both the label and the coordinates look off.\nThis should be expected though‚Ä¶\nWhile our model has been pretrained on the COCO dataset, it hasn‚Äôt been trained on our specific data.\nThe good news is we can hopefully (there are no guarantees in machine learning) improve our model‚Äôs box predictions by fine-tuning it on our dataset.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#aside-bounding-box-formats-in-and-out-of-our-model",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#aside-bounding-box-formats-in-and-out-of-our-model",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "11 Aside: Bounding box formats in and out of our model",
    "text": "11 Aside: Bounding box formats in and out of our model\nWe‚Äôve done a fair bit of data transformation to get our data ready to go into our model and we‚Äôve also taken a fair few steps to postprocess it into a usable format.\nThis is often a standard practice in many machine learning workflows.\nMuch of the work before ever training a model is preparing the data for the model.\nAnd much of the work after training a model is preparing the data for your use case.\nThe following table highlights the different states our bounding boxes go in and out of.\n\n\n\nStep\nBox format\nScale\nGoes into\n\n\n\n\nStarting data (default downloaded from our Hugging Face dataset, note: not all boxes start in this format)\nXYWH or [x1, y1, width, height]\nAbsolute\npreprocess() method\n\n\nOut of preprocess()\nCXCYWH or [center_x, center_y, width, height]\nNormalized\nmodel.forward()\n\n\nOut of model.forward()\nCXCYWH or [center_x, center_y, width, height]\nNormalized\npost_process_object_detection()\n\n\nOut of post_process_object_detection()\nXYXY or [x_top_left, y_top_left, x_bottom_right, y_bottom_right]\nAbsolute (in relation to the target_sizes parameter).\nPlotting or display function.\n\n\n\n\n\n\nOur bounding boxes go through a series of format changes from input to final output. Keeping track of what format our boduning boxes are in is important for both training models and visualizing boxes on images. If we use the wrong format for plotting boxes on images, we may falsely assume our model is performing better or worse than it actually is.\n\n\nKeeping track of these input and output formats is helpful for knowing the state of your data.\nBut remember, just because our current workflow is like this, doesn‚Äôt mean all future workflows you work on will have the same transformation steps.\nFor more on different bounding box formats, see the bounding box formats guide.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#preparing-data-at-scale",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#preparing-data-at-scale",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "12 Preparing data at scale",
    "text": "12 Preparing data at scale\nWe‚Äôve performed preprocessing and postprocessing steps on a single data sample.\nHowever, in practice, we‚Äôll likely want to work with many more samples.\nOur model is hungry for more data.\nSo let‚Äôs step it up a notch and write some code that‚Äôs capable of preprocessing many samples to pass to our model.\nWe‚Äôll break it down into three subsections:\n\nSplitting the data into training, validation and test sets. We‚Äôll train our model on the training set and check its performance on the validation and test sets (our model won‚Äôt see any of these samples during training). We perform these splits before preprocessing the samples in them in case we‚Äôd like to perform different preprocessing steps depending on the split. For example, we may want to use data augmentation on the training set and not use it on the testing set.\nPreprocessing multiple samples at a time by iterating over groups of samples. Rather than preprocess a single sample at a time, we‚Äôll write code capable of processing lists of examples simultaneously.\nCollate samples into batches so our model can view multiple samples simultaneously. Rather than performing a forward pass on a single sample at a time, we‚Äôll pass batches of data to the model. For example, we may pass 32 samples (image and label pairs) at a time to our model for it to try and learn the patterns between them. We use batches of data rather than the whole dataset as it‚Äôs often much more memory efficient. If you have a really large dataset, all of your samples may not fit into memory at once, so in practice, you break it up into smaller batches of samples.\n\nLet‚Äôs start by splitting the data into different sets.\n\n12.1 Splitting the data into training and test sets\nRight now our data is all in one big group.\nHowever, it‚Äôs best practice to split our data into two (or three) different sets:\n\nTraining set (~70-80% of data) - This is the data the model will learn from, all samples in this set are seen by the model during training.\nValidation set (~5-20% of data) - This is the data we can fine-tune our model‚Äôs hyperparameters on, all samples in this set are not seen by the model during training.\nTest set (~5-20% of data) - This is the data we will evaluate what our model has learned after going through the training set, all samples in this set are not seen by the model during training.\n\nUsing the analogy of a student at univeristy, the training set would be the course materials throughout the semester, the validation set would be the practice exam and the test set would be the final exam.\nIf a student doesn‚Äôt perform well on the final exam, then we would usually say perhaps the course materials weren‚Äôt of the highest quality.\nThis is similar to our machine learning workflow.\nIn an ideal world, the samples in the training set are sufficiently representative of those in the test set and in turn, sufficiently representative of samples in the wild.\nBefore we split our dataset into different sets, let‚Äôs remind ourselves of what it looks like.\n\n# Original dataset (only a \"train\" split)\ndataset = load_dataset(path=\"mrdbourke/trashify_manual_labelled_images\")\noriginal_dataset_length = len(dataset[\"train\"])\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n        num_rows: 1128\n    })\n})\n\n\nWonderful! Right now, we‚Äôve only got one split, \"train\".\nTo make our required splits, we can call the train_test_split() method on our dataset and pass in the size of the split we‚Äôd like via the test_size parameter.\nFor example, test_size=0.3 means 30% of the data will go to the test set and 70% will go to the training set.\nWe‚Äôll make the following splits:\n\n70% of data to training set.\n~10% of data to validation set.\n~20% of data to testing set.\n\nTo do so, we‚Äôll call train_test_split() twice with different amounts:\n\nFirst on dataset[\"train\"] with test_size=0.3 to make the 70/30 training/test split, we‚Äôll save this split to the variable dataset_split.\nNext on dataset_split[\"test\"] with test_size=0.66 to make the 66/33 test/validation split, we‚Äôll set this variable to dataset_test_val_split.\n\n\n\n\nAn approximate breakdown of the different dataset splits we‚Äôre going to create. We‚Äôll start with the whole dataset and then break it into training and test splits before breaking the subsequent test split into test and validation splits. Our model will train on the training data and be evaluated on the validation and testing data.\n\n\nOnce we‚Äôve done this, we‚Äôll reassign all of the splits back to our original dataset.\nWe‚Äôll also set seed=42 for reproducibility.\nLet‚Äôs do it!\n\n# Note: Be careful of running this cell multiple times, if you do, the dataset size will get smaller. \n# If this happens, just reload the whole `dataset` as above.\n\n# 1. Split the data into \"train\" and \"test\" splits\ndataset_split = dataset[\"train\"].train_test_split(test_size=0.3, seed=42) # split the dataset into 70/30 train/test\n\n# 2. Split the test split into \"test\" and \"validation\" splits\ndataset_test_val_split = dataset_split[\"test\"].train_test_split(test_size=0.66, seed=42) # split the test set into 40/60 validation/test\n\n# Create \"train\" split from 1.\ndataset[\"train\"] = dataset_split[\"train\"]\n\n# Create a \"validation\" and \"test\" split from 2.\ndataset[\"validation\"] = dataset_test_val_split[\"train\"]\ndataset[\"test\"] = dataset_test_val_split[\"test\"]\n\n# Ensure splits lengths add to equal original dataset length (otherwise there's a mistmatch somewhere)\nassert original_dataset_length == len(dataset[\"train\"]) + len(dataset[\"validation\"]) + len(dataset[\"test\"]), \"Total dataset split lengths don't equal original dataset length, is there a mismatch? Perhaps try reloading the original dataset and re-running this cell.\"\n\n# View the dataset (now with splits)\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n        num_rows: 789\n    })\n    validation: Dataset({\n        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n        num_rows: 115\n    })\n    test: Dataset({\n        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n        num_rows: 224\n    })\n})\n\n\nPerfect!\nNow we‚Äôve got three splits of our dataset to work with.\nWe‚Äôll make sure our model never sees the validation and test splits during training, so when evaluate it we know that it‚Äôs only seeing new samples.\n\n\n12.2 Writing a function for preprocessing multiple samples at a time\nWe‚Äôve preprocessed and passed one sample through our model, new let‚Äôs do the same for multiple samples.\nWe‚Äôre going to work towards having a function that can go from a group or batch of samples (images and their annotations) and return them in preprocessed form (via transformers.RTDetrImageProcessor.preprocess) ready to be used with our model.\nLet‚Äôs first remind ourselves of what a single unprocessed sample looks like.\n\n# Get one sample from the training dataset \none_sample = dataset[\"train\"][42]\none_sample\n\n{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n 'image_id': 663,\n 'annotations': {'file_name': ['1d2ea64a-0296-403d-93cd-31e3f116c995.jpeg',\n   '1d2ea64a-0296-403d-93cd-31e3f116c995.jpeg'],\n  'image_id': [663, 663],\n  'category_id': [1, 5],\n  'bbox': [[413.29998779296875,\n    529.7000122070312,\n    343.6000061035156,\n    687.0999755859375],\n   [435.8999938964844, 463.0, 77.19999694824219, 99.9000015258789]],\n  'iscrowd': [0, 0],\n  'area': [236087.5625, 7712.27978515625]},\n 'label_source': 'manual_prodigy_label',\n 'image_source': 'manual_taken_photo'}\n\n\nAwesome, we get an image in PIL.Image.Image form as well as a single dictionary of annotations.\nHow about if we were to inspect a group of three samples?\n\n# Get three samples from the training set\ngroup_of_samples = dataset[\"train\"][0:3]\n\n# Uncomment for full output (commented for brevity)\n# group_of_samples \n\n\n\nOutput of random_samples\n\nSignature:\n{'image': [&lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n  &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n  &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;],\n 'image_id': [69, 1027, 1092],\n 'annotations': [{'file_name': ['c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg'],\n   'image_id': [69, 69, 69, 69, 69, 69, 69, 69],\n   'category_id': [5, 0, 1, 4, 4, 4, 4, 4],\n   'bbox': [[360.20001220703125, 528.5, 177.1999969482422, 261.79998779296875],\n    [298.29998779296875,\n     495.1000061035156,\n     381.1000061035156,\n     505.70001220703125],\n    [81.5999984741211,\n     592.0999755859375,\n     358.79998779296875,\n     316.29998779296875],\n    [1.2999999523162842,\n     776.7000122070312,\n     193.8000030517578,\n     211.89999389648438],\n    [301.1000061035156, 60.79999923706055, 146.89999389648438, 115.0],\n    [501.0, 75.9000015258789, 24.200000762939453, 71.19999694824219],\n    [546.4000244140625,\n     54.70000076293945,\n     130.3000030517578,\n     115.0999984741211],\n    [862.9000244140625,\n     41.099998474121094,\n     75.69999694824219,\n     80.19999694824219]],\n   'iscrowd': [0, 0, 0, 0, 0, 0, 0, 0],\n   'area': [46390.9609375,\n    192722.265625,\n    113488.4375,\n    41066.21875,\n    16893.5,\n    1723.0400390625,\n    14997.5302734375,\n    6071.14013671875]},\n  {'file_name': ['b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg',\n    'b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg',\n    'b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg',\n    'b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg',\n    'b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg'],\n   'image_id': [1027, 1027, 1027, 1027, 1027],\n   'category_id': [5, 4, 1, 0, 0],\n   'bbox': [[378.29998779296875, 657.5, 139.8000030517578, 165.10000610351562],\n    [463.29998779296875, 754.5, 39.400001525878906, 30.299999237060547],\n    [451.20001220703125,\n     734.7999877929688,\n     109.19999694824219,\n     163.8000030517578],\n    [140.39999389648438, 400.29998779296875, 460.8999938964844, 491.5],\n    [2.299999952316284,\n     322.29998779296875,\n     201.6999969482422,\n     429.20001220703125]],\n   'iscrowd': [0, 0, 0, 0, 0],\n   'area': [23080.98046875,\n    1193.8199462890625,\n    17886.9609375,\n    226532.34375,\n    86569.640625]},\n  {'file_name': ['d822c383-f53a-4a2e-b2f2-3eac55c0e515.jpeg',\n    'd822c383-f53a-4a2e-b2f2-3eac55c0e515.jpeg',\n    'd822c383-f53a-4a2e-b2f2-3eac55c0e515.jpeg',\n    'd822c383-f53a-4a2e-b2f2-3eac55c0e515.jpeg'],\n   'image_id': [1092, 1092, 1092, 1092],\n   'category_id': [2, 5, 1, 0],\n   'bbox': [[97.80000305175781, 93.30000305175781, 177.5, 101.5999984741211],\n    [342.20001220703125, 572.5999755859375, 350.0, 344.20001220703125],\n    [185.1999969482422, 803.0, 304.3999938964844, 371.6000061035156],\n    [219.39999389648438, 259.1000061035156, 598.7000122070312, 584.5]],\n   'iscrowd': [0, 0, 0, 0],\n   'area': [18034.0, 120470.0, 113115.0390625, 349940.15625]}],\n 'label_source': ['manual_prodigy_label',\n  'manual_prodigy_label',\n  'manual_prodigy_label'],\n 'image_source': ['manual_taken_photo',\n  'manual_taken_photo',\n  'manual_taken_photo']}\n\nOkay, now we get a list of image objects as well as a list of annotation dictionaries and more in the format:\n{\n    \"image\": [&lt;PIL.Image.Image&gt;, &lt;PIL.Image.Image&gt;, ...],\n    \"image_id\": [int, int, ...],\n    \"annotations\": [\n        {\n            \"file_name\": [str, str, ...],\n            \"image_id\": [int, int, ...],\n            \"category_id\": [int, int, ...],\n            \"bbox\": [[float, float, float, float], ...],\n            \"iscrowd\": [int, int, ...],\n            \"area\": [float, float, ...]\n        },\n        {...},\n        {...}\n    ],\n    \"label_source\": [str, str, ...],\n    \"image_source\": [str, str, ...]\n}\nKnowing this structure, we‚Äôll want to write a function capable of taking it as input and then preparing it for the preprocess method.\n\n\n\n\n\n\nNote\n\n\n\nOur data is in this structure (a dictionary of lists, rather than a list of dictionaries) because it is built on Hugging Face Datasets and Hugging Face Datasets is built on Apache Arrow.\nAnd Apache Arrow is column-orientated in nature.\nSo instead of our dataset being represented as many rows (list of dictionaries), it is represented as many columns (dictionary of lists).\n\n\nThe transformers.RTDetrImageProcessor.preprocess method expects a list of images as well as COCO formatted annotations as input.\nSo we‚Äôll create a function called preprocess_batch which will take a list of our raw examples and format them into a list of images and COCO formatted annotations.\nTo do so we‚Äôll:\n\nTake in a list of examples (these will be in the format above), an image_processor and optional transforms for data augmentation (we don‚Äôt need to pass these in for now but it‚Äôs good to have the option).\nCreate empty lists of images and coco_annotations we‚Äôll fill throughout the rest of the function.\nExtract the image, image_id and annotations_dict from our list of input examples.\nCreate lists of annotations attributes such as bbox, category_id and area (these are required for our format_image_annotations_as_coco function).\nOptionally perform transforms/augmentations on the image and related boxes (because in object detection if you transform an image, should transform the related boxes as well).\nConvert the annotations into COCO format using the format_image_annotations_as_coco helper function we created earlier.\nAppend the images and COCO formatted annotations to the empty lists created in 2.\nPass the list of images and COCO formatted annotations to the image_processor.preprocess method to get the preprocessed batch.\nReturn the preprocessed batch.\n\nLet‚Äôs do it!\n\n# 1. Take in a list of examples, image processor and optional transforms\ndef preprocess_batch(examples, \n                     image_processor,\n                     transforms=None, # Note: Could optionally add transforms (e.g. data augmentation) here \n                     ):\n    \"\"\"\n    Preprocesses a batch of image data with annotations for object detection models.\n\n    This function takes a batch of examples in a custom dataset format, extracts images and\n    their corresponding annotations, and converts them into a format suitable for model training\n    or inference using the provided image processor.\n\n    Args:\n        examples (dict): A dictionary containing the batch data with the following structure:\n            - \"image\" (List[PIL.Image.Image]): List of PIL Image objects\n            - \"image_id\" (List[int]): List of unique image identifiers\n            - \"annotations\" (List[dict]): List of annotation dictionaries, where each contains:\n                - \"file_name\" (List[str]): List of image filenames\n                - \"image_id\" (List[int]): List of image identifiers\n                - \"category_id\" (List[int]): List of object category IDs\n                - \"bbox\" (List[List[float]]): List of bounding boxes as [x, y, width, height]\n                - \"iscrowd\" (List[int]): List of crowd indicators (0 or 1)\n                - \"area\" (List[float]): List of object areas\n            - \"label_source\" (List[str]): List of label sources\n            - \"image_source\" (List[str]): List of image sources\n\n        image_processor: An image processor object to preprocess images for model input.\n            For example, can be `transformers.RTDetrDetrImageProcessor`.\n\n        transforms (optional): Image and annotations transforms for data augmentation.\n            Defaults to None.\n\n    Returns:\n        dict: Preprocessed batch with images and annotations converted to tensors\n            in the format required for a `transformers.RTDetrV2ForObjectDetection` model.\n\n    Note:\n        The `format_image_annotations_as_coco` function converts the input annotation format to COCO\n        format before applying the image_processor. This is required as the image_processor is designed\n        to handle COCO format annotations. \n    \"\"\"\n    # 2. Create empty lists to store images and annotations\n    images = []\n    coco_annotations = [] \n\n    # 3. Extract the image, image_id and annotations from the examples\n    for image, image_id, annotations_dict in zip(examples[\"image\"], \n                                                 examples[\"image_id\"], \n                                                 examples[\"annotations\"]):\n\n        # 4. Create lists of annotation attributes\n        bbox_list = annotations_dict[\"bbox\"]\n        category_list = annotations_dict[\"category_id\"]\n        area_list = annotations_dict[\"area\"]\n\n        ###\n        # 5. Note: Could optionally apply a transform/augmentation here.\n        # See PyTorch docs for more: https://docs.pytorch.org/vision/main/auto_examples/transforms/plot_transforms_e2e.html\n        if transforms:\n            # Perform transform on image/boxes\n            pass\n        ###\n\n        # 6. Format the annotations into COCO format\n        coco_format_annotations = format_image_annotations_as_coco(image_id=image_id,\n                                                                   categories=category_list,\n                                                                   areas=area_list,\n                                                                   bboxes=bbox_list)\n        \n        # 7. Add images/annotations to their respective lists\n        images.append(image) # Note: may need to open image if it is an image path rather than PIL.Image\n        coco_annotations.append(coco_format_annotations)\n\n    \n    # 8. Apply the image processor to lists of images and annotations\n    preprocessed_batch = image_processor.preprocess(images=images,\n                                                    annotations=coco_annotations,\n                                                    return_tensors=\"pt\")\n\n    # 9. Return the preprocessed batch\n    return preprocessed_batch\n\nNice!\nNow how about we test it out on our group_of_samples?\n\npreprocessed_samples = preprocess_batch(examples=group_of_samples,\n                                        image_processor=image_processor)\n\npreprocessed_samples.keys()\n\ndict_keys(['pixel_mask', 'pixel_values', 'labels'])\n\n\nPerfect, we get the same keys() as with our single sample.\nExcept this time, we‚Äôve got multiple samples, let‚Äôs check the shape.\n\n# Check the shape of our preprocessed samples\nprint(f\"[INFO] Shape of preprocessed samples: {preprocessed_samples['pixel_values'].shape} -&gt; [batch_size, colour_channels, height, width]\")\n\n[INFO] Shape of preprocessed samples: torch.Size([3, 3, 640, 480]) -&gt; [batch_size, colour_channels, height, width]\n\n\nWonderful, our batch of three samples have been preprocessed and are ready for input to our model.\n\n\n12.3 Applying our preprocessing function to each data split\nWe‚Äôve seen our preprocess_batch function in action on a small group of samples.\nNow let‚Äôs apply it to our different data splits.\nTo do so, we can call the datasets.Dataset.with_transform() method on our target dataset split and pass it our desired transform.\nUsing with_transform() means our transformations will be applied on-the-fly when we call on our split datasets.\nBecause the with_transform() method expects a callable with a single argument (the input examples), we‚Äôll turn our preprocess_batch into a Python partial function.\nDoing this will mean we can prefill the image_processor and optionally the transforms parameter of our preprocess_batch function meaning it will only take examples as input, this is inline with the with_transform() method.\n\n# Create a partial function for preprocessing\nfrom functools import partial\n\n# Note: Could create separate preprocess functions with different inputs depending on the split \n# (e.g. use data augmentation on training but not on validation/test)\npreprocess_batch_partial = partial(preprocess_batch,\n                                   image_processor=image_processor,\n                                   transforms=None) # could use transforms here if wanted\n\n# Inspect the preprocess_batch_partial function\npreprocess_batch_partial\n\nBeautiful, now let‚Äôs pass the preprocess_batch_partial function to the with_transform() method on each of our data splits.\n\n# Create a copy of the original dataset \n# (we don't need to do this, this is just so we can inspect the original dataset later on)\nprocessed_dataset = dataset.copy()\n\n# Apply the preprocessing function to the datasets (the preprocessing will happen on the fly, e.g. when the dataset is called rather than in-place)\nprocessed_dataset[\"train\"] = dataset[\"train\"].with_transform(transform=preprocess_batch_partial)\nprocessed_dataset[\"validation\"] = dataset[\"validation\"].with_transform(transform=preprocess_batch_partial)\nprocessed_dataset[\"test\"] = dataset[\"test\"].with_transform(transform=preprocess_batch_partial)\n\nNow when we get (via __getitem__) one of our samples from a processed_dataset split, it will be preprocessed on the fly.\n\n# Get an item from the dataset (in will be preprocessed as we get it)\nprocessed_dataset[\"train\"][42]\n\n{'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]]),\n 'pixel_values': tensor([[[0.0824, 0.2275, 0.2471,  ..., 0.3255, 0.3059, 0.3804],\n          [0.2588, 0.1608, 0.2706,  ..., 0.4000, 0.4588, 0.4667],\n          [0.2706, 0.2588, 0.2549,  ..., 0.7059, 0.5686, 0.4431],\n          ...,\n          [0.4941, 0.3137, 0.2235,  ..., 0.2745, 0.2314, 0.1647],\n          [0.4824, 0.5490, 0.2392,  ..., 0.1725, 0.1451, 0.2157],\n          [0.3176, 0.5294, 0.3137,  ..., 0.2039, 0.1059, 0.1490]],\n \n         [[0.0941, 0.2392, 0.2549,  ..., 0.3176, 0.2941, 0.3765],\n          [0.2706, 0.1686, 0.2784,  ..., 0.3922, 0.4471, 0.4588],\n          [0.2784, 0.2667, 0.2588,  ..., 0.6980, 0.5569, 0.4353],\n          ...,\n          [0.4667, 0.2824, 0.1882,  ..., 0.2902, 0.2549, 0.2000],\n          [0.4510, 0.5098, 0.2000,  ..., 0.1922, 0.1843, 0.2588],\n          [0.2824, 0.4902, 0.2706,  ..., 0.2353, 0.1529, 0.2000]],\n \n         [[0.0353, 0.1725, 0.1647,  ..., 0.1686, 0.1373, 0.1804],\n          [0.1882, 0.1020, 0.1725,  ..., 0.2353, 0.2824, 0.2667],\n          [0.1922, 0.1804, 0.1490,  ..., 0.5412, 0.3804, 0.2471],\n          ...,\n          [0.3137, 0.1922, 0.1255,  ..., 0.1451, 0.1333, 0.0745],\n          [0.2863, 0.3922, 0.1333,  ..., 0.0667, 0.0549, 0.1137],\n          [0.1373, 0.3490, 0.2000,  ..., 0.0863, 0.0118, 0.0510]]]),\n 'labels': {'size': tensor([640, 480]), 'image_id': tensor([663]), 'class_labels': tensor([1, 5]), 'boxes': tensor([[0.6095, 0.6822, 0.3579, 0.5368],\n         [0.4943, 0.4007, 0.0804, 0.0780]]), 'area': tensor([59021.8906,  1928.0699]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}}\n\n\nAnd the same happens when we get multiple (a batch) samples!\n\n# Now when we call one or more of our samples, the preprocessing will take place\nbatch_size_to_get = 32\nprint(f\"[INFO] Shape of preprocessed images: {processed_dataset['train'][:batch_size_to_get]['pixel_values'].shape} -&gt; [batch_size, colour_channels, height, width]\")\n\n[INFO] Shape of preprocessed images: torch.Size([32, 3, 640, 480]) -&gt; [batch_size, colour_channels, height, width]\n\n\n\n# We can pass these straight to our model! (note: may take a while if it's on CPU)\n# model(processed_dataset[\"train\"][:batch_size_to_get][\"pixel_values\"]) # uncomment to view output\n\n\n\n12.4 Creating a collation function\nWe can now preprocess multiple samples at once.\nTime to create a collation function which will tell our model trainer how to stack these samples together into batches.\nWe do this because processing more samples at once (e.g.¬†16, 32 or 128 samples in a batch) in a batch is generally more efficient than one sample at a time or trying to process all samples at once.\nOur collation function will be used for the data_collator parameter in our transformers.Trainer instance later on.\nThe input to our data collation function will be the output of image_processor.preprocess() (a preprocessed sample).\nAnd the output will be passed as a batch (we‚Äôll define the batch size later on) to our model‚Äôs forward() method.\n\n\n\n\n\n\nNote\n\n\n\nWhat batch size should I use?\nYou should generally use the batch size which uses the maximum amount of GPU memory you have.\nFor example, if you have 16GB of GPU memory and a batch size of 32 only uses 8GB of that memory, you should try doubling the batch size to 64.\nThe ideal batch size for a given dataset/model/hardware is often discovered in an iterative process.\n\n\n\nfrom typing import List, Dict, Any\n\ndef data_collate_function(preprocessed_batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n    \"\"\"Stacks together groups of preprocessed samples into batches for our model.\n\n    Args:\n        preprocessed_batch: A list of dictionaries where each dictionary represnets a preprocessed sample.\n\n    Returns:\n        collated_data: A dictionary containing the batched data ready in the format our model\n            is expecting. The dictionary has the following keys: \n                - \"pixel_values\": A stacked tensor of preprocessed pixel values.\n                - \"labels\": A list of label dictionaries.\n                - \"pixel_mask\": (Optional) A stacked tensor of pixel masks (this will be present \n                    only if the input contains a \"pixel_mask\" key.\n    \"\"\"\n    # Create an empty dictionary (our model wants a dictionary input) \n    collated_data = {} \n\n    # Stack together a collection of pixel_values tensors\n    collated_data[\"pixel_values\"] = torch.stack([sample[\"pixel_values\"] for sample in preprocessed_batch])\n\n    # Get the labels (these are dictionaries so no need to use torch.stack)\n    collated_data[\"labels\"] = [sample[\"labels\"] for sample in preprocessed_batch]\n\n    # If there is a pixel_mask key, return the pixel_mask's as well\n    if \"pixel_mask\" in preprocessed_batch[0]:\n        collated_data[\"pixel_mask\"] = torch.stack([sample[\"pixel_mask\"] for sample in preprocessed_batch])\n\n    return collated_data\n\nExcellent! Now let‚Äôs try out our data collation function.\n\n%%time\n\n# Try data_collate_function \nexample_collated_data_batch = data_collate_function(processed_dataset[\"train\"].select(range(32)))\nexample_collated_data_batch.keys()\n\nCPU times: user 9.67 s, sys: 319 ms, total: 9.99 s\nWall time: 865 ms\n\n\ndict_keys(['pixel_values', 'labels', 'pixel_mask'])\n\n\nPerfect! Looks like it worked. We‚Äôve now got a batch of preprocessed images and label pairs.\nLet‚Äôs check the shapes.\n\n# Check shapes of batched preprocessed samples\nprint(f\"[INFO] Batch of pixel value shapes: {example_collated_data_batch['pixel_values'].shape}\")\nprint(f\"[INFO] Batch of labels: {example_collated_data_batch['labels']}\")\nif \"pixel_mask\" in example_collated_data_batch:\n    print(f\"[INFO] Batch of pixel masks: {example_collated_data_batch['pixel_mask'].shape}\")\n\n[INFO] Batch of pixel value shapes: torch.Size([32, 3, 640, 480])\n[INFO] Batch of labels: [{'size': tensor([640, 480]), 'image_id': tensor([69]), 'class_labels': tensor([5, 0, 1, 4, 4, 4, 4, 4]), 'boxes': tensor([[0.4675, 0.5152, 0.1846, 0.2045],\n        [0.5092, 0.5843, 0.3970, 0.3951],\n        [0.2719, 0.5861, 0.3738, 0.2471],\n        [0.1023, 0.6896, 0.2019, 0.1655],\n        [0.3902, 0.0924, 0.1530, 0.0898],\n        [0.5345, 0.0871, 0.0252, 0.0556],\n        [0.6370, 0.0877, 0.1357, 0.0899],\n        [0.9383, 0.0634, 0.0789, 0.0627]]), 'area': tensor([11597.7402, 48180.5664, 28372.1094, 10266.5547,  4223.3750,   430.7600,\n         3749.3826,  1517.7850]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1027]), 'class_labels': tensor([5, 4, 1, 0, 0]), 'boxes': tensor([[0.4669, 0.5782, 0.1456, 0.1290],\n        [0.5031, 0.6013, 0.0410, 0.0237],\n        [0.5269, 0.6380, 0.1138, 0.1280],\n        [0.3863, 0.5047, 0.4801, 0.3840],\n        [0.1074, 0.4195, 0.2101, 0.3353]]), 'area': tensor([ 5770.2451,   298.4550,  4471.7402, 56633.0859, 21642.4102]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1092]), 'class_labels': tensor([2, 5, 1, 0]), 'boxes': tensor([[0.1943, 0.1126, 0.1849, 0.0794],\n        [0.5387, 0.5818, 0.3646, 0.2689],\n        [0.3515, 0.7725, 0.3171, 0.2903],\n        [0.5404, 0.4307, 0.6236, 0.4566]]), 'area': tensor([ 4508.5000, 30117.5000, 28278.7598, 87485.0391]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([228]), 'class_labels': tensor([0]), 'boxes': tensor([[0.5187, 0.5418, 0.4982, 0.5698]]), 'area': tensor([87218.0078]), 'iscrowd': tensor([0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([511]), 'class_labels': tensor([5, 1]), 'boxes': tensor([[0.5284, 0.5886, 0.2903, 0.3347],\n        [0.7784, 0.7873, 0.4400, 0.4222]]), 'area': tensor([29848.7695, 57066.2383]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([338]), 'class_labels': tensor([5, 0, 1]), 'boxes': tensor([[0.4990, 0.5424, 0.2227, 0.1716],\n        [0.5455, 0.5335, 0.3754, 0.3595],\n        [0.7111, 0.6979, 0.3313, 0.2838]]), 'area': tensor([11742.9648, 41455.0117, 28882.3496]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([405]), 'class_labels': tensor([0, 1, 5]), 'boxes': tensor([[0.4952, 0.6559, 0.6088, 0.4872],\n        [0.2074, 0.7760, 0.4117, 0.4459],\n        [0.4132, 0.5714, 0.0663, 0.0580]]), 'area': tensor([91107.9609, 56385.1602,  1179.7800]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([3]), 'class_labels': tensor([0, 5, 1, 4, 4, 4]), 'boxes': tensor([[0.5020, 0.4466, 0.6579, 0.5829],\n        [0.5148, 0.5684, 0.2288, 0.1367],\n        [0.7040, 0.7836, 0.4468, 0.4219],\n        [0.3160, 0.8416, 0.3991, 0.2993],\n        [0.4095, 0.0661, 0.0888, 0.0666],\n        [0.7489, 0.1356, 0.3843, 0.2637]]), 'area': tensor([117809.1875,   9607.5000,  57901.5000,  36691.4023,   1814.7600,\n         31125.9375]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([182]), 'class_labels': tensor([0, 1, 5]), 'boxes': tensor([[0.5786, 0.5016, 0.5992, 0.4539],\n        [0.6307, 0.7197, 0.4165, 0.3323],\n        [0.4415, 0.6429, 0.1546, 0.2070]]), 'area': tensor([83547.7969, 42508.7344,  9827.7900]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([640]), 'class_labels': tensor([5, 1, 0]), 'boxes': tensor([[0.5314, 0.6391, 0.2920, 0.4553],\n        [0.7088, 0.7733, 0.5596, 0.4422],\n        [0.5282, 0.5060, 0.5678, 0.4612]]), 'area': tensor([40839.7109, 76013.7969, 80443.1328]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1126]), 'class_labels': tensor([5, 1, 0, 0, 4]), 'boxes': tensor([[0.4897, 0.6114, 0.2720, 0.2612],\n        [0.6082, 0.7287, 0.2006, 0.2145],\n        [0.4549, 0.5349, 0.4550, 0.3859],\n        [0.1698, 0.4514, 0.3276, 0.2998],\n        [0.6611, 0.1925, 0.4202, 0.1516]]), 'area': tensor([21821.4316, 13217.1748, 53944.8008, 30168.4121, 19574.9844]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([902]), 'class_labels': tensor([5, 1, 0, 4]), 'boxes': tensor([[0.5237, 0.4816, 0.0443, 0.0498],\n        [0.6509, 0.3957, 0.2670, 0.1695],\n        [0.3200, 0.4485, 0.6094, 0.6062],\n        [0.6201, 0.1730, 0.1955, 0.0725]]), 'area': tensor([   676.8125,  13904.2754, 113490.0000,   4354.6401]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([166]), 'class_labels': tensor([5, 1, 0, 4, 0]), 'boxes': tensor([[0.4320, 0.5441, 0.2114, 0.1963],\n        [0.2735, 0.6612, 0.3580, 0.2412],\n        [0.5321, 0.5080, 0.3639, 0.3277],\n        [0.1142, 0.7866, 0.2067, 0.1561],\n        [0.7246, 0.4182, 0.2477, 0.2401]]), 'area': tensor([12742.1201, 26533.6406, 36624.1055,  9910.0801, 18268.9844]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([409]), 'class_labels': tensor([0, 4, 4, 5, 1]), 'boxes': tensor([[0.3715, 0.6465, 0.7429, 0.5014],\n        [0.5047, 0.6748, 0.2114, 0.1916],\n        [0.1167, 0.7180, 0.2303, 0.1904],\n        [0.4180, 0.6086, 0.0883, 0.0780],\n        [0.3020, 0.6926, 0.3045, 0.2649]]), 'area': tensor([114432.9375,  12437.7695,  13470.5176,   2117.8799,  24779.7324]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([504]), 'class_labels': tensor([1, 0]), 'boxes': tensor([[0.2105, 0.6075, 0.3550, 0.2591],\n        [0.4267, 0.5508, 0.5474, 0.3703]]), 'area': tensor([28260.8398, 62271.7500]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1012]), 'class_labels': tensor([0, 2]), 'boxes': tensor([[0.4518, 0.4870, 0.5355, 0.5652],\n        [0.9084, 0.5812, 0.1724, 0.4217]]), 'area': tensor([92987.8359, 22334.2246]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([769]), 'class_labels': tensor([6, 5, 0, 2]), 'boxes': tensor([[0.7015, 0.4236, 0.5892, 0.0759],\n        [0.4368, 0.4307, 0.1043, 0.1327],\n        [0.2781, 0.5959, 0.3932, 0.4465],\n        [0.6999, 0.3721, 0.5797, 0.7238]]), 'area': tensor([ 13744.0801,   4249.2451,  53935.3125, 128899.3125]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([510]), 'class_labels': tensor([5, 1]), 'boxes': tensor([[0.3557, 0.4248, 0.2382, 0.1798],\n        [0.6917, 0.7145, 0.6135, 0.5677]]), 'area': tensor([ 13155.9678, 106991.8516]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([429]), 'class_labels': tensor([4, 0, 1, 5]), 'boxes': tensor([[0.4661, 0.8003, 0.4432, 0.1715],\n        [0.4992, 0.6146, 0.9984, 0.6917],\n        [0.2310, 0.6193, 0.3612, 0.2520],\n        [0.4227, 0.5342, 0.0790, 0.0650]]), 'area': tensor([ 23349.3125, 212163.9688,  27969.4199,   1576.6400]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([714]), 'class_labels': tensor([5, 1, 0]), 'boxes': tensor([[0.3350, 0.6024, 0.2067, 0.2968],\n        [0.2292, 0.7662, 0.4445, 0.4472],\n        [0.5794, 0.6870, 0.6228, 0.5439]]), 'area': tensor([ 18843.0391,  61060.7695, 104064.4922]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([301]), 'class_labels': tensor([5, 1, 0]), 'boxes': tensor([[0.4706, 0.5429, 0.0994, 0.0970],\n        [0.2963, 0.6009, 0.3128, 0.2155],\n        [0.4525, 0.4761, 0.8737, 0.6209]]), 'area': tensor([  2959.7849,  20713.1934, 166669.5625]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([816]), 'class_labels': tensor([6, 5, 0]), 'boxes': tensor([[0.7607, 0.7381, 0.4707, 0.3945],\n        [0.5418, 0.5427, 0.1593, 0.1055],\n        [0.4945, 0.5723, 0.5662, 0.4344]]), 'area': tensor([57052.3750,  5160.3750, 75560.3984]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([392]), 'class_labels': tensor([5, 1, 4, 4, 4]), 'boxes': tensor([[0.4599, 0.6063, 0.0836, 0.0493],\n        [0.2533, 0.7866, 0.5063, 0.4221],\n        [0.5349, 0.6495, 0.7540, 0.5713],\n        [0.8369, 0.9173, 0.3234, 0.1632],\n        [0.5333, 0.9232, 0.1924, 0.1514]]), 'area': tensor([  1266.7325,  65646.4531, 132310.6406,  16215.8623,   8948.7148]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([439]), 'class_labels': tensor([5, 0, 1]), 'boxes': tensor([[0.4638, 0.6099, 0.2429, 0.3724],\n        [0.4283, 0.5034, 0.4528, 0.3891],\n        [0.7492, 0.6229, 0.4982, 0.4316]]), 'area': tensor([27791.6094, 54120.1484, 66053.2266]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([319]), 'class_labels': tensor([5, 1, 0, 4, 4]), 'boxes': tensor([[0.4927, 0.4708, 0.1688, 0.0946],\n        [0.7135, 0.5453, 0.3644, 0.2980],\n        [0.4998, 0.5359, 0.6276, 0.4492],\n        [0.5456, 0.8173, 0.1482, 0.1584],\n        [0.4667, 0.9237, 0.1009, 0.1277]]), 'area': tensor([ 4904.5498, 33353.4297, 86609.3750,  7214.6099,  3960.7876]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1106]), 'class_labels': tensor([5, 1, 4, 0]), 'boxes': tensor([[0.4597, 0.4787, 0.1184, 0.0961],\n        [0.5932, 0.6244, 0.2401, 0.2405],\n        [0.6587, 0.7589, 0.2219, 0.1490],\n        [0.3902, 0.5373, 0.7309, 0.5996]]), 'area': tensor([  3496.2749,  17742.7383,  10154.7754, 134638.6875]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([763]), 'class_labels': tensor([0, 0, 0, 5, 1]), 'boxes': tensor([[0.4510, 0.5231, 0.5637, 0.4548],\n        [0.7868, 0.4366, 0.4092, 0.3365],\n        [0.2204, 0.4396, 0.3318, 0.3187],\n        [0.5497, 0.5397, 0.2101, 0.0714],\n        [0.6421, 0.6682, 0.3070, 0.2901]]), 'area': tensor([78758.1328, 42294.7383, 32479.0371,  4608.8452, 27355.5273]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([379]), 'class_labels': tensor([5, 1]), 'boxes': tensor([[0.5053, 0.5406, 0.5852, 0.7876],\n        [0.7293, 0.6370, 0.5284, 0.4556]]), 'area': tensor([141587.6406,  73964.3438]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([394]), 'class_labels': tensor([1, 5, 0]), 'boxes': tensor([[0.2053, 0.7470, 0.4101, 0.4966],\n        [0.4299, 0.5713, 0.1728, 0.0933],\n        [0.4994, 0.6560, 0.9984, 0.6693]]), 'area': tensor([ 62568.7734,   4952.1152, 205286.7344]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([665]), 'class_labels': tensor([0, 2]), 'boxes': tensor([[0.5282, 0.6071, 0.4164, 0.3630],\n        [0.6520, 0.8419, 0.5095, 0.2905]]), 'area': tensor([46425.1562, 45461.8438]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([362]), 'class_labels': tensor([5, 1, 0]), 'boxes': tensor([[0.4643, 0.5164, 0.3293, 0.3087],\n        [0.6197, 0.7712, 0.7412, 0.4446],\n        [0.4982, 0.5305, 0.9742, 0.8731]]), 'area': tensor([ 31222.7773, 101242.8906, 261294.8750]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1019]), 'class_labels': tensor([5, 1, 0]), 'boxes': tensor([[0.4699, 0.5841, 0.2358, 0.3263],\n        [0.5916, 0.6374, 0.2653, 0.2050],\n        [0.4858, 0.5195, 0.6066, 0.5119]]), 'area': tensor([23641.8203, 16708.3203, 95380.7422]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}]\n[INFO] Batch of pixel masks: torch.Size([32, 640, 480])\n\n\nNow let‚Äôs try to pass the \"pixel_values\" through our model.\n\n%%time \n\n# Try pass a batch through our model (note: this will be relatively slow if our model is on the CPU)\nmodel = create_model()\n\n# example_batch_outputs = model(example_collated_data_batch[\"pixel_values\"])\nexample_batch_outputs = model(example_collated_data_batch[\"pixel_values\"])\n# example_batch_outputs # uncomment for full output\nexample_batch_outputs.keys()\n\nSome weights of RTDetrV2ForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_v2_r50vd and are newly initialized because the shapes did not match:\n- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([8, 256]) in the model instantiated\n- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nCPU times: user 1min 44s, sys: 1min 7s, total: 2min 52s\nWall time: 14.2 s\n\n\nodict_keys(['logits', 'pred_boxes', 'last_hidden_state', 'intermediate_hidden_states', 'intermediate_logits', 'intermediate_reference_points', 'encoder_last_hidden_state', 'init_reference_points', 'enc_topk_logits', 'enc_topk_bboxes', 'enc_outputs_class', 'enc_outputs_coord_logits'])\n\n\n\n# We get 300 predictions per image in our batch, each with a logit value for each of the classes in our dataset \nexample_batch_outputs.logits.shape\n\ntorch.Size([32, 300, 7])\n\n\nThis is what will happen during training, our model will continually go over batches (the size of these batches will be defined by us) over data and try to match its own predictions with the ground truth labels.\nIn summary, we‚Äôve created two major steps:\n\npreprocess_batch - Preprocesses single or groups of samples into the specific format required by our model.\ndata_collate_function - Stacks together groups/batches of samples to be passed to our model‚Äôs forward() method.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#setting-up-trainingarguments-and-a-trainer-instance-to-train-our-model",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#setting-up-trainingarguments-and-a-trainer-instance-to-train-our-model",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "13 Setting up TrainingArguments and a Trainer instance to train our model",
    "text": "13 Setting up TrainingArguments and a Trainer instance to train our model\nData ready and prepared, time to train a model!\nWe‚Äôll use transformers.TrainingArguments to set various hyperparameters for our model (many of these will be set by default, however, we can tweak them to our liking).\nWe‚Äôll also create an instance of transformers.Trainer which we can pass our preprocessed datasets for it to train/evaluate on.\nTo train a model, we‚Äôll go through the following steps:\n\nCreate a fresh instance of our model using the create_model() function.\nMake a directory for saving our trained models to.\nDefine our model‚Äôs hyperparameters using transformers.TrainingArguments, we‚Äôll take many of these settings from the assosciated research papers that introduced our model.\nOptional: Create a custom instance of transformers.Trainer to use a custom optimizer with different learning rates for different parameters (similar to the research papers).\nCreate an evaluation function we can pass to our transformers.Trainer instance as the compute_metrics parameter to evaluate our model.\nCreate an instance of transformers.Trainer and pass it our training arguments from 2 as well as our preprocessed data.\nCall transformers.Trainer.train() to train the model from 1 on our own data.\nPlot our model‚Äôs loss curves to see how it performed throughout training (ideally our model‚Äôs loss goes down and its evaluation metric goes up).\n\nLet‚Äôs do it!\n\n# 1. Create a model instance \nmodel = create_model()\n\nSome weights of RTDetrV2ForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_v2_r50vd and are newly initialized because the shapes did not match:\n- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([8, 256]) in the model instantiated\n- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated\n- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nModel ready, let‚Äôs now create a folder where we can save our trained models to.\n\nfrom pathlib import Path\n\n# 2. Make a models directory for saving models\nmodels_dir = Path(\"models\")\nmodels_dir.mkdir(parents=True, exist_ok=True)\n\nPerfect! Time to setup our model‚Äôs hyperparameters with transformers.TrainingArguments.\n\n13.1 Setting up our TrainingArguments\nThe transformers.TrainingArguments class holds many of the hyperparameters/settings for training our model.\nMany of them are set by default in the transformers.RTDetrV2Config class.\nHowever, we can tweak any of them to our own liking.\nWhere do we get the settings from?\nThe original RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer paper states that all hyperparameters are the same as the original RT-DETR (see Table A in DETRs Beat YOLOs on Real-time Object Detection).\nThe main hyperparameters we are going to set are:\n\n\n\nHyperparameter\nValue\nWhat does it do?\n\n\n\n\nper_device_train_batch_size, per_device_eval_batch_size\n16, 32 or larger (hardware dependent)\nDefines the number of samples passed to our model at one time. For example, if batch size is 16, our model will see 16 samples at a time. It‚Äôs usually best practice to set this value to the highest your hardware can handle.\n\n\nlearning_rate\n0.0001 (as per the listed papers)\nDefines the multiplier on the size of gradient updates during training. Too high and gradients will explode, too low and gradients won‚Äôt update, both lead to poor training results. The papers mention two different learning rates for the backbone and the detection head, I tried these and got poor results (likely because of our smaller dataset), a single learning rate for the whole network turned out to be better.\n\n\nweight_decay\n0.0001 (as per the listed papers)\nPrevents model weights from getting too large by applying a small decay penalty over time. This prevents a single weight providing too much information. In essence, the model is forced to learn smaller, simpler weights to represent the data. A form of regularization (overfitting prevention). See more at paperswithcode.com/method/weight-decay.\n\n\nmax_grad_norm\n0.1 (as per the listed papers)\nPrevents gradients from getting too large during training. This will help to ensure stable training. See more at paperswithcode.com/method/gradient-clipping.\n\n\nnum_train_epochs\n10 (depends on training data and available time)\nDefines how many laps of the data your model will do. For example, setting epochs to 25 means the model will do 25 laps of the training data to learn different patterns. In practice, I‚Äôve found this value to be a good starting point for our dataset and also because we are fine-tuning rather than training from scratch. However, if you had more data you might want to do more epochs (when training from scratch, the papers did 300 epochs).\n\n\nwarmup_ratio\n0.05\nPercentage of total training steps to take learning rate from 0 to to the set value (e.g.¬†0.0001). Can help with training stability in the early training steps of the model by not doing too large updates when first starting out. The papers state 2000 warmup steps, however, in practice I found this to be too many for our smaller dataset.\n\n\ndataloader_num_workers\n4 (hardware dependent)\nNumber of workers to load data from the CPU to the GPU. Higher is generally better if it is available, however, it can often cap out. Experimentally I‚Äôve found that 0.5 * os.cpu_count() generally works well.\n\n\n\n\n\n\nDifferent hyperparameter settings from the official papers for the RT-DETR model (left) and the original DETR model (right).\n\n\nIt‚Äôs important to note that all of these values can be experimented with.\nAnd just because a research paper mentions a specific value, doesn‚Äôt mean you have to use.\nFor example, all the mentioned research papers tend to focus on training a model from scratch on the COCO dataset (330k images, 80 classes).\nWhich is a much larger dataset with more classes than our dataset (1k images, 7 classes) which we are trying to fine-tune an existing model on rather than train from scratch.\nThere are many more possible arguments/settings we‚Äôve left out in the above table but if you‚Äôd like to explore these, I‚Äôd encourage you to check out the documentation for transformers.TrainingArguments.\n\n# 3. Create an instance of TrainingArguments to pass to Trainer\nfrom transformers import TrainingArguments\n\n# Hardware dependent hyperparameters\n# Set the batch size according to the memory you have available on your GPU\n# e.g. on my NVIDIA RTX 4090 with 24GB of VRAM, I can use a batch size of 32 \n# without running out of memory\nBATCH_SIZE = 16\nDATALOADER_NUM_WORKERS = 4 # note: if you're on Google Colab, you may have to lower this to os.cpu_count() or to 0 \n\n# Set number of epochs to how many laps you'd like to do over the data\nNUM_EPOCHS = 10\n\n# Setup hyperameters for training from the DETR paper(s)\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 1e-4\nMAX_GRAD_NORM = 0.1 \nWARMUP_RATIO = 0.05 # learning rate warmup from 0 to learning_rate as a ratio of total steps (e.g. 0.05 = 5% of total steps)\n\n# Create directory to save models to \nOUTPUT_DIR = Path(models_dir, \"rt_detrv2_finetuned_trashify_box_detector_v1\")\nprint(f\"[INFO] Saving model to: {OUTPUT_DIR}\")\n\n# Create TrainingArguments to pass to Trainer\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    learning_rate=LEARNING_RATE,\n    weight_decay=WEIGHT_DECAY,\n    max_grad_norm=MAX_GRAD_NORM,\n    num_train_epochs=NUM_EPOCHS,\n    lr_scheduler_type=\"linear\",\n    warmup_ratio=WARMUP_RATIO, \n    # warmup_steps=2000, # number of warmup steps from 0 to learning_rate (overrides warmup_ratio, found this to be too long for our dataset)\n    logging_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    fp16=True, # use mixed precision training\n    dataloader_num_workers=DATALOADER_NUM_WORKERS, # note: if you're on Google Colab, you may have to lower this to os.cpu_count() or to 0\n    eval_strategy=\"epoch\",\n    load_best_model_at_end=True, \n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False, # want to minimize eval_loss (e.g. lower is better)\n    report_to=\"none\", # don't save experiments to a third party service\n    push_to_hub=False,\n    eval_do_concat_batches=False, # this defaults to True but we'll set it to False for our evaluation function\n    # save_safetensors=False # turn this off to prevent potential checkpoint issues\n)\n\n[INFO] Saving model to: models/rt_detrv2_finetuned_trashify_box_detector_v1\n\n\n\n\n13.2 Optional: Setting up an optimizer for multiple learning rates\nWe‚Äôre using PekingU/rtdetr_v2_r50vd from Hugging Face and in the paper that introduced it, they state that they used a different learning rate value for the backbone, learning_rate=1e-5, as well as the object detection head, learning_rate=1e-4 (see Table 1 for RT-DETRv2-L with the ResNet50 backbone in the RT-DETRv2 paper).\n\n\n\nDifferent learning rates used for different sections of the model from the RT-DETRv2 paper. The backbone uses a slightly lower learning rate than the detection head. For more on the ResNet backbones, see the original ResNet Paper.\n\n\nTo set this up ourselves, we can extract which parameters of our model belong to the ResNet50 backbone as well as which don‚Äôt.\nTo find the backbone parameters, we can loop through our model‚Äôs named_parameters() method and filter for any which contain the string \"backbone\" in their name.\nWe‚Äôll append these to a list called backbone_parameters and assume any that don‚Äôt have \"backbone\" in their name are not part of the model‚Äôs backbone.\nWe can use these two lists of parameters to pass to torch.optim.AdamW with different learning rate values for each.\n\n\n\n\n\n\nNote\n\n\n\nIn my experiments with our smaller dataset size (~1100 images), I found that setting two different learning rates for the backbone and the object detection head led to poorer performance than just setting a single learning rate for the whole model.\nThe code below is an example of how to create a custom optimizer with different learning rates for different parts of the model.\nHowever, in our actual training code, we‚Äôll use a single learning rate for the whole model.\n\n\nWe can then subclass transformers.Trainer and update the method create_optimizer() to use our custom optimizer.\n\nfrom transformers import Trainer\n\n# Create lists for different kinds of parameters\nbackbone_parameters = []\nother_parameters = []\n\n# Can loop through model parameters and extract different model sections\nfor name, param in model.model.named_parameters(): \n    if \"backbone\" in name:\n        # print(f\"Backbone parameter: {name}\")\n        backbone_parameters.append(param)\n    else:\n        # print(f\"Other parameter: {name}\")\n        other_parameters.append(param)\n\nprint(f\"[INFO] Number of backbone parameter modules: {len(backbone_parameters)}\")\nprint(f\"[INFO] Number of other parameter modules: {len(other_parameters)}\")\n\nBACKBONE_LEARNING_RATE = 1e-5\nDETECTION_HEAD_LEARNING_RATE = 1e-4\n\nprint(f\"[INFO] Using learning rate for backbone: {BACKBONE_LEARNING_RATE}\")\nprint(f\"[INFO] Using learning rate for other parameters and detection head: {DETECTION_HEAD_LEARNING_RATE}\")\n\n# Setup a custom subclass of Trainer to use different learning rates for different parts of the model\nclass CustomTrainer(Trainer):\n    def create_optimizer(self):\n        self.optimizer = torch.optim.AdamW([\n            {\"params\": backbone_parameters, \"lr\": BACKBONE_LEARNING_RATE},\n            {\"params\": other_parameters, \"lr\": DETECTION_HEAD_LEARNING_RATE}\n        ], weight_decay=0.0001)\n        return self.optimizer\n\n[INFO] Number of backbone parameter modules: 55\n[INFO] Number of other parameter modules: 363\n\n\nAwesome!\nNow if we wanted to use our custom optimizer, we could use CustomTrainer instead of Trainer.\n\n\n13.3 Creating an evaluation function\nEvaluating a model‚Äôs performance is just as important as training a model.\nAfter all, if you don‚Äôt know how well your model is performing, how can you be confident in deploying it or using it in the real world?\nIn this section, let‚Äôs create an evaluation function we can pass to transformers.Trainer‚Äôs compute_metrics parameter.\nThe main goal of an evaluation function is to compare the model‚Äôs predictions to the ground truth labels.\nFor example, how does a model‚Äôs box predictions look like compared to the ground truth box predictions?\nOnce we‚Äôve got a trained model, we can inspect these visually by plotting them on images.\nHowever, during model training, we‚Äôll get our Trainer instance to output evaluation metrics so we can get a snapshot of performance along the way.\nSome things to note about the evaluation function we‚Äôll create:\n\nReading the documentation for the compute_metrics parameter, we can see our evaluation function will be required to take a transformers.EvalPrediction as input.\n\nThis contains our model‚Äôs predictions and labels as predictions and label_ids attributes respectively.\n\nWe must also return a dictionary with string to metric values for it to be displayed during training. For example, {\"metric_value\": 42, ...}.\nTo evaluate our object detection model we‚Äôre going to use the mAP metric (Mean Average Precision, a standard metric used amongst object detection models, see the COCO evaluation section for more details). To do so, we‚Äôll use torchmetrics package, specifically torchmetrics.detection.mean_ap.MeanAveragePrecision.\n\nThis method expects boxes in format XYXY absolute format by default.\nIn any case, higher mAP is better, for example, a model with 65 mAP on a given benchmark is better than a model with 60 mAP on the same benchmark.\n\nOur evaluation function will be an adaptation of the code example in the object detection example on the Hugging Face GitHub.\nFor an in-depth overview on object detection metrics, see the Roboflow Guide to Object Detection Metrics.\n\nPhew! A fair bit to take in.\nBut nothing we can‚Äôt handle.\nLet‚Äôs create our function.\nWe‚Äôll start by making a small helper function to convert bounding boxes from CXCYWH normalized format to XYXY absolute format.\n\nfrom torchvision.ops import box_convert\n\ndef convert_bbox_cxcywh_to_xyxy_absolute(boxes, \n                                         image_size_target):\n    \"\"\"\n    Converts CXCYWH normalized boxes to XYXY absolute boxes.\n\n    The output of our preprocess method puts boxes in CXCYWH format.\n\n    But our evaluation metric torchmetrics.detection.mean_ap.MeanAveragePrecision expects\n        boxes in XYXY absolute format.\n\n    Args:\n        boxes (torch.Tensor): A tensor of shape (N, 4) where N is the number of boxes and each box is in CXCYWH format.\n        image_size_target (tuple): A tuple containing the target image size as (height, width).\n    \n    Returns:\n        torch.Tensor: A tensor of shape (N, 4) where each box is converted to XYXY absolute format.\n    \"\"\"\n    # Convert normalized CXCYWH (output of model) -&gt; absolute XYXY format (required for evaluation)\n    boxes = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\")\n\n    # Convert normalized box coordinates to absolute pixel values based on the target size \n    image_size_target_height = image_size_target[0]\n    image_size_target_width = image_size_target[1]\n    boxes = boxes * torch.tensor([image_size_target_width, \n                                  image_size_target_height, \n                                  image_size_target_width, \n                                  image_size_target_height]) # Multiply X coordinates by the width and Y coordinates by the height\n\n    return boxes\n\nPerfect!\nTime to craft our compute_metrics function.\nThe main goal of the function will be to take a transformers.EvalPrediction output from our model and return a dictionary mapping metric names to values, for example, {\"metric_name\": 42.0 ...}.\nTo do so, we‚Äôll go through the following steps:\n\nCreate a Python dataclass to hold our model‚Äôs outputs. We could use a dictionary but this will give our code a bit more structure.\nCreate a compute_metrics function which takes in an EvalPrediction object as well as other required evaluation parameters such as image_processor (for post processing boxes), id2label (for mapping metrics to class names) and threshold (for assigning a prediction probability threshold to boxes).\nExtract predictions and targets from EvalPrediction via EvalPrediction.predictions and EvalPrediction.label_ids respectively.\nCreate empty lists of image_sizes (for post processing boxes), post_processed_predictions and post_processed_targets (we‚Äôll compare the latter two to each other).\nCollect target samples in format required for torchmetrics.detection.mean_ap.MeanAveragePrecision, for example, [{\"boxes\": [...], \"labels\": [...]}].\nCollect predictions in the required formart for MeanAveragePrecision, our model produces boxes in CXCYWH format, then we use image_processor.post_process_object_detection to convert the predictions to XYXY format, and append them to post_processed_predictions in form [{\"boxes\": [...], \"labels\": [...], \"scores\": [...]}].\nInitialize an instance of torchmetrics.detection.mean_ap.MeanAveragePrecision (see documentation for output of MeanAveragePrecision) and pass it predictions and labels to compute on.\nExtract lists of target metrics from the output of MeanAveragePrecision, for example, with metrics.pop(\"target_item\").\nPrepare metrics for output in the form of a dict with metric names -&gt; values, for example, {\"metric_name\": 42.0, ...}.\nRound metric values in output dictionary for visual display during training.\nCreate a partial function we can pass to transformers.Trainer‚Äôs compute_metrics parameter to run as a callable with appropriate parameter inputs to our compute_metrics function.\n\nEasy.\nWe‚Äôve got this.\n\n# Create an evaluation function to test our model's performance\nimport numpy as np\n\nfrom typing import Optional, Mapping\n\nfrom transformers import EvalPrediction\n\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\n\n# 1. Create a dataclass to hold our model's outputs\n@dataclass\nclass ModelOutput:\n    logits: torch.Tensor\n    pred_boxes: torch.Tensor\n\n# 2. Create a compute_metrics function which takes in EvalPrediction and other required parameters\n@torch.no_grad()\ndef compute_metrics(\n    evaluation_results: EvalPrediction, # these come out of the Trainer.evaluate method, see: https://huggingface.co/docs/transformers/en/internal/trainer_utils#transformers.EvalPrediction \n    image_processor: AutoImageProcessor,\n    threshold: float = 0.0,\n    id2label: Optional[Mapping[int, str]] = None,\n) -&gt; Mapping[str, float]:\n    \"\"\"\n    Compute mean average mAP, mAR and their variants for the object detection task.\n\n    Args:\n        evaluation_results (EvalPrediction): Predictions and targets from evaluation.\n        threshold (float, optional): Threshold to filter predicted boxes by confidence. Defaults to 0.0.\n        id2label (Optional[dict], optional): Mapping from class id to class name. Defaults to None.\n\n    Returns:\n        Mapping[str, float]: Metrics in a form of dictionary {&lt;metric_name&gt;: &lt;metric_value&gt;}\n    \"\"\"\n\n    # 3. Extract predictions and targets from EvalPrediction\n    predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n\n    # For metric computation we need to provide to MeanAveragePrecision\n    #  - 'targets' in a form of list of dictionaries with keys \"boxes\", \"labels\"\n    #  - 'predictions' in a form of list of dictionaries with keys \"boxes\", \"scores\", \"labels\"\n\n    # 4. Get a list of image sizes, processed targets and processed predictions\n    image_sizes = []\n    post_processed_targets = []\n    post_processed_predictions = []\n\n    ### Target collection ###\n\n    # 5. Collect target attributes in the required format for metric computation\n    for batch in targets:\n        # Collect ground truth image sizes, we will need them for predictions post processing\n        batch_image_sizes = torch.tensor(np.array([x[\"orig_size\"] for x in batch])) # turn into a list of numpy arrays first, then tensors\n        image_sizes.append(batch_image_sizes)\n\n        # Collect targets in the required format for metric computation\n        # boxes were converted to YOLO format needed for model training\n        # here we will convert them to Pascal VOC format (x_min, y_min, x_max, y_max) \n        # or XYXY format. We do this because the boxes out of preprocess() are in \n        # CXCYWH normalized format.\n        for image_target in batch:\n\n            # Get boxes and convert from CXCYWH to XYXY\n            boxes = torch.tensor(image_target[\"boxes\"])\n            boxes = convert_bbox_cxcywh_to_xyxy_absolute(boxes=boxes, \n                                                         image_size_target=image_target[\"orig_size\"])\n            \n            # Get labels\n            labels = torch.tensor(image_target[\"class_labels\"])\n\n            # Append box and label pairs in format requried for MeanAveragePrecision class\n            post_processed_targets.append({\"boxes\": boxes, \n                                           \"labels\": labels})\n    \n    ### Prediction collection ###\n\n    # 6. Collect predictions in the required format for metric computation,\n    # model produce boxes in YOLO format (CXCYWH), then image_processor.post_process_object_detection to \n    # convert them to Pascal VOC format (XYXY).\n    for batch, target_sizes in zip(predictions, image_sizes):\n        batch_logits, batch_boxes = batch[1], batch[2]\n        output = ModelOutput(logits=torch.tensor(batch_logits), \n                             pred_boxes=torch.tensor(batch_boxes))\n        \n        # Post process the model outputs\n        post_processed_output = image_processor.post_process_object_detection(\n                                                    outputs=output, \n                                                    threshold=threshold, \n                                                    target_sizes=target_sizes) # target sizes required to shape boxes in correct ratio of original image\n        \n        # Append post_processed_output in form `[{\"boxes\": [...], \"labels\": [...], \"scores\": [...]}]`\n        post_processed_predictions.extend(post_processed_output)\n\n    # 7. Compute mAP\n    max_detection_thresholds = [1, 10, 100] # 1 = mar@1, mar@10, mar@100 (100 = default max total boxes for post processed predictions out of object detection model)\n    metric = MeanAveragePrecision(box_format=\"xyxy\", \n                                  class_metrics=True,\n                                  max_detection_thresholds=max_detection_thresholds) \n    metric.warn_on_many_detections = False # don't output a warning when large amount of detections come out (the sorting handles this anyway)\n    metric.update(post_processed_predictions, \n                  post_processed_targets)\n    metrics = metric.compute()\n    \n    # Optional: print metrics dict for troubleshooting\n    # print(metrics)\n\n    # 8. Extract list of per class metrics with separate metric for each class\n    classes = metrics.pop(\"classes\")\n    map_per_class = metrics.pop(\"map_per_class\")\n\n    # Optional: mAR@N per class (mAR = Mean Average Recall)\n    mar_per_class = metrics.pop(\"mar_100_per_class\")\n    \n    # 9. Prepare metrics per class in the form of a dict with metric names -&gt; values, e.g. {\"metric_name\": 42.0, ...}\n    # for class_id, class_map in zip(classes, map_per_class):\n    for class_id, class_map, class_mar in zip(classes, map_per_class, mar_per_class):\n        class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n        metrics[f\"map_{class_name}\"] = class_map\n\n        # Optional: mAR@100 per class\n        metrics[f\"mar_100_{class_name}\"] = class_mar\n    \n    # 10. Round metrics for suitable visual output\n    metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n    \n    # Optional: print metrics dict for troubleshooting\n    # print(metrics)\n\n    return metrics\n\n# 11. Create a partial function for our compute_metrics function (we'll pass this to compute_metrics in Trainer)\neval_compute_metrics_fn = partial(\n        compute_metrics, \n        image_processor=image_processor, \n        threshold=0.0,\n        id2label=id2label, \n)\n\n\n\n13.4 Training our model with Trainer\nWe‚Äôve now got all the ingredients needed to train our model!\nThe good news is since we‚Äôve put so much effort into preparing our dataset, creating an evaluation function and setting up our training arguments, we can train our model in a few lines of code.\nTo train our model, we‚Äôll set up an instance of transformers.Trainer and then we‚Äôll pass it the following arguments:\n\nmodel - The model we‚Äôd like to train. In our case it will be the fresh insteand of model we created using our create_model() function.\nargs - An instance of transformers.TrainingArguments (or training_args in our case) containing various hyperparameter settings to use for our model.\ndata_collator - The function to use which will turn a list of samples from train_dataset into a batch of samples.\ntrain_dataset - The dataset we‚Äôd like our model to train on, in our case this will be processed_dataset[\"train\"], the dataset we‚Äôve already preprocessed.\neval_dataset - The dataset we‚Äôd like our model to be evaluated on, in our case this will be processed_dataset[\"validation\"], our model will never see these samples during training, it will only test itself on these.\ncompute_metrics - A Callable which takes in [EvalPrediction] and is able to return a string to metric ({\"metric_name\": value}) dictionary, these will displayed during training.\n\nAfter we‚Äôve done all that, we can start to train our model with by calling transformers.Trainer.train().\n\n# Note: Depending on the size/speed of your GPU, this may take a while\nfrom transformers import Trainer\n\n# 5. Setup instance of Trainer\nmodel_v1_trainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collate_function,\n    train_dataset=processed_dataset[\"train\"], # pass in the already preprocessed data\n    eval_dataset=processed_dataset[\"validation\"],\n    compute_metrics=eval_compute_metrics_fn,\n)\n\n# 6. Train the model \nmodel_v1_results = model_v1_trainer.train(\n    # resume_from_checkpoint=False # you can continue training a model here by passing in the path to a previous checkpoint\n) \n\n\n      \n      \n      [500/500 03:29, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nMap\nMap 50\nMap 75\nMap Small\nMap Medium\nMap Large\nMar 1\nMar 10\nMar 100\nMar Small\nMar Medium\nMar Large\nMap Bin\nMar 100 Bin\nMap Hand\nMar 100 Hand\nMap Not Bin\nMar 100 Not Bin\nMap Not Hand\nMar 100 Not Hand\nMap Not Trash\nMar 100 Not Trash\nMap Trash\nMar 100 Trash\nMap Trash Arm\nMar 100 Trash Arm\n\n\n\n\n1\n187.744900\n76.783653\n0.075800\n0.142000\n0.070300\n0.000000\n0.014500\n0.078600\n0.133100\n0.287500\n0.328500\n0.000000\n0.185800\n0.365700\n0.169400\n0.644000\n0.198200\n0.350000\n0.010300\n0.428600\n-1.000000\n-1.000000\n0.002000\n0.170800\n0.075000\n0.377900\n0.000000\n0.000000\n\n\n2\n62.347700\n24.752775\n0.199900\n0.306500\n0.186000\n0.000000\n0.056000\n0.210900\n0.326700\n0.533600\n0.550300\n0.000000\n0.175600\n0.576500\n0.283500\n0.752500\n0.441200\n0.692200\n0.006400\n0.335700\n-1.000000\n-1.000000\n0.113600\n0.304200\n0.175600\n0.684100\n0.178900\n0.533300\n\n\n3\n25.808900\n14.790608\n0.255100\n0.387700\n0.276000\n0.000000\n0.073300\n0.266500\n0.381400\n0.590300\n0.650800\n0.000000\n0.317600\n0.694500\n0.207500\n0.866000\n0.461000\n0.689200\n0.017200\n0.507100\n-1.000000\n-1.000000\n0.098200\n0.411100\n0.164100\n0.664600\n0.582700\n0.766700\n\n\n4\n18.084500\n11.792915\n0.331600\n0.491200\n0.374800\n0.050000\n0.091300\n0.354400\n0.411600\n0.608600\n0.688000\n0.100000\n0.509100\n0.720100\n0.497400\n0.867400\n0.536700\n0.785300\n0.099700\n0.628600\n-1.000000\n-1.000000\n0.181500\n0.465300\n0.281600\n0.681400\n0.392900\n0.700000\n\n\n5\n15.275100\n11.574305\n0.369100\n0.576400\n0.393500\n0.150000\n0.058200\n0.390900\n0.432400\n0.594200\n0.672200\n0.150000\n0.343200\n0.716800\n0.563300\n0.851800\n0.512100\n0.795100\n0.089900\n0.585700\n-1.000000\n-1.000000\n0.165100\n0.515300\n0.317600\n0.652200\n0.566500\n0.633300\n\n\n6\n13.623300\n10.943520\n0.424000\n0.611300\n0.483300\n0.088900\n0.062400\n0.446400\n0.506400\n0.658900\n0.731000\n0.300000\n0.356800\n0.768600\n0.625800\n0.861700\n0.532900\n0.767600\n0.106700\n0.614300\n-1.000000\n-1.000000\n0.177500\n0.588900\n0.376900\n0.686700\n0.724300\n0.866700\n\n\n7\n12.436100\n10.461347\n0.442100\n0.641400\n0.509000\n0.066700\n0.075100\n0.469800\n0.495100\n0.640100\n0.720900\n0.200000\n0.386900\n0.759600\n0.641300\n0.891500\n0.485900\n0.767600\n0.155600\n0.621400\n-1.000000\n-1.000000\n0.221000\n0.581900\n0.414800\n0.696500\n0.733800\n0.766700\n\n\n8\n11.657900\n10.532933\n0.430400\n0.610300\n0.473200\n0.035400\n0.061900\n0.456400\n0.488900\n0.642200\n0.718300\n0.350000\n0.243800\n0.763500\n0.641100\n0.858200\n0.511300\n0.772500\n0.157700\n0.485700\n-1.000000\n-1.000000\n0.184800\n0.588900\n0.384100\n0.704400\n0.703100\n0.900000\n\n\n9\n11.009100\n10.670994\n0.455300\n0.632500\n0.513800\n0.066700\n0.079400\n0.482400\n0.503800\n0.647500\n0.727300\n0.400000\n0.326700\n0.771400\n0.664900\n0.863800\n0.480000\n0.761800\n0.148100\n0.557100\n-1.000000\n-1.000000\n0.213200\n0.598600\n0.401300\n0.715900\n0.824300\n0.866700\n\n\n10\n10.597600\n10.565559\n0.464000\n0.641600\n0.504400\n0.053600\n0.079700\n0.490200\n0.514300\n0.666100\n0.739800\n0.350000\n0.480100\n0.774500\n0.677700\n0.859600\n0.488800\n0.771600\n0.153600\n0.628600\n-1.000000\n-1.000000\n0.204200\n0.605600\n0.392100\n0.707100\n0.867300\n0.866700\n\n\n\n\n\n\nThere were missing keys in the checkpoint model loaded: ['class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias', 'bbox_embed.0.layers.0.weight', 'bbox_embed.0.layers.0.bias', 'bbox_embed.0.layers.1.weight', 'bbox_embed.0.layers.1.bias', 'bbox_embed.0.layers.2.weight', 'bbox_embed.0.layers.2.bias', 'bbox_embed.1.layers.0.weight', 'bbox_embed.1.layers.0.bias', 'bbox_embed.1.layers.1.weight', 'bbox_embed.1.layers.1.bias', 'bbox_embed.1.layers.2.weight', 'bbox_embed.1.layers.2.bias', 'bbox_embed.2.layers.0.weight', 'bbox_embed.2.layers.0.bias', 'bbox_embed.2.layers.1.weight', 'bbox_embed.2.layers.1.bias', 'bbox_embed.2.layers.2.weight', 'bbox_embed.2.layers.2.bias', 'bbox_embed.3.layers.0.weight', 'bbox_embed.3.layers.0.bias', 'bbox_embed.3.layers.1.weight', 'bbox_embed.3.layers.1.bias', 'bbox_embed.3.layers.2.weight', 'bbox_embed.3.layers.2.bias', 'bbox_embed.4.layers.0.weight', 'bbox_embed.4.layers.0.bias', 'bbox_embed.4.layers.1.weight', 'bbox_embed.4.layers.1.bias', 'bbox_embed.4.layers.2.weight', 'bbox_embed.4.layers.2.bias', 'bbox_embed.5.layers.0.weight', 'bbox_embed.5.layers.0.bias', 'bbox_embed.5.layers.1.weight', 'bbox_embed.5.layers.1.bias', 'bbox_embed.5.layers.2.weight', 'bbox_embed.5.layers.2.bias'].\n\n\n\n\n13.5 Plotting our model‚Äôs loss curves\nLooking at the metrics output by our model‚Äôs training process, we can see the loss values going down on the training and evaluation datasets.\nAs well as the mAP going up almost universally across the board.\nLet‚Äôs make things visual by inspecting the loss curves and evaluation metric curves of our model.\nWe can extract our model‚Äôs training history values via the model_v1_trainer.state.log_history attribute, this will return us a list of dictionaries containing training metrics related to each epoch.\nOnce we‚Äôve got these, we can create lists of relevant values based on their keys and then plot them with matplotlib.\n\n# 7. Plotting our model's loss curves\nimport matplotlib.pyplot as plt\n\nlog_history = model_v1_trainer.state.log_history\n\n# Exctract loss values\ntrain_loss = [item[\"loss\"] for item in log_history if \"loss\" in item]\neval_loss = [item[\"eval_loss\"] for item in log_history if \"eval_loss\" in item]\n\n# Extract mAP values\neval_map = [item[\"eval_map\"] for item in log_history if \"eval_map\" in item]\n\n# Plot loss curves and mAP\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 7))\nax[0].plot(train_loss, label=\"Train loss\")\nax[0].plot(eval_loss, label=\"Eval loss\")\nax[0].set_title(\"Loss Curves (lower is better)\")\nax[0].set_ylabel(\"Loss Value\")\nax[0].set_xlabel(\"Epochs\")\nax[0].legend()\n\nax[1].plot(eval_map, label=\"Eval mAP\")\nax[1].set_title(\"Eval mAP (higher is better)\")\nax[1].set_ylabel(\"mAP (Mean Average Precision)\")\nax[1].set_xlabel(\"Epochs\")\nax[1].legend();\n\n\n\n\n\n\n\n\nBeautiful!\nThose are the exact kind of performance curves we‚Äôre looking for.\nIn an ideal world, the loss curves trend downwards and the mAP (Mean Average Percision) curves trend upwards.\nWe‚Äôve only trained for 10 epochs here (10 laps of the data), perhaps our metrics would be even better if we were to train for longer?\nI‚Äôll leave this as an extension for you to try.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#making-predictions-on-the-test-dataset",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#making-predictions-on-the-test-dataset",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "14 Making predictions on the test dataset",
    "text": "14 Making predictions on the test dataset\nWe‚Äôve trained a model on our training data (processed_dataset[\"train\"]) and considering the metrics on the validation data (processed_dataset[\"validation\"]) it looks like it‚Äôs performing well.\nHowever, there‚Äôs nothing quite like performing predictions on unseen test data and seeing how they go.\nWe can make predictions using our trained model by passing it samples formatted in the same way it was trained on.\nGood news is, we‚Äôve already got preprocessed test samples (our model has never seen these) in processed_dataset[\"test\"].\nLet‚Äôs start by inspecting a single processed test sample and then we‚Äôll make predictions on the whole test dataset.\n\n# Our dataset is broken into \"train\", \"validation\", \"test\"\nprocessed_dataset\n\n{'train': Dataset({\n     features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n     num_rows: 789\n }),\n 'validation': Dataset({\n     features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n     num_rows: 115\n }),\n 'test': Dataset({\n     features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n     num_rows: 224\n })}\n\n\n\n# Inspect a single sample of the processed test dataset\nprocessed_dataset[\"test\"][0]\n\n{'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]]),\n 'pixel_values': tensor([[[0.2627, 0.3176, 0.2627,  ..., 0.0510, 0.0667, 0.1843],\n          [0.1882, 0.2706, 0.3961,  ..., 0.0510, 0.0902, 0.3569],\n          [0.1451, 0.2235, 0.4392,  ..., 0.0549, 0.1922, 0.3608],\n          ...,\n          [0.7882, 0.7882, 0.7922,  ..., 0.3373, 0.4196, 0.2588],\n          [0.7843, 0.7961, 0.8078,  ..., 0.2863, 0.4941, 0.3725],\n          [0.7765, 0.7922, 0.8078,  ..., 0.2627, 0.5255, 0.4471]],\n \n         [[0.3333, 0.3765, 0.3098,  ..., 0.0745, 0.0941, 0.2118],\n          [0.2588, 0.3333, 0.4471,  ..., 0.0784, 0.1137, 0.3843],\n          [0.2157, 0.2902, 0.4902,  ..., 0.0863, 0.2196, 0.3882],\n          ...,\n          [0.0745, 0.0745, 0.0784,  ..., 0.3686, 0.4627, 0.2941],\n          [0.0706, 0.0824, 0.0941,  ..., 0.3176, 0.5412, 0.4157],\n          [0.0627, 0.0784, 0.0941,  ..., 0.2980, 0.5725, 0.4902]],\n \n         [[0.1686, 0.2471, 0.2196,  ..., 0.0275, 0.0471, 0.1765],\n          [0.0941, 0.1922, 0.3412,  ..., 0.0235, 0.0784, 0.3490],\n          [0.0353, 0.1373, 0.3686,  ..., 0.0314, 0.1725, 0.3412],\n          ...,\n          [0.1216, 0.1216, 0.1255,  ..., 0.1922, 0.2196, 0.1294],\n          [0.1176, 0.1294, 0.1412,  ..., 0.1451, 0.2863, 0.1804],\n          [0.1098, 0.1255, 0.1412,  ..., 0.1020, 0.2941, 0.2039]]]),\n 'labels': {'size': tensor([640, 480]), 'image_id': tensor([61]), 'class_labels': tensor([4, 5, 1, 0]), 'boxes': tensor([[0.2104, 0.8563, 0.2855, 0.2720],\n         [0.4194, 0.4927, 0.2398, 0.1785],\n         [0.3610, 0.6227, 0.2706, 0.2330],\n         [0.4974, 0.4785, 0.3829, 0.3820]]), 'area': tensor([23860.4043, 13150.1748, 19368.0898, 44929.9102]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}}\n\n\nWonderful, looks like these are ready to go!\nWe can make predictions on the test dataset using the transformers.Trainer.predict method, this will output a named tuple of predictions and label_ids.\n\n# Make predictions with trainer containing trained model\ntest_dataset_preds = model_v1_trainer.predict(test_dataset=processed_dataset[\"test\"])\n# test_dataset_preds\n\n\n\n\nOur predictions come in batches, just like our training was done.\nWe can extract the prediction values (logits and predicted bounding boxes) via the .predictions attribute. And the label outputs (the ground truths) via the label_ids attribute.\n\n# Predictions come in the same batch size as our training setup\ntest_dataset_prediction_outputs = test_dataset_preds.predictions\ntest_dataset_label_outputs = test_dataset_preds.label_ids\n\nprint(f\"[INFO] Found {len(test_dataset_prediction_outputs)} batches of prediction samples and {len(test_dataset_label_outputs)} batches of labels.\")\n\n[INFO] Found 14 batches of prediction samples and 14 batches of labels.\n\n\nWe can inspect a batch of predictions by taking the 0th index of test_dataset_prediction_outputs, inside this batch are:\n\nIndex 0 - Metrics in the form of a dictionary.\nIndex 1 - Logits in the form of a NumPy array.\nIndex 2 - Bounding box coordinates in the form of a NumPy array.\n\n\n# Get the first batch of test prediction samples \ntest_batch_metrics = test_dataset_prediction_outputs[0][0] # metrics come at index 0 in the form of a dictionary\ntest_batch_logits = test_dataset_prediction_outputs[0][1] # logits come at index 1 in a numpy array\ntest_batch_boxes = test_dataset_prediction_outputs[0][2] # predicted boxes come at index 2 in a numpy array\n\nprint(f\"[INFO] Metrics keys: {test_batch_metrics.keys()}\")\nprint(f\"[INFO] Test predictions single batch logits shape: {test_batch_logits.shape} -&gt; (batch_size, num_predictions, logit_per_class)\")\nprint(f\"[INFO] Test predictions single batch boxes shape: {test_batch_boxes.shape} -&gt; (batch_size, num_predictions, box_coordinates)\")\nprint(f\"[INFO] Test logits type: {type(test_batch_logits)} | Test boxes type: {type(test_batch_boxes)}\")\n\n[INFO] Metrics keys: dict_keys(['loss_vfl', 'loss_bbox', 'loss_giou', 'loss_vfl_aux_0', 'loss_bbox_aux_0', 'loss_giou_aux_0', 'loss_vfl_aux_1', 'loss_bbox_aux_1', 'loss_giou_aux_1', 'loss_vfl_aux_2', 'loss_bbox_aux_2', 'loss_giou_aux_2', 'loss_vfl_aux_3', 'loss_bbox_aux_3', 'loss_giou_aux_3', 'loss_vfl_aux_4', 'loss_bbox_aux_4', 'loss_giou_aux_4', 'loss_vfl_aux_5', 'loss_bbox_aux_5', 'loss_giou_aux_5'])\n[INFO] Test predictions single batch logits shape: (16, 300, 7) -&gt; (batch_size, num_predictions, logit_per_class)\n[INFO] Test predictions single batch boxes shape: (16, 300, 4) -&gt; (batch_size, num_predictions, box_coordinates)\n[INFO] Test logits type: &lt;class 'numpy.ndarray'&gt; | Test boxes type: &lt;class 'numpy.ndarray'&gt;\n\n\nLet‚Äôs concatenate all of the batches of test predictions into one single numpy.ndarray. We‚Äôll then turn them into torch.tensor‚Äôs so we can use them with our post-processing methods.\n\n# We can stack these together to get the full outputs\ntest_dataset_pred_logits = []\ntest_dataset_pred_boxes = []\n\nfor test_pred_batch in test_dataset_prediction_outputs:\n    test_dataset_pred_logits.append(test_pred_batch[1]) # logits come at index 1\n    test_dataset_pred_boxes.append(test_pred_batch[2]) # boxes come at index 2\n\ntest_dataset_pred_logits = torch.tensor(np.concatenate(test_dataset_pred_logits))\ntest_dataset_pred_boxes = torch.tensor(np.concatenate(test_dataset_pred_boxes))\n\nprint(f\"[INFO] Test predictions logits shape: {test_dataset_pred_logits.shape} -&gt; (num_samples, num_predictions, logit_per_class)\")\nprint(f\"[INFO] Test predictions boxes shape: {test_dataset_pred_boxes.shape} -&gt; (num_samples, num_predictions, box_coordinates - CXCYWH normalize format)\")\n\n[INFO] Test predictions logits shape: torch.Size([224, 300, 7]) -&gt; (num_samples, num_predictions, logit_per_class)\n[INFO] Test predictions boxes shape: torch.Size([224, 300, 4]) -&gt; (num_samples, num_predictions, box_coordinates - CXCYWH normalize format)\n\n\n\n14.1 Evaluating our test predictions\nNow we‚Äôve got our predicted logits and boxes, we can format them in a way so we can evaluate them with torchmetrics.detection.meap_ap.MeanAveragePrecision.\nThe MeanAveragePrecision metric wants the following:\n\npreds (List) - a list of dictionaries (one per image) with the keys boxes (in the default format XYXY and absolute), scores and labels. Where all values in the dictionaries are torch.Tensor.\ntarget (List) - a list of dictionaries (one per image) with the keys boxes (in the default format XYXY and absolute), labels. Where all values in the dictionaries are torch.Tensor.\n\nIn essence, our preds have scores (prediction probabilities) where as our targets do not.\nLet‚Äôs start by collecting a list of dictionaries for our preds.\nWe‚Äôll do so by iterating over our test_dataset_pred_logits and test_dataset_pred_boxes and passing the required inputs to transformers.RTDetrImageProcessor.post_process_object_detection.\n\n# Create an empty list for preds\ntest_dataset_prediction_dicts = []\n\n# Set a threshold for prediction probabilities (we'll use 0.0 to allow all possible predictions, change this if you feel like)\nTHRESHOLD = 0.0\n\n# Iterate through prediction logits and prediction boxes\nfor i in range(len(test_dataset_pred_boxes)):\n    pred_logits = test_dataset_pred_logits[i].unsqueeze(0) # add a batch dimension of 1\n    pred_boxes = test_dataset_pred_boxes[i].unsqueeze(0) \n\n    # Get original size of input image (required for post processing)\n    original_size = processed_dataset[\"test\"][i][\"labels\"][\"orig_size\"].unsqueeze(0) # comes in height, width, we add a batch dimension of 1\n\n    # Collect prediction outputs\n    pred_outputs = ModelOutput(logits=pred_logits,\n                               pred_boxes=pred_boxes)\n    \n    # Post process (boxes will automatically be output in XYXY absolute format)\n    pred_outputs_post_processed = image_processor.post_process_object_detection(\n        outputs=pred_outputs,\n        threshold=THRESHOLD,\n        target_sizes=original_size\n    )\n\n    # Create a dictionary of post processed outputs\n    prediction_dict = {\"boxes\": pred_outputs_post_processed[0][\"boxes\"],\n                       \"scores\": pred_outputs_post_processed[0][\"scores\"],\n                       \"labels\": pred_outputs_post_processed[0][\"labels\"]}\n    \n    # Append dictionary to list\n    test_dataset_prediction_dicts.append(prediction_dict)\n\nprint(f\"[INFO] Number of prediction dicts: {len(test_dataset_prediction_dicts)}\")\nprint(f\"[INFO] Example prediction dict:\")\ntest_dataset_prediction_dicts[0]\n\n[INFO] Number of prediction dicts: 224\n[INFO] Example prediction dict:\n\n\n{'boxes': tensor([[ 221.0374,  646.1925,  466.7746,  948.0635],\n         [ 284.1779,  500.8758,  513.3552,  748.1806],\n         [ 297.0548,  382.5186,  656.9966,  854.1584],\n         ...,\n         [ 360.6175,  257.2795,  654.1285,  513.4293],\n         [   6.4380,  659.8294,  456.9098, 1273.5051],\n         [ 509.2941,  337.9673,  714.2663,  552.0175]]),\n 'scores': tensor([0.6817, 0.5330, 0.5030, 0.3065, 0.1816, 0.1395, 0.1393, 0.1264, 0.1200,\n         0.1047, 0.1012, 0.1011, 0.0998, 0.0969, 0.0922, 0.0863, 0.0857, 0.0818,\n         0.0805, 0.0775, 0.0774, 0.0763, 0.0707, 0.0700, 0.0689, 0.0678, 0.0671,\n         0.0656, 0.0637, 0.0603, 0.0591, 0.0583, 0.0572, 0.0565, 0.0565, 0.0564,\n         0.0550, 0.0548, 0.0537, 0.0533, 0.0530, 0.0527, 0.0522, 0.0520, 0.0514,\n         0.0514, 0.0505, 0.0502, 0.0500, 0.0500, 0.0489, 0.0485, 0.0480, 0.0480,\n         0.0477, 0.0473, 0.0472, 0.0466, 0.0466, 0.0461, 0.0460, 0.0459, 0.0458,\n         0.0457, 0.0450, 0.0449, 0.0448, 0.0448, 0.0445, 0.0444, 0.0442, 0.0442,\n         0.0438, 0.0436, 0.0422, 0.0422, 0.0419, 0.0416, 0.0415, 0.0410, 0.0408,\n         0.0404, 0.0399, 0.0399, 0.0397, 0.0394, 0.0393, 0.0393, 0.0392, 0.0392,\n         0.0390, 0.0389, 0.0389, 0.0387, 0.0385, 0.0385, 0.0384, 0.0383, 0.0382,\n         0.0380, 0.0380, 0.0380, 0.0380, 0.0380, 0.0380, 0.0374, 0.0373, 0.0372,\n         0.0371, 0.0370, 0.0370, 0.0368, 0.0368, 0.0368, 0.0365, 0.0365, 0.0365,\n         0.0364, 0.0362, 0.0362, 0.0361, 0.0358, 0.0358, 0.0356, 0.0354, 0.0353,\n         0.0352, 0.0349, 0.0349, 0.0348, 0.0348, 0.0348, 0.0347, 0.0345, 0.0344,\n         0.0344, 0.0343, 0.0342, 0.0342, 0.0338, 0.0332, 0.0332, 0.0331, 0.0331,\n         0.0330, 0.0330, 0.0330, 0.0328, 0.0327, 0.0326, 0.0326, 0.0323, 0.0323,\n         0.0322, 0.0322, 0.0320, 0.0319, 0.0319, 0.0318, 0.0317, 0.0316, 0.0315,\n         0.0315, 0.0314, 0.0312, 0.0311, 0.0309, 0.0308, 0.0306, 0.0304, 0.0302,\n         0.0300, 0.0300, 0.0299, 0.0299, 0.0296, 0.0294, 0.0293, 0.0291, 0.0289,\n         0.0287, 0.0287, 0.0286, 0.0284, 0.0281, 0.0281, 0.0280, 0.0280, 0.0279,\n         0.0279, 0.0277, 0.0276, 0.0276, 0.0275, 0.0275, 0.0273, 0.0271, 0.0270,\n         0.0268, 0.0267, 0.0267, 0.0267, 0.0265, 0.0264, 0.0263, 0.0263, 0.0263,\n         0.0262, 0.0261, 0.0260, 0.0260, 0.0259, 0.0259, 0.0257, 0.0257, 0.0256,\n         0.0256, 0.0256, 0.0256, 0.0256, 0.0254, 0.0254, 0.0253, 0.0253, 0.0252,\n         0.0252, 0.0250, 0.0249, 0.0248, 0.0248, 0.0247, 0.0247, 0.0246, 0.0246,\n         0.0245, 0.0245, 0.0244, 0.0242, 0.0241, 0.0241, 0.0241, 0.0241, 0.0240,\n         0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0238, 0.0238, 0.0238, 0.0238,\n         0.0237, 0.0237, 0.0237, 0.0236, 0.0236, 0.0235, 0.0234, 0.0232, 0.0232,\n         0.0232, 0.0231, 0.0231, 0.0230, 0.0229, 0.0228, 0.0228, 0.0228, 0.0228,\n         0.0228, 0.0228, 0.0227, 0.0227, 0.0227, 0.0226, 0.0225, 0.0224, 0.0222,\n         0.0222, 0.0222, 0.0221, 0.0221, 0.0220, 0.0220, 0.0219, 0.0219, 0.0219,\n         0.0219, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0217, 0.0217,\n         0.0217, 0.0216, 0.0216]),\n 'labels': tensor([1, 5, 0, 0, 4, 5, 5, 5, 4, 1, 4, 0, 4, 1, 1, 4, 1, 3, 1, 4, 4, 2, 0, 1,\n         4, 1, 5, 0, 4, 0, 1, 6, 1, 4, 5, 4, 0, 0, 0, 3, 1, 0, 4, 3, 6, 0, 1, 4,\n         5, 0, 4, 1, 4, 0, 4, 1, 4, 1, 0, 5, 3, 4, 1, 1, 0, 1, 1, 3, 3, 6, 5, 4,\n         2, 4, 0, 5, 4, 2, 1, 4, 4, 1, 0, 0, 5, 4, 1, 1, 0, 4, 1, 1, 3, 3, 4, 5,\n         0, 4, 1, 3, 5, 1, 0, 4, 4, 5, 4, 4, 6, 4, 5, 4, 4, 4, 4, 1, 5, 3, 2, 4,\n         3, 4, 4, 0, 4, 0, 3, 0, 0, 2, 2, 5, 5, 3, 4, 0, 4, 3, 0, 5, 0, 1, 4, 1,\n         5, 4, 1, 1, 2, 5, 4, 1, 4, 4, 2, 5, 0, 2, 1, 1, 2, 0, 6, 4, 4, 1, 5, 1,\n         4, 1, 4, 1, 3, 3, 1, 5, 0, 3, 3, 1, 0, 5, 5, 0, 0, 4, 5, 4, 0, 5, 1, 1,\n         1, 1, 4, 5, 5, 4, 2, 1, 4, 4, 1, 0, 4, 0, 0, 0, 0, 5, 1, 0, 4, 1, 6, 3,\n         1, 5, 4, 5, 3, 1, 1, 0, 1, 6, 2, 5, 2, 0, 3, 4, 4, 0, 1, 5, 0, 6, 5, 5,\n         0, 0, 5, 1, 0, 6, 1, 1, 0, 5, 0, 3, 5, 4, 4, 5, 2, 6, 4, 5, 1, 3, 3, 1,\n         0, 0, 0, 2, 0, 2, 3, 5, 5, 5, 3, 4, 4, 5, 3, 0, 3, 4, 4, 0, 1, 2, 0, 0,\n         5, 1, 4, 6, 4, 4, 5, 1, 5, 1, 3, 1])}\n\n\nBeautiful! We‚Äôve now got a list of prediction dictionaries.\nLet‚Äôs do the same for our targets.\nWe‚Äôll iterate through each sample in proecess_dataset[\"test\"] and create a target dictionary for each sample.\nThe main difference is that we‚Äôll have to convert the boxes from CXCYWH normalized to XYXY absolute.\nLuckily, we‚Äôve got our handy convert_bbox_cxcywh_to_xyxy_absolute helper function to do just that!\n\n# Create a list for targets\ntest_dataset_target_dicts = []\n\n# Iterate through test samples\nfor test_sample in processed_dataset[\"test\"]:\n    \n    # Extract truth labels\n    sample_labels = test_sample[\"labels\"]\n\n    # Extract class labels and boxes\n    truth_class_labels = sample_labels[\"class_labels\"]\n    truth_boxes = sample_labels[\"boxes\"]\n\n    # Get original size of image\n    original_size = sample_labels[\"orig_size\"] # size of original image in (height, width)\n\n    # Convert boxes from CXCYWH normalized to XYXY absolute\n    truth_boxes_xyxy = convert_bbox_cxcywh_to_xyxy_absolute(boxes=truth_boxes, \n                                                            image_size_target=original_size)\n\n    # Create target truth dictionary\n    target_dict = {\"boxes\": truth_boxes_xyxy,\n                   \"labels\": truth_class_labels}\n    \n    # Append target dictionary to list\n    test_dataset_target_dicts.append(target_dict)\n\nprint(f\"[INFO] Number of target dictionaries: {len(test_dataset_target_dicts)}\")\nprint(f\"[INFO] Example target dictionary:\")\ntest_dataset_target_dicts[0]\n\n[INFO] Number of target dictionaries: 224\n[INFO] Example target dictionary:\n\n\n{'boxes': tensor([[  64.9000,  922.0001,  339.0000, 1270.2000],\n         [ 287.5000,  516.4000,  517.7000,  744.9000],\n         [ 216.7000,  647.9999,  476.5000,  946.2000],\n         [ 293.7000,  368.0000,  661.3000,  856.9000]]),\n 'labels': tensor([4, 5, 1, 0])}\n\n\nAlright, now we‚Äôve got a list of preds in test_dataset_prediction_dicts and a list of targets in test_dataset_target_dicts, let‚Äôs create an instance of MeanAveragePrecision and use to calculate metrics comparing our predictions to the ground truth.\nWe‚Äôll set the class_metrics=True parameter so we can get a breakdown of the mAP (Mean Average Precision) and mAR (Mean Average Recall) for each class.\n\n# Compare predictions to targets\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\n\n# Instantiate MAP metric instance\nmap_metric = MeanAveragePrecision(iou_type=\"bbox\", \n                                  class_metrics=True) # we want metrics for each individual class \nmap_metric.warn_on_many_detections = False # hide extra detection warnings\n\n# Update our metric with list of pred dicts and list of target dicts\nmap_metric.update(preds=test_dataset_prediction_dicts, \n                  target=test_dataset_target_dicts)\n\n# Calculate the metric\ntest_metric_outputs = map_metric.compute()\n\n# Extract per class metrics (we'll use these later on)\ntest_map_per_class = test_metric_outputs.pop(\"map_per_class\")\ntest_mar_per_class = test_metric_outputs.pop(\"mar_100_per_class\")\n\n# Inspect the metrics\ntest_metric_outputs\n\n{'map': tensor(0.3779),\n 'map_50': tensor(0.5424),\n 'map_75': tensor(0.4236),\n 'map_small': tensor(0.),\n 'map_medium': tensor(0.0672),\n 'map_large': tensor(0.3950),\n 'mar_1': tensor(0.4512),\n 'mar_10': tensor(0.6943),\n 'mar_100': tensor(0.7473),\n 'mar_small': tensor(0.),\n 'mar_medium': tensor(0.5421),\n 'mar_large': tensor(0.7628),\n 'classes': tensor([0, 1, 2, 3, 4, 5, 6], dtype=torch.int32)}\n\n\n\n\n14.2 Visualizing our test dataset evaluation mertics\nWe‚Äôve now got some test dataset evaluation metrics, how about we follow the data explorer‚Äôs motto and visualize, visualize, visualize!\nLet‚Äôs visualize a these in a bar chart.\n\nimport matplotlib.pyplot as plt\n\n# Extract mAP and mAR metrics\ntest_map_metrics = {key: value for key, value in test_metric_outputs.items() if \"map\" in key}\ntest_mar_metrics = {key: value for key, value in test_metric_outputs.items() if \"mar\" in key}\n\n# Get labels and values\ntest_map_labels, test_map_values = zip(*sorted(test_map_metrics.items()))\ntest_mar_labels, test_mar_values = zip(*sorted(test_mar_metrics.items()))\n\n# Create a subplot\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5), sharey=True)\n\n# Add mAP values\nax[0].bar(test_map_labels, test_map_values)\nax[0].set_title(\"Mean Average Precision (mAP)\")\nax[0].set_xlabel(\"Metric\")\nax[0].set_ylabel(\"Value\")\nax[0].tick_params(axis=\"x\", rotation=45)\n\n# Add mAR values\nax[1].bar(test_mar_labels, test_mar_values, color=\"tab:orange\")\nax[1].set_title(\"Mean Average Recall (mAR)\")\nax[1].set_xlabel(\"Metric\")\nax[1].tick_params(axis=\"x\", rotation=45)\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nNice! It looks like our model generally has a higher recall than precision, this is most likely because we set our prediction probability threshold to 0.0 (THRESHOLD=0.0).\nThis means all possible predictions are allowed through, in turn leading to the highest possible recall metric.\nLet‚Äôs now visualize per class values.\n\n# Map class names to metric values \ntest_map_per_class_dict = dict(zip(list(label2id.keys()), test_map_per_class))\ntest_mar_per_class_dict = dict(zip(list(label2id.keys()), test_mar_per_class))\n\n# Get labels and values\ntest_map_per_class_labels, test_map_per_class_values = zip(*sorted(test_map_per_class_dict.items()))\ntest_mar_per_class_labels, test_mar_per_class_values = zip(*sorted(test_mar_per_class_dict.items()))\n\n# Create a list of RGB colour floats for matplotlib\nlabel_to_colour_dict = {key: normalize_rgb(value) for key, value in colour_palette.items()}\n\ncolours_for_map = [label_to_colour_dict.get(label_name, (0.6, 0.6, 0.6)) for label_name in test_map_per_class_labels] # (0.6, 0.6, 0.6) = fallback to grey colour\ncolours_for_mar = [label_to_colour_dict.get(label_name, (0.6, 0.6, 0.6)) for label_name in test_mar_per_class_labels] # (0.6, 0.6, 0.6) = fallback to grey colour\n\n# Create a subplot\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5), sharey=True)\n\n# Add mAP values\nax[0].bar(test_map_per_class_labels, \n          test_map_per_class_values,\n          color=colours_for_map)\nax[0].set_title(\"Mean Average Precision (mAP)\")\nax[0].set_xlabel(\"Metric\")\nax[0].set_ylabel(\"Value\")\nax[0].tick_params(axis=\"x\", rotation=45)\n\n# Add mAR values\nax[1].bar(test_mar_per_class_labels, \n          test_mar_per_class_values, \n          color=colours_for_mar)\nax[1].set_title(\"Mean Average Recall (mAR)\")\nax[1].set_xlabel(\"Metric\")\nax[1].tick_params(axis=\"x\", rotation=45)\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nBeautiful!\nIt looks like our main target classes (bin, hand, trash) are performing quite similarly on precision and recall.\nWe could balance the prediction probability threshold depending on what we‚Äôre trying to optimize for.\n\n\n\n\n\n\nNote\n\n\n\nWhich metric should you optimize for?\nPrecision or recall?\nTo avoid false positives, optimize for precision (higher predicition probability threshold), this will mean less predictions will be made overall but they will have a higher likelihood of being correct.\nTo avoid false negatives, optimize for recall (lower prediction probability threshold), more overall predictions will be made, making it more likely that items will not be missed.\nWhich you choose will depend on your problem space.\nIf you are in a safety critical space, you might want to optimize for recall (less chance of something being missed but more false positives).\nIf user experience matters most, for example, in a consumer app like Trashify, optimize for recall, users often find deleting wrong results preferable to adding missed items.\nIf a false positive predicition is costly, optimize for precision.\n\n\n\n\n14.3 Evaluating and visualizing predictions one by one\nWe‚Äôve seen how our model performs on the test dataset in metric form but nothing quite compares to visualizing actual predictions.\nTo do so, we‚Äôll extract a random sample from processed_dataset[\"test\"], pass it to our model, post process the outputs and then plot the predicted boxes on an actual image.\n\n\n\n\n\n\nNote\n\n\n\nIf your predictions aren‚Äôt the exact same as below, this is because of the randomness of machine learning, what‚Äôs important is that the direction is similar. For example, do your loss curves go down and evaluation metrics trend up?\nIdeally, your predictions will be not too dissimiliar.\n\n\n\nimport time\n\n# Get a random sample from the test preds\nrandom_test_pred_index = random.randint(0, len(processed_dataset[\"test\"]))\nprint(f\"[INFO] Making predictions on test item with index: {random_test_pred_index}\")\n\n# Get a random sample from the processed dataset\nrandom_test_sample = processed_dataset[\"test\"][random_test_pred_index]\n\n# Do a single forward pass with the model (we'll time how long it takes for fun)\nstart_pred_time = time.time()\nrandom_test_sample_outputs = model(pixel_values=random_test_sample[\"pixel_values\"].unsqueeze(0).to(\"cuda\"), # model expects input [batch_size, color_channels, height, width]\n                                   pixel_mask=None)\nend_pred_time = time.time()\nprint(f\"[INFO] Total time to perform prediction: {round(end_pred_time - start_pred_time, 3)} seconds.\")\n\n# Post process a random item from test preds\nrandom_test_sample_outputs_post_processed = image_processor.post_process_object_detection(\n    outputs=random_test_sample_outputs,\n    threshold=0.35, # prediction probability threshold for boxes (note: boxes from an untrained model will likely be bad)\n    target_sizes=random_test_sample[\"labels\"][\"orig_size\"].unsqueeze(0) # original input image size (or whichever target size you'd like), required to be same number of input items in a list\n)\n\n# Extract scores, labels and boxes\nrandom_test_sample_pred_scores = random_test_sample_outputs_post_processed[0][\"scores\"]\nrandom_test_sample_pred_labels = random_test_sample_outputs_post_processed[0][\"labels\"]\nrandom_test_sample_pred_boxes = half_boxes(random_test_sample_outputs_post_processed[0][\"boxes\"])\n\n# Create a list of labels and colours to plot on the boxes \nrandom_test_sample_pred_to_score_tuples = [(id2label[label_pred.item()], round(score_pred.item(), 4)) \n                                           for label_pred, score_pred in zip(random_test_sample_pred_labels, random_test_sample_pred_scores)]\nrandom_test_sample_labels_to_plot = [f\"Pred: {item[0]} ({item[1]})\" for item in random_test_sample_pred_to_score_tuples]\nrandom_test_sample_colours_to_plot = [colour_palette[item[0]] for item in random_test_sample_pred_to_score_tuples]\n\nprint(f\"[INFO] Labels with scores:\")\nfor label in random_test_sample_labels_to_plot:\n    print(label)\n\n# Plot the predicted boxes on the random test image \ntest_pred_box_image = to_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=half_image(dataset[\"test\"][random_test_pred_index][\"image\"])),\n        boxes=random_test_sample_pred_boxes,\n        colors=random_test_sample_colours_to_plot,\n        labels=random_test_sample_labels_to_plot,\n        width=3\n    )\n)\n\ntest_pred_box_image\n\n[INFO] Making predictions on test item with index: 163\n[INFO] Total time to perform prediction: 0.07 seconds.\n[INFO] Labels with scores:\nPred: hand (0.7147)\nPred: bin (0.555)\nPred: trash (0.5036)\n\n\n\n\n\n\n\n\n\nNice!\nThese prediction boxes look far better than our randomly predicted boxes with an untrained model.\n\n\n\n\n\n\nNote\n\n\n\nIf you have two predictions appearing for the same class on the image (e.g.¬†two boxes around the hand), this can be remedied by post processing the predicted boxes with a technique called NMS (Non-maximum Suppression).\nNMS can help to only keep the highest scoring box per class (the one with the maximum prediction probability).\nThis would mean that if there are two (or more) boxes predicted for the hand class, only the one with the highest prediction probability will remain.\nThis same filtering technique can be applied to each predicted class.\n\n\n\n\n14.4 Comparing our model‚Äôs predicted boxes to the ground truth boxes\nHow about we compare our model‚Äôs predicted boxes to the ground truth boxes?\nTo do so, we‚Äôll extract the same test sample from the test dataset, plot the ground truth boxes on it and then create a side by side comparison of truth versus predictions.\n\n# Get ground truth image\nground_truth_image = half_image(dataset[\"test\"][random_test_pred_index][\"image\"])\n\n# Get ground truth boxes (we'll convert these from CXCYWH -&gt; XYXY to be in the same format as our prediction boxes)\nground_truth_boxes = [convert_bbox_cxcywh_to_xyxy_absolute(boxes=input_box,\n                                                           image_size_target=random_test_sample[\"labels\"][\"orig_size\"]) for input_box in random_test_sample[\"labels\"][\"boxes\"]]\nground_truth_boxes = torch.stack(half_boxes(ground_truth_boxes))\n\n# Get ground truth labels and colours\nground_truth_labels = [id2label[label.item()] for label in random_test_sample[\"labels\"][\"class_labels\"]]\nground_truth_colours = [colour_palette[label] for label in ground_truth_labels]\n\n# Create ground truth box plot image\ntest_ground_truth_box_image = to_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=ground_truth_image),\n        boxes=ground_truth_boxes,\n        colors=ground_truth_colours,\n        labels=ground_truth_labels,\n        width=3\n    )\n)\n\n# Plot ground truth image and boxes to predicted image and boxes\nfig, ax = plt.subplots(ncols=2, figsize=(16, 10))\nax[0].imshow(test_ground_truth_box_image)\nax[0].set_title(\"Ground Truth Image and Boxes\")\nax[0].axis(False)\nax[1].imshow(test_pred_box_image)\nax[1].set_title(\"Predicted Boxes\")\nax[1].axis(False)\n\nplt.show()\n\n\n\n\n\n\n\n\nWoah! It looks like our model does fairly well to reproduce boxes that are similar to the ground truth.\nThere are some slight mistakes such as where our model predicts more than one of the same box in similar areas (this could be filtered later on with NMS or non-maximum suppression which removes all but the highest prediction probability boxes for each class).\n\n\n14.5 Predict on image from the wild\nWe‚Äôve seen how our model performs on test data which is similar to our training data.\nBut how does it do on an image from the wild?\nFor the image below, I searched for ‚Äúperson putting trash in bin‚Äù and selected one of the first images to appear.\nYou can see it at the URL: https://images.pexels.com/photos/7565384/pexels-photo-7565384.jpeg.\nIf this image doesn‚Äôt work, we could even try our model on an AI generated image of a person throwing trash in a bin and see how it performs.\nLet‚Äôs write some code to download our target image from the URL above and save it to file.\n\nimport requests\nfrom PIL import Image\n\n# Example image of person putting trash in bin\nurl = \"https://images.pexels.com/photos/7565384/pexels-photo-7565384.jpeg\"\nfilename = \"pexels-photo-7565384.jpeg\"\n\n# Donwload image\nwith requests.get(url, stream=True, timeout=10) as response:\n    response.raise_for_status() # ensure the download succeeded\n    with open(filename, \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n\nprint(f\"[INFO] Saved to {filename}\")\nimage_from_wild = Image.open(filename)\nfor _ in range(3): # the default image is quite large so we'll half it three times\n    image_from_wild = half_image(image_from_wild)\nimage_from_wild\n\n[INFO] Saved to pexels-photo-7565384.jpeg\n\n\n\n\n\n\n\n\n\nNice!\nThis one looks slightly different to some of the images our model saw during training, so it‚Äôll be interesting to see how it goes.\nTo make predictions on the downloaded image we‚Äôll go through the following steps:\n\nOpen the image.\nPreprocess the image with image_processor.\nMake predictions on the processed image with our model.\nGet the original size of the image for doing box post processing conversions.\nSet a prediction probability threshold of how confident we‚Äôd like our model to be in its predictions.\nPost process our model‚Äôs predictions.\nExtract the post processed labels, scores and box coordinates.\nCreate a list of labels, scores and colours to plot.\nDraw our model‚Äôs predicted bounding boxes on the target image with draw_bounding_boxes and to_pil_image.\n\nLet‚Äôs do it!\n\n# Pred on image from pathname\nfrom pathlib import Path\nfrom PIL import Image\n\ndef get_image_dimensions_from_pil(image: Image.Image) -&gt; torch.tensor:\n    \"\"\"\n    Convert the dimensions of a PIL image to a PyTorch tensor in the order (height, width).\n\n    Args:\n        image (Image.Image): The input PIL image.\n\n    Returns:\n        torch.Tensor: A tensor containing the height and width of the image.\n    \"\"\"\n    # Get (width, height) of image (PIL.Image.size returns width, height)\n    width, height = image.size\n\n    # Convert to a tensor in the order (height, width)\n    image_dimensions_tensor = torch.tensor([height, width])\n\n    return image_dimensions_tensor\n\n# Get a test image \ntest_image_pil = Image.open(\"pexels-photo-7565384.jpeg\").resize(size=(640, 640))\n\n# Preprocess the image\ntest_image_preprocessed = image_processor.preprocess(images=test_image_pil,\n                                                     return_tensors=\"pt\")\n\n# Make predictions on the preprocessed image\nrandom_test_sample_outputs = model(pixel_values=test_image_preprocessed[\"pixel_values\"].to(\"cuda\"), # model expects input [batch_size, color_channels, height, width]\n                                   pixel_mask=None)\n\n# Get image original size\ntest_image_size = get_image_dimensions_from_pil(image=test_image_pil)\nprint(f\"[INFO] Test image size: {test_image_size}\")\n\n# Create the threshold, we can adjust this based on how confident we'd like our model to be about its predictions\nTHRESHOLD = 0.4\n\n# Post process the predictions\nrandom_test_sample_outputs_post_processed = image_processor.post_process_object_detection(\n    outputs=random_test_sample_outputs,\n    threshold=THRESHOLD,\n    target_sizes=test_image_size.unsqueeze(0) # needs to be same length as batch dimension of the logits (e.g. [[height, width]])\n)\n\n# Extract scores, labels and boxes\nrandom_test_sample_pred_scores = random_test_sample_outputs_post_processed[0][\"scores\"]\nrandom_test_sample_pred_labels = random_test_sample_outputs_post_processed[0][\"labels\"]\nrandom_test_sample_pred_boxes = random_test_sample_outputs_post_processed[0][\"boxes\"]\n\n# Create a list of labels to plot on the boxes \nrandom_test_sample_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\"\n                                     for label_pred, score_pred in zip(random_test_sample_pred_labels, random_test_sample_pred_scores)]\nrandom_test_sample_colours_to_plot = [colour_palette[id2label[label_pred.item()]] for label_pred in random_test_sample_pred_labels]\n\nprint(\"[INFO] Labels with scores:\")\nfor item in random_test_sample_labels_to_plot:\n    print(item)\n\n# Plot the predicted boxes on the random test image \nto_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=test_image_pil),\n        colors=random_test_sample_colours_to_plot,                     \n        boxes=random_test_sample_pred_boxes,\n        labels=random_test_sample_labels_to_plot,\n        width=3\n    )\n)\n\n[INFO] Test image size: tensor([640, 640])\n[INFO] Labels with scores:\nPred: trash (0.7413)\nPred: trash (0.5808)\nPred: bin (0.4705)\nPred: trash (0.4051)\nPred: trash (0.4042)\n\n\n\n\n\n\n\n\n\nHow did the model do?\nIt‚Äôs good to test on random images which may be in our domain (e.g.¬†random photos or AI generated photos of someone putting trash in a bin), this way we can test to see if there are any conditions where our model fails.\nIn the example above, I noticed the model often fails to detect the hand.\nThis is likely because many of our training images are from first person point of views rather than third person point of views.\nTo fix this, we could incorportate more diverse training data into our pipeline.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#uploading-our-trained-model-to-hugging-face-hub",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#uploading-our-trained-model-to-hugging-face-hub",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "15 Uploading our trained model to Hugging Face Hub",
    "text": "15 Uploading our trained model to Hugging Face Hub\nSince our model looks like it‚Äôs working quite well, how about we upload to the Hugging Face Hub to make it accessible to others?\nWe‚Äôll first start by creating a path to save the model to locally.\nThen we‚Äôll save the model to file using the transformers.Trainer.save_model method.\n\n# Save the model\nfrom datetime import datetime # optional: add a date of when we trained our model\n\n# Get details to add to model's save path\ntraining_epochs_ = training_args.num_train_epochs\nlearning_rate_ = \"{:.0e}\".format(training_args.learning_rate)\n\n# Create model save path with some training details\nmodel_save_path = f\"models/learn_hf_rt_detrv2_finetuned_trashify_box_dataset_only_manual_data_no_aug_{training_epochs_}_epochs_lr_{learning_rate_}\"\n\n# Save model to file\nprint(f\"[INFO] Saving model to: {model_save_path}\")\nmodel_v1_trainer.save_model(model_save_path)\n\n[INFO] Saving model to: models/learn_hf_rt_detrv2_finetuned_trashify_box_dataset_only_manual_data_no_aug_10_epochs_lr_1e-04\n\n\nNow let‚Äôs make sure we add our model‚Äôs image_processor to the our Trainer instance, so when someone loads our model, it automatically knows how to preprocess an input sample.\nThis is usually done automatically but I‚Äôve run into some issues in the past where the model doesn‚Äôt load the preprocessor.\nTo do this we can see the processing_class attribute of model_v1_trainer to be our image_processor.\n\n# Make sure trainer has the processor class (this can sometimes be automatically assigned, however, we'll hard code it just to be safe)\nmodel_v1_trainer.processing_class = image_processor\n\nNice!\nNow let‚Äôs push our model_v1_trainer to the Hugging Face Hub using transformers.Trainer.push_to_hub (this will push our trained model and processing class to the Hugging Face Hub).\n\n\n\n\n\n\nNote\n\n\n\nWhenever you try to push something to the Hugging Face Hub, make sure you‚Äôve got your Hugging Face account and token credentials setup correctly.\nSee the Hugging Face setup guide for a walkthrough of how to do this.\n\n\n\n# Push the model to the hub\n# Note: this will require you to have your Hugging Face account setup \nmodel_on_hub_url = model_v1_trainer.push_to_hub(commit_message=\"upload fine-tuned RT-DETRv2 trashify object detection model\",\n                                                # token=None # Optional to add a token manually\n                                                )\n\n\n\n\n\n\n\n\n\n\nPerfect! Our model has been uploaded to the Hugging Face Hub.\nIf no changes have been made to a previously uploaded model file, you might see a message like the following:\n\nNo files have been modified since last commit. Skipping to prevent empty commit.\n\nOtherwise, we can check the commit URL of our model using the commit_url attribute.\n\nprint(f\"[INFO] Our model has been uploaded with the following commit URL: {model_on_hub_url.commit_url}\")\n\n[INFO] Our model has been uploaded with the following commit URL: https://huggingface.co/mrdbourke/rt_detrv2_finetuned_trashify_box_detector_v1/commit/46003b6b8f8e9855a0d8979ba5cdb1b8ca437646",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#creating-a-demo-of-our-model-with-gradio",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#creating-a-demo-of-our-model-with-gradio",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "16 Creating a demo of our model with Gradio",
    "text": "16 Creating a demo of our model with Gradio\nOne of the best ways to share your machine learning work is by creating a demo application.\nAnd one of the best places to share your applications is Hugging Face Spaces.\nHugging Face Spaces allows you to host machine learning (and non-machine learning) applications for free (with optional paid hardware upgrades).\nIf you‚Äôre familiar with GitHub, Hugging Face Spaces works similar to a GitHub repository (each Space is a Git repository itself).\nIf not, that‚Äôs okay, think of Hugging Face Spaces as an online folder where you can upload your files and have them accessed by others.\nCreating a Hugging Face Space can be done in two main ways:\n\nManually - By going to the Hugging Face Spaces website and clicking ‚ÄúCreate new space‚Äù. Or by going directly to https://www.huggingface.co/new-space. Here, you‚Äôll be able to setup a few settings for your Space and choose the framework/runtime (e.g.¬†Streamlit, Gradio, Docker and more).\nProgrammatically - By using the Hugging Face Hub Python API we can write code to directly upload files to the Hugging Face Hub, including Hugging Face Spaces.\n\nBoth are great options but we‚Äôre going to take the second approach.\nThis is so we can create our Hugging Face Space right from this notebook.\nTo do so, we‚Äôll create three files and a folder:\n\napp.py (main file that Hugging Face Spaces looks for) - This will be the Python file which will be the main running file on our Hugging Face Space. Inside we‚Äôll include all the code necessary to run our Gradio demo (as above). Hugging Face Spaces will automatically recoginize the app.py file and run it for us.\nrequirements.txt - This text file will include all of the Python packages we need to run our app.py file. Before our Space starts to run, all of the packages in this file will be installed.\nREADME.md - This markdown file will include details about our Space as well as specific Space-related metadata (we‚Äôll see this later on).\ntrashify_examples/ - This folder will contain several images that our Trashify demo will showcase as examples in the demo.\n\nWe‚Äôll create these files with the following file structure:\ndemos/\n‚îî‚îÄ‚îÄ trashify_object_detector/\n    ‚îú‚îÄ‚îÄ app.py\n    ‚îú‚îÄ‚îÄ README.md\n    ‚îú‚îÄ‚îÄ requirements.txt\n    ‚îî‚îÄ‚îÄ trashify_examples/\n        ‚îú‚îÄ‚îÄ trashify_example_1.jpeg\n        ‚îú‚îÄ‚îÄ trashify_example_2.jpeg\n        ‚îî‚îÄ‚îÄ trashify_example_3.jpeg\nWhy this way?\nDoing it in the above style means we‚Äôll have a directory which contains all of our demos (demos/) as well as a dedicated directory which contains our Trashify demo application (trashify_object_detector/).\nThis way, we‚Äôll be able to upload the whole demos/trashify_object_detector/ folder to Hugging Face Spaces.\nLet‚Äôs start by making a directory to store our demo application files.\n\n# Setup path to trashify demo folder (we'll store all of our demo requirements in here)\ndemo_path = Path(\"../demos/trashify_object_detector\")\n\n# Create the directory\ndemo_path.mkdir(parents=True, exist_ok=True)\n\n\n16.1 Making an app file\nUPTOHERE - write the steps required for the app.py file\nOur app.py file will be the main part of our Hugging Face Space.\nInside the app.py file we‚Äôll:\n\nImport the required libraries/packages for running our demo app.\nSetup preprocessing and helper functions for our trained ojbect detection model. Because our model is already hosted on the Hugging Face Hub, we can load it directly with transformers.AutoModelForObjectDetection.from_pretrained and passing it our model‚Äôs name (e.g.¬†mrdbourke/rt_detrv2_finetuned_trashify_box_detector_v1) and when we upload our app.py file to Hugging Face Spaces, it will load the model directly from the Hub.\n\nNote: Be sure to change ‚Äúmrdbourke‚Äù to your own Hugging Face username.\n\nCreate a function predict_on_image to:\n\nTake in an image and confidence threshold.\nPredict on the image with our model.\nPost process the predictions.\nDraw the predictions on the target image (see step 4).\nReturn the target image with drawn predictions as well as a text label output as to whether trash, bin and hand were detected (see step 4).\n\nWe‚Äôll draw the model‚Äôs predicted boxes (if there are any) on the image with PIL.ImageDraw.\nWrite some logic to detect whether trash, bin and hand objects are detected as this is the overall goal of Trashify, so if all three are present, we‚Äôll output a message saying +1! for the person picking up trash.\nWe‚Äôll create a demo using Gradio‚Äôs gr.Interface class. This will take an image and float as inputs as well as an image and string as outputs. We can add descriptions and other information to our demo so they are visible in the live app. To finish off, we‚Äôll launch the demo with gr.Interface.launch.\n\nWe can write all of the above in a notebook cell.\nAnd we can turn it into a file by using the %%writefile magic command and passing it our target filepath.\nLet‚Äôs do it!\n\n%%writefile ../demos/trashify_object_detector/app.py\n\n# 1. Import the required libraries and packages\nimport gradio as gr\nimport torch\nfrom PIL import Image, ImageDraw, ImageFont # could also use torch utilities for drawing\n\nfrom transformers import AutoImageProcessor\nfrom transformers import AutoModelForObjectDetection\n\n### 2. Setup preprocessing and helper functions ###\n\n# Setup target model path to load\n# Note: Can load from Hugging Face or can load from local \nmodel_save_path = \"mrdbourke/rt_detrv2_finetuned_trashify_box_detector_v1\"\n\n# Load the model and preprocessor\n# Because this app.py file is running directly on Hugging Face Spaces, the model will be loaded from the Hugging Face Hub\nimage_processor = AutoImageProcessor.from_pretrained(model_save_path)\nmodel = AutoModelForObjectDetection.from_pretrained(model_save_path)\n\n# Set the target device (use CUDA/GPU if it is available)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\n\n# Get the id2label dictionary from the model\nid2label = model.config.id2label\n\n# Set up a colour dictionary for plotting boxes with different colours\ncolor_dict = {   \n    \"bin\": \"green\",\n    \"trash\": \"blue\",\n    \"hand\": \"purple\",\n    \"trash_arm\": \"yellow\",\n    \"not_trash\": \"red\",\n    \"not_bin\": \"red\",\n    \"not_hand\": \"red\",\n}\n\n# Create helper functions for seeing if items from one list are in another \ndef any_in_list(list_a, list_b):\n    \"Returns True if *any* item from list_a is in list_b, otherwise False.\"\n    return any(item in list_b for item in list_a)\n\ndef all_in_list(list_a, list_b):\n    \"Returns True if *all* items from list_a are in list_b, otherwise False.\"\n    return all(item in list_b for item in list_a)\n\n### 3. Create function to predict on a given image with a given confidence threshold ###\ndef predict_on_image(image, conf_threshold):\n    # Make sure model is in eval mode\n    model.eval()\n\n    # Make a prediction on target image \n    with torch.no_grad():\n        inputs = image_processor(images=[image], return_tensors=\"pt\")\n        model_outputs = model(**inputs.to(device))\n\n        target_sizes = torch.tensor([[image.size[1], image.size[0]]]) # -&gt; [batch_size, height, width] \n        \n        # Post process the raw outputs from the model \n        results = image_processor.post_process_object_detection(model_outputs,\n                                                                threshold=conf_threshold,\n                                                                target_sizes=target_sizes)[0]\n\n    # Return all items in results to CPU (we'll want this for displaying outputs with matplotlib)\n    for key, value in results.items():\n        try:\n            results[key] = value.item().cpu() # can't get scalar as .item() so add try/except block\n        except:\n            results[key] = value.cpu()\n\n    ### 4. Draw the predictions on the target image ###\n\n    # Can return results as plotted on a PIL image (then display the image)\n    draw = ImageDraw.Draw(image)\n\n    # Get a font from ImageFont\n    font = ImageFont.load_default(size=20)\n\n    # Get class names as text for print out\n    class_name_text_labels = []\n\n    # Iterate through the predictions of the model and draw them on the target image\n    for box, score, label in zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"]):\n        # Create coordinates\n        x, y, x2, y2 = tuple(box.tolist())\n\n        # Get label_name\n        label_name = id2label[label.item()]\n        targ_color = color_dict[label_name]\n        class_name_text_labels.append(label_name)\n\n        # Draw the rectangle\n        draw.rectangle(xy=(x, y, x2, y2), \n                       outline=targ_color,\n                       width=3)\n        \n        # Create a text string to display\n        text_string_to_show = f\"{label_name} ({round(score.item(), 3)})\"\n\n        # Draw the text on the image\n        draw.text(xy=(x, y),\n                  text=text_string_to_show,\n                  fill=\"white\",\n                  font=font)\n    \n    # Remove the draw each time\n    del draw\n\n    # Setup blank string to print out\n    return_string = \"\"\n\n    # Setup list of target items to discover\n    target_items = [\"trash\", \"bin\", \"hand\"]\n\n    ### 5. Create logic for outputting information message ### \n\n    # If no items detected or trash, bin, hand not in list, return notification \n    if (len(class_name_text_labels) == 0) or not (any_in_list(list_a=target_items, list_b=class_name_text_labels)):\n        return_string = f\"No trash, bin or hand detected at confidence threshold {conf_threshold}. Try another image or lowering the confidence threshold.\"\n        return image, return_string\n\n    # If there are some missing, print the ones which are missing\n    elif not all_in_list(list_a=target_items, list_b=class_name_text_labels):\n        missing_items = []\n        for item in target_items:\n            if item not in class_name_text_labels:\n                missing_items.append(item)\n        return_string = f\"Detected the following items: {class_name_text_labels}. But missing the following in order to get +1: {missing_items}. If this is an error, try another image or altering the confidence threshold. Otherwise, the model may need to be updated with better data.\"\n        \n    # If all 3 trash, bin, hand occur = + 1\n    if all_in_list(list_a=target_items, list_b=class_name_text_labels):\n        return_string = f\"+1! Found the following items: {class_name_text_labels}, thank you for cleaning up the area!\"\n\n    print(return_string)\n    \n    return image, return_string\n\n### 6. Setup the demo application to take in image, make a prediction with our model, return the image with drawn predicitons ### \n\n# Write description for our demo application\ndescription = \"\"\"\nHelp clean up your local area! Upload an image and get +1 if there is all of the following items detected: trash, bin, hand.\n\nModel is a fine-tuned version of [RT-DETRv2](https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config) on the [Trashify dataset](https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images).\n\nSee the full data loading and training code on [learnhuggingface.com](https://www.learnhuggingface.com/notebooks/hugging_face_object_detection_tutorial).\n\nThis version is v4 because the first three versions were using a different model and did not perform as well, see the [README](https://huggingface.co/spaces/mrdbourke/trashify_demo_v4/blob/main/README.md) for more.\n\"\"\"\n\n# Create the Gradio interface to accept an image and confidence threshold and return an image with drawn prediction boxes\ndemo = gr.Interface(\n    fn=predict_on_image,\n    inputs=[\n        gr.Image(type=\"pil\", label=\"Target Image\"),\n        gr.Slider(minimum=0, maximum=1, value=0.3, label=\"Confidence Threshold\")\n    ],\n    outputs=[\n        gr.Image(type=\"pil\", label=\"Image Output\"),\n        gr.Text(label=\"Text Output\")\n    ],\n    title=\"üöÆ Trashify Object Detection Demo V4\",\n    description=description,\n    # Examples come in the form of a list of lists, where each inner list contains elements to prefill the `inputs` parameter with\n    # See where the examples originate from here: https://huggingface.co/datasets/mrdbourke/trashify_examples/\n    examples=[\n        [\"trashify_examples/trashify_example_1.jpeg\", 0.3],\n        [\"trashify_examples/trashify_example_2.jpeg\", 0.3], \n        [\"trashify_examples/trashify_example_3.jpeg\", 0.3],\n    ],\n    cache_examples=True\n)\n\n# Launch the demo\ndemo.launch()\n\nOverwriting ../demos/trashify_object_detector/app.py\n\n\n\n\n16.2 Making a requirements file\nWhen you upload an app.py file to Hugging Face Spaces, it will attempt to run it automatically.\nAnd just like running the file locally, we need to make sure all of the required packages are available.\nOtherwise our Space will produce an error like the following:\n===== Application Startup at ... =====\n\nTraceback (most recent call last):\n  File \"/home/user/app/app.py\", line 1, in &lt;module&gt;\n    import torch\nModuleNotFoundError: No module named 'torch'\nGood news is, our demo only has three requirements: gradio, torch, transformers.\nLet‚Äôs create a requirements.txt file with the packages we need and save it to the same directory as our app.py file.\n%%writefile ../demos/trashify_object_detector/requirements.txt timm gradio torch transformers\n\n\n16.3 Making a README file\nOur app.py can contain information about our demo, however, we can also use a README.md file to further communicate our work.\n\n\n\n\n\n\nNote\n\n\n\nIt is common practice in Git repositories (including GitHub and Hugging Face Hub) to add a README.md file to your project so people can read more (hence ‚Äúread me‚Äù) about what your project is about.\n\n\nWe can include anything in markdown-style text in the README.md file.\nHowever, Spaces also have a special YAML block at the top of the README.md file in the root directory with configuration details.\nInside the YAML block you can put special metadata details about your Space including:\n\ntitle - The title of your Space (e.g.¬†title: Trashify Demo V4 üöÆ).\nemoji - The emoji to display on your Space (e.g.¬†emoji: üóëÔ∏è).\napp_file - The target app file for Spaces to run (set to app_file: app.py by default).\n\nAnd there are plenty more in the Spaces Configuration References documentation.\n\n\n\n\nExample of Hugging Face Spaces README.md file with YAML front matter (front matter is another term for ‚Äúthings at the front/top of the file‚Äù) for formatting the Space.\n\n\nLet‚Äôs create a README.md file with a YAML block at the top detailing some of the metadata about our project.\n\n\n\n\n\n\nNote\n\n\n\nThe YAML block at the top of the README.md can take some practice.\nIf you want to see a demo of how one gets created, try making a Hugging Face Space with the ‚ÄúCreate new Space‚Äù button on the https://huggingface.co/spaces page and seeing what the README.md file starts with (that‚Äôs how I found out what to do!).\n\n\n\n%%writefile ../demos/trashify_object_detector/README.md\n---\ntitle: Trashify Demo V4 üöÆ\nemoji: üóëÔ∏è\ncolorFrom: purple\ncolorTo: blue\nsdk: gradio\nsdk_version: 5.34.0\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n\n# üöÆ Trashify Object Detector V4 \n\nObject detection demo to detect `trash`, `bin`, `hand`, `trash_arm`, `not_trash`, `not_bin`, `not_hand`. \n\nUsed as example for encouraging people to cleanup their local area.\n\nIf `trash`, `hand`, `bin` all detected = +1 point.\n\n## Dataset\n\nAll Trashify models are trained on a custom hand-labelled dataset of people picking up trash and placing it in a bin.\n\nThe dataset can be found on Hugging Face as [`mrdbourke/trashify_manual_labelled_images`](https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images).\n\n## Demos\n\n* [V1](https://huggingface.co/spaces/mrdbourke/trashify_demo_v1) = Fine-tuned [Conditional DETR](https://huggingface.co/docs/transformers/en/model_doc/conditional_detr) model trained *without* data augmentation.\n* [V2](https://huggingface.co/spaces/mrdbourke/trashify_demo_v2) = Fine-tuned Conditional DETR model trained *with* data augmentation.\n* [V3](https://huggingface.co/spaces/mrdbourke/trashify_demo_v3) = Fine-tuned Conditional DETR model trained *with* data augmentation (same as V2) with an NMS (Non Maximum Suppression) post-processing step.\n* [V4](https://huggingface.co/spaces/mrdbourke/trashify_demo_v4) = Fine-tuned [RT-DETRv2](https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2) model trained *without* data augmentation or NMS post-processing (current best mAP).\n\n## Learn more\n\nSee the full end-to-end code of how this demo was built at [learnhuggingface.com](https://www.learnhuggingface.com/notebooks/hugging_face_object_detection_tutorial). \n\nOverwriting ../demos/trashify_object_detector/README.md\n\n\n\n\n16.4 Making an examples folder\nWhen we create our demo application, it‚Äôll be good to show people how to use it.\nTo do so, we can add some example images to use with our demo.\nFirst we‚Äôll create a folder to store the demo images.\n\n# Make a directory to save examples to\nfrom pathlib import Path\n\ndemo_example_dir = \"../demos/trashify_object_detector/trashify_examples/\"\nPath(demo_example_dir).mkdir(exist_ok=True, parents=True)\n\nAnd now we can download some pre-made examples I‚Äôve added to Hugging Face Datasets (none of these were in the Trashify training data).\nYou can find the example Trashify images at mrdbourke/trashify_examples.\n\n# Download the examples from Hugging Face Datasets\nfrom datasets import load_dataset\n\ntrashify_examples = load_dataset(\"mrdbourke/trashify_examples\")\ntrashify_examples\n\nDatasetDict({\n    train: Dataset({\n        features: ['image'],\n        num_rows: 3\n    })\n})\n\n\nPerfect!\nNow let‚Äôs save each of these images to our target example folder in the Trashify demo directory.\n\nfor i, sample in enumerate(trashify_examples[\"train\"]):\n    save_path = Path(demo_example_dir, f\"trashify_example_{i+1}.jpeg\")\n    print(f\"[INFO] Saving image to: {save_path}\")\n    sample[\"image\"].save(save_path)\n\n[INFO] Saving image to: ../demos/trashify_object_detector/trashify_examples/trashify_example_1.jpeg\n[INFO] Saving image to: ../demos/trashify_object_detector/trashify_examples/trashify_example_2.jpeg\n[INFO] Saving image to: ../demos/trashify_object_detector/trashify_examples/trashify_example_3.jpeg\n\n\nNow let‚Äôs check the demo folder for Trashify.\n\n!ls ../demos/trashify_object_detector/\n\nREADME.md  app.py  requirements.txt  trashify_examples\n\n\nPerfect!\nLooks like we‚Äôve got all the files we need to create our Space.\nLet‚Äôs upload them to the Hugging Face Hub.\n\n\n16.5 Uploading our demo to Hugging Face Spaces\nWe‚Äôve created all of the files required for our demo, now for the fun part!\nLet‚Äôs upload them to Hugging Face Spaces.\nTo do so programmatically, we can use the Hugging Face Hub Python API.\n\n\n\n\n\n\nNote\n\n\n\nThe Hugging Face Hub Python API has many different options for interacting with the Hugging Face Hub programmatically.\nYou can create repositories, upload files, upload folders, add comments, change permissions and much much more.\nBe sure to explore the documentation for at least 10-15 minutes to get an idea of what‚Äôs possible.\n\n\nTo get our demo hosted on Hugging Face Spaces we‚Äôll go through the following steps:\n\nImport the required methods from the huggingface_hub package, including create_repo, get_full_repo_name, upload_file (optional, we‚Äôll be using upload_folder) and upload_folder.\nDefine the demo folder we‚Äôd like to upload as well as the different parameters for the Hugging Face Space such as repo type (\"space\"), our target Space name, the target Space SDK (\"gradio\"), our Hugging Face token with write access (optional if it already isn‚Äôt setup).\nCreate a repository on Hugging Face Spaces using the huggingface_hub.create_repo method and filling out the appropriate parameters.\nGet the full name of our created repository using the huggingface_hub.get_full_repo_name method (we could hard code this but I like to get it programmatically incase things change).\nUpload the contents of our target demo folder (../demos/trashify_object_detector/) to Hugging Face Hub with huggingface_hub.upload_folder.\nHope it all works and inspect the results! ü§û\n\nA fair few steps but we‚Äôve got this!\n\n# 1. Import the required methods for uploading to the Hugging Face Hub\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file, # for uploading a single file (if necessary)\n    upload_folder # for uploading multiple files (in a folder)\n)\n\n# 2. Define the parameters we'd like to use for the upload\nLOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"../demos/trashify_object_detector\" \nHF_TARGET_SPACE_NAME = \"trashify_demo_v4\"\nHF_REPO_TYPE = \"space\" # we're creating a Hugging Face Space\nHF_SPACE_SDK = \"gradio\"\nHF_TOKEN = \"\" # optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)\n\n# 3. Create a Space repository on Hugging Face Hub \nprint(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\ncreate_repo(\n    repo_id=HF_TARGET_SPACE_NAME,\n    # token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)\n    repo_type=HF_REPO_TYPE,\n    private=False, # set to True if you don't want your Space to be accessible to others\n    space_sdk=HF_SPACE_SDK,\n    exist_ok=True, # set to False if you want an error to raise if the repo_id already exists \n)\n\n# 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})\nfull_hf_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\nprint(f\"[INFO] Full Hugging Face Hub repo name: {full_hf_repo_name}\")\n\n# 5. Upload our demo folder\nprint(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo: {full_hf_repo_name}\")\nfolder_upload_url = upload_folder(\n    repo_id=full_hf_repo_name,\n    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n    path_in_repo=\".\", # upload our folder to the root directory (\".\" means \"base\" or \"root\", this is the default)\n    # token=HF_TOKEN, # optional: set token manually\n    repo_type=HF_REPO_TYPE,\n    commit_message=\"Uploading Trashify box detection model app.py\"\n)\nprint(f\"[INFO] Demo folder successfully uploaded with commit URL: {folder_upload_url}\")\n\n[INFO] Creating repo on Hugging Face Hub with name: trashify_demo_v4\n[INFO] Full Hugging Face Hub repo name: mrdbourke/trashify_demo_v4\n[INFO] Uploading ../demos/trashify_object_detector to repo: mrdbourke/trashify_demo_v4\n[INFO] Demo folder successfully uploaded with commit URL: https://huggingface.co/spaces/mrdbourke/trashify_demo_v4/tree/main/.\n\n\nWoohoo!! Looks like our demo is now live on the Hugging Face Hub!\n\n\n16.6 Testing the hosted demo\nWe are showing Trashify v4 because like Star Wars episodes, I‚Äôve already done a few experiments before launching it.\nOne of the cool things about using Hugging Facce Spaces is we can embed the Space in our notebook using HTML.\nTo do so, you can click the ‚Äú‚Ä¶‚Äù button (three dots) in the top right hand corner of the Hugging Face Space and choose the option ‚ÄúEmbed this Space‚Äù.\nTo embed it with HTML, you can choose the Iframe option.\n\nfrom IPython.display import HTML\n\n# You can get embeddable HTML code for your demo by clicking the \"Embed\" button on the demo page\nHTML(data='''\n&lt;iframe\n    src=\"https://mrdbourke-trashify-demo-v4.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"1000\"\n&gt;&lt;/iframe&gt;     \n''')\n\n\n     \n\n\nIsn‚Äôt that cool!\nOur very our object detection model trained on a custom dataset and now live on the internet for other people to try out!",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#summary",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#summary",
    "title": "Object Detection with Hugging Face Transformers Tutorial",
    "section": "17 Summary",
    "text": "17 Summary\nWe‚Äôve followed the data, model, demo paradigm and built a custom object detection model on a custom dataset and deployed it into a functional demo.\nObject detection models can be a bit of challenge to begin with as you need coordinate data for items in your images as well as label data.\nHowever, as we‚Äôve seen, with the right mix of data, model and training techniques we can take an existing object detection model like RT-DETRv2 and tailor it to our own projects.\nThe following extensions and extra-curriculum are good follow ups to practice what we‚Äôve learned here.\n\n17.1 Extensions\n\nCan you improve the model by training it for longer? What happens if you double the amount of epochs we did?\nWhat are some ways to improve the model on differnet kinds of data? Could you take 10-30 photos of your own and add it to the dataset to improve the model?\nData augmentation is one way to improve image classification models but it can also work for object detection models, how might you implement data augmentation into our training pipeline?\n\nHint: See PyTorch‚Äôs guide for data augmentation on detection problems.\n\nSo far the RT-DETRv2 model we used seemed to work pretty well, what happens if you try the D-FINE model?\nSometimes our model predicts multiple of the same kind of boxes, for example, it will predict 2 boxes for ‚Äúhand‚Äù when there is only one, a technique to help with this is called NMS (Non Maximum Suppression), how might you implement this into our post processing pipeline?\nHow does our model perform when removing the \"not_X\" classes? For example, only have the positive classes (\"bin\", \"trash\", \"hand\") in the training data. Does the model naturally learn what is \"not_bin\", \"not_hand\", \"not_trash\"? For example, distinguishing background items from non-background items?\n\n\n\n17.2 Extra-Curriculum\n\n‚ÄúBut what if I have images but no box labels?‚Äù Great questions. One way to acquire labels is to use a zero-shot detection model such as OmDet Turbo or Grounding DINO which are capable of producing box labels on images given a text input. You could use these to bootstrap a labelled dataset and then train a custom model on them/improve them by reviewing.\nAnother way to get high quality labels is to manually annotate images, you can do with tools such as Label Studio and Prodigy.\nFor more on the RT-DETRv2 model, I‚Äôd encourage you to read the original paper where it was introduced, RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer.\n\n\n\n17.3 Extra resources\n\nApache 2.0 object detector models - For a list of high-performing Apache 2.0 (permissive open-source licence which enables commercial use) object detection models, I‚Äôve created a short guide which collects them.\nsupervision library - An excellent open-source library with plenty of visualization utilities for computer vision projects.\nObject detection evaluation metrics by Roboflow - This is a great guide to all of the important detection metrics you‚Äôll want to look at when creating object detection models.\nA Guide to Bounding Box Formats and How to Draw Them by Daniel Bourke - One of the most important things when it comes to evaluating object detection models is to see how they look on images, this guide shows you how to draw bounding boxes on images.\nA Hands-on Guide to IoU (Intersection over Union) for Bounding Boxes by Daniel Bourke - Intersection over Union (IoU) is a measure of how much one box overlaps another and is used for evaluating the quality of your predicted bounding boxes, this guide walks through code examples of calculating IoU.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "Website dedicated to teaching the Hugging Face ecosystem with practical examples.\nEach example will include an end-to-end approach of starting with a dataset (custom or existing), building and evaluating a model and creating a demo to share.\nTeaching style:\nA machine learning cooking show! üë®‚Äçüç≥\nMottos:\nProject style:\nData, model, demo!\nThis will be our (rough) workflow:"
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Learn Hugging Face ü§ó",
    "section": "Updates",
    "text": "Updates\n\n18 June 2025 - All code has been completed for the object detection project, train a custom object detection model and make a demo with it for others to try! (video course to come soon)\n1 Oct 2024 - Video course version of text classification is live on ZTM! Inside, we‚Äôll walkthrough every line of code building the text classification project with Hugging Face Datasets, Transformers and Spaces."
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Learn Hugging Face ü§ó",
    "section": "Contents",
    "text": "Contents\nAll code and text will be free/open-source, video step-by-step walkthroughs are available as a paid upgrade.\n\n\n\n\n\n\n\n\n\n\n\nProject\nDescription\nDataset\nModel\nDemo\nVideo Course\n\n\n\n\nText classification\nBuild project ‚ÄúFood Not Food‚Äù, a text classification model to classify image captions into ‚Äúfood‚Äù if they‚Äôre about food or ‚Äúnot_food‚Äù if they‚Äôre not about food. This is the ideal place to get started if you‚Äôve never used the Hugging Face ecosystem.\nDataset\nModel\nDemo\nVideo Course\n\n\nObject Detection\nBuild Trashify üöÆ, an object detection model to detect ‚Äútrash‚Äù, ‚Äúhand‚Äù, ‚Äúbin‚Äù to incentivize people to clean up their local area. Start with a dataset, customize an open-source object detection model and turn it into a demo application that others can use and try out on their own images.\nDataset\nModel\nDemo\nVideo Course (coming soon)\n\n\nMore to come soon!\nLet me know if you‚Äôd like to see anything specific by leaving an issue."
  },
  {
    "objectID": "index.html#who-is-it-for",
    "href": "index.html#who-is-it-for",
    "title": "Learn Hugging Face ü§ó",
    "section": "Who is it for?",
    "text": "Who is it for?\nIdeal for:\n\nBeginners who love things explained in detail.\nSomeone who wants to create more of their own end-to-end machine learning projects.\n\nNot ideal for:\n\nPeople with 2-3+ years of machine learning projects & experience^.\n\n^Note: This being said, you may actually find some things helpful along the way. Best to explore and see!"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Learn Hugging Face ü§ó",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n3-6 months Python experience.\n1x beginner machine learning or deep learning course (see my begineer-friendly ML course to learn Python + important ML concepts in one).\n\nPyTorch experience is a bonus (see my Learn PyTorch in a Day video or learnpytorch.io)"
  },
  {
    "objectID": "index.html#what-is-hugging-face",
    "href": "index.html#what-is-hugging-face",
    "title": "Learn Hugging Face ü§ó",
    "section": "What is Hugging Face?",
    "text": "What is Hugging Face?\nHugging Face is a platform that offers access to many different kinds of open-source machine learning models and datasets.\nThey‚Äôre also the creators of the popular transformers library (and many more helpful libraries) which is a Python-based library for working with pre-trained models as well as custom models.\nIf you‚Äôre getting into the world of AI and machine learning, you‚Äôre going to come across Hugging Face.\n\n\n\n\nA handful of pieces from the Hugging Face ecosystem. There are many more available in Hugging Face documentation."
  },
  {
    "objectID": "index.html#why-hugging-face",
    "href": "index.html#why-hugging-face",
    "title": "Learn Hugging Face ü§ó",
    "section": "Why Hugging Face?",
    "text": "Why Hugging Face?\nMany of the biggest companies in the world use Hugging Face for their open-source machine learning projects including Apple, Google, Facebook (Meta), Microsoft, OpenAI, ByteDance and more.\nNot only does Hugging Face make it so you can use state-of-the-art machine learning models such as Stable Diffusion (for image generation) and Whipser (for audio transcription) easily, it also makes it so you can share your own models, datasets and resources.\nAside from your own website, consider Hugging Face the homepage of your AI/machine learning profile."
  },
  {
    "objectID": "index.html#todo",
    "href": "index.html#todo",
    "title": "Learn Hugging Face ü§ó",
    "section": "TODO",
    "text": "TODO\n\nFinish outline of this (index.md) page\n\nCopy a similar version to the README.md for GitHub\nMake share image for the whole thing\n\nMake index of different projects\nEcosystem overview: transformers, datasets, accelerate, Spaces, Hub, models etc\nPractical tutorials\n\nText classification (this will be like a ‚Äústart here‚Äù for the Hugging Face ecosystem)\nMore to come‚Ä¶\n\nWhere to get help? HF forums, HF GitHub, etc\nFinish setup page\n\nLocal setup\n\nFinish deployment to learnhuggingface.com page\nGet started: text classification shows an end-to-end workflow with detailed steps, I‚Äôd advise starting here to get to know the ecosystem a bit\n\nOther projects are more focused on specific tasks with less explanations but still complete code examples"
  },
  {
    "objectID": "index.html#faq",
    "href": "index.html#faq",
    "title": "Learn Hugging Face ü§ó",
    "section": "FAQ",
    "text": "FAQ\n\nIs this an official Hugging Face website?\n\nNo, it‚Äôs a personal project by myself (Daniel Bourke) to learn and help others learn the Hugging Face ecosystem.\n\nHow is this website made?\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôd like to learn the Hugging Face ecosystem more.\nSo this website is dedicated to documenting my learning journey + creating usable resources and tutorials for others.\nIt‚Äôs made by Daniel Bourke and will be in a similiar style to learnpytorch.io.\nYou can see more of my tutorials on:\n\nYouTube\nGitHub\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html",
    "href": "notebooks/hugging_face_text_classification_tutorial.html",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "",
    "text": "Source code on GitHub | Online book version | Setup guide | Video Course (step by step walkthrough)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#overview",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#overview",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "1 Overview",
    "text": "1 Overview\nWelcome to the Learn Hugging Face Text Classificaiton project!\nThis tutorial is hands-on and focused on writing resuable code.\nWe‚Äôll start with a text dataset, build a model to classify text samples and then share our model as a demo others can use.\nTo do so, we‚Äôll be using a handful of helpful open-source tools from the Hugging Face ecosystem.\n\n\n\n\nWe‚Äôre going to put on our internship hats and build a food not food text classification model using tools from the Hugging Face ecosystem.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFeel to keep reading through the notebook but if you‚Äôd like to run the code yourself, be sure to go through the setup guide first.\n\n\n\n1.1 What we‚Äôre going to build\nWe‚Äôre going to be bulding a food/not_food text classification model.\nGiven a piece of a text (such as an image caption), our model will be able to predict if it‚Äôs about food or not.\nThis is the same kind of model I use in my own work on Nutrify (an app to help people learn about food).\nMore specifically, we‚Äôre going to follow the following steps:\n\nData: Problem defintion and dataset preparation - Getting a dataset/setting up the problem space.\nModel: Finding, training and evaluating a model - Finding a text classification model suitable for our problem on Hugging Face and customizing it to our own dataset.\nDemo: Creating a demo and put our model into the real world - Sharing our trained model in a way others can access and use.\n\nBy the end of this project, you‚Äôll have a trained model and demo on Hugging Face you can share with others:\n\nfrom IPython.display import HTML \n\nHTML(\"\"\"\n&lt;iframe\n    src=\"https://mrdbourke-learn-hf-food-not-food-text-classifier-demo.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"650\"\n&gt;&lt;/iframe&gt;\n\"\"\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote this is a hands-on project, so we‚Äôll be focused on writing reusable code and building a model that can be used in the real world. If you are looking for explainers to the theory of what we‚Äôre doing, I‚Äôll leave links in the extra-curriculum section.\n\n\n\n\n1.2 What is text classification?\nText classification is the process of assigning a category to a piece of text.\nWhere a category can be almost anything and a piece of text can be a word, phrase, sentence, paragraph or entire document.\nExample text classification problems include:\n\n\n\n\n\n\n\n\nProblem\nDescription\nProblem Type\n\n\n\n\nSpam/phishing email detection\nIs an email spam or not spam? Or is it a phishing email or not?\nBinary classification (one thing or another)\n\n\nSentiment analysis\nIs a piece of text positive, negative or neutral? Such as classifying product reviews into good/bad/neutral.\nMulti-class classification (one thing from many)\n\n\nLanguage detection\nWhat language is a piece of text written in?\nMulti-class classification (one thing from many)\n\n\nTopic classification\nWhat topic(s) does a news article belong to?\nMulti-label classification (one or more things from many)\n\n\nHate speech detection\nIs a comment hateful or not hateful?\nBinary classification (one thing or another)\n\n\nProduct categorization\nWhat categories does a product belong to?\nMulti-label classification (one or more things from many)\n\n\nBusiness email classification\nWhich category should this email go to?\nMulti-class classification (one thing from many)\n\n\n\nText classification is a very common problem in many business settings.\nFor example, a project I‚Äôve worked on previously as a machine learning engineer was building a text classification model to classify different insurance claims into claimant_at_fault/claimant_not_at_fault for a large insurance company.\nIt turns out the deep learning-based model we built was very good (98%+ accuracy on the test dataset).\n\n\n\n\nAn example text classification problem I once worked on to classify insurance claim texts into at fault or not fault. This result of the model would send the claim to a different department in the insurance company.\n\n\nSpeaking of models, there are several different kinds of models you can use for text classification.\nAnd each will have its pros and cons depending on the problem you‚Äôre working on.\nExample text classification models include:\n\n\n\nModel\nDescription\nPros\nCons\n\n\n\n\nRule-based\nUses a set of rules to classify text (e.g.¬†if text contains ‚Äúsad‚Äù -&gt; sentiment = low)\nSimple, easy to understand\nRequires manual creation of rules\n\n\nBag of Words\nCounts the frequency of words in a piece of text\nSimple, easy to understand\nDoesn‚Äôt capture word order\n\n\nTF-IDF\nWeighs the importance of words in a piece of text\nSimple, easy to understand\nDoesn‚Äôt capture word order\n\n\nDeep learning-based models\nUses neural networks to learn patterns in text\nCan learn complex patterns at scale\nCan require large amounts of data/compute power to run, not as easy to understand (can be hard to debug)\n\n\n\nFor our project, we‚Äôre going to go with a deep learning model.\nWhy?\nBecause Hugging Face helps us do so.\nAnd in most cases, with a quality dataset, a deep learning model will often perform better than a rule-based or other model.\n\n\n1.3 Why train your own text classification models?\nYou can customize pre-trained models for text classification as well as API-powered models and LLMs such as GPT, Gemini, Claude or Mistral.\nDepending on your requirements, there are several pros and cons for using your own model versus using an API.\nTraining/fine-tuning your own model:\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nControl: Full control over model lifecycle.\nCan be complex to get setup.\n\n\nNo usage limits (aside from compute constraints).\nRequires dedicated compute resources for training/inference.\n\n\nCan train once and deploy everywhere/whenever you want (for example, Tesla deploying a model to all self-driving cars).\nRequires maintenance over time to ensure performance remains up to par.\n\n\nPrivacy: Data can be kept in-house/app and doesn‚Äôt need to go to a third party.\nCan require longer development cycles compared to using existing APIs.\n\n\nSpeed: Customizing a small model for a specific use case often means it runs much faster.\n\n\n\n\nUsing a pre-built model API (e.g.¬†GPT, Gemini, Claude, Mistral):\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEase of use: often can be setup within a few lines of code.\nIf the model API goes down, your service goes down.\n\n\nNo maintenance of compute resources.\nData is required to be sent to a third-party for processing.\n\n\nAccess to the most advanced models.\nThe API may have usage limits per day/time period.\n\n\nCan scale if usage increases.\nCan be much slower than using dedicated models due to requiring an API call.\n\n\n\nFor this project, we‚Äôre going to focus on fine-tuning our own model.\n\n\n1.4 Workflow we‚Äôre going to follow\nOur motto is data, model, demo!\nSo we‚Äôre going to follow the rough workflow of:\n\nCreate and preprocess data.\nDefine the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\nDefine training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nI say rough because machine learning projects are often non-linear in nature.\nAs in, because machine learning projects involve many experiments, they can kind of be all over the place.\nBut this worfklow will give us some good guidelines to follow.\n\n\n\n\nA general Hugging Face workflow from idea to shared model and demo using tools from the Hugging Face ecosystem. You‚Äôll notice some of the steps don‚Äôt match with our workflow outline above. This is because the text-based workflow outline above breaks some of the steps down for educational purposes. These kind of workflows are not set in stone and are more of guide than specific directions. See information on each of the tools in the Hugging Face documentation.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#importing-necessary-libraries",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#importing-necessary-libraries",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "2 Importing necessary libraries",
    "text": "2 Importing necessary libraries\nLet‚Äôs get started!\nFirst, we‚Äôll import the required libraries.\nIf you‚Äôre running on your local computer, be sure to check out the getting setup guide to make sure you have everything you need.\nIf you‚Äôre using Google Colab, many of them the following libraries will be installed by default.\nHowever, we‚Äôll have to install a few extras to get everything working.\n\n\n\n\n\n\nNote\n\n\n\nIf you‚Äôre running on Google Colab, this notebook will work best with access to a GPU. To enable a GPU, go to Runtime ‚û°Ô∏è Change runtime type ‚û°Ô∏è Hardware accelerator ‚û°Ô∏è GPU.\n\n\nWe‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\nevaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\naccelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio.\n\nWe can also check the versions of our software with package_name.__version__.\n\n# Install dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)\ntry:\n  import datasets, evaluate, accelerate\n  import gradio as gr\nexcept ModuleNotFoundError:\n  !pip install -U datasets evaluate accelerate gradio # -U stands for \"upgrade\" so we'll get the latest version by default\n  import datasets, evaluate, accelerate\n  import gradio as gr\n\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport transformers\n\nprint(f\"Using transformers version: {transformers.__version__}\")\nprint(f\"Using datasets version: {datasets.__version__}\")\nprint(f\"Using torch version: {torch.__version__}\")\n\nUsing transformers version: 4.43.2\nUsing datasets version: 2.20.0\nUsing torch version: 2.4.0+cu121\n\n\nWonderful, as long as your versions are the same or higher to the versions above, you should be able to run the code below.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#getting-a-dataset",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#getting-a-dataset",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "3 Getting a dataset",
    "text": "3 Getting a dataset\nOkay, now we‚Äôre got the required libraries, let‚Äôs get a dataset.\nGetting a dataset is one of the most important things a machine learning project.\nThe dataset you often determines the type of model you use as well as the quality of the outputs of that model.\nMeaning, if you have a high quality dataset, chances are, your future model could also have high quality outputs.\nIt also means if your dataset is of poor quality, your model will likely also have poor quality outputs.\nFor a text classificaiton problem, your dataset will likely come in the form of text (e.g.¬†a paragraph, sentence or phrase) and a label (e.g.¬†what category the text belongs to).\n\n\n\n\nOur  food not food image caption dataset on the Hugging Face Hub.\n\n\nIn our case, our dataset comes in the form of a collection of synthetic image captions and their corresponding labels (food or not food).\nThis is a dataset I‚Äôve created earlier to help us practice building a text classification model.\nYou can find it on Hugging Face under the name mrdbourke/learn_hf_food_not_food_image_captions.\n\n\n\n\n\n\nFood Not Food Image Caption Dataset Creation\n\n\n\nYou can see how the Food Not Food image caption dataset was created in the example Google Colab notebook.\nA Large Language Model (LLM) was asked to generate various image caption texts about food and not food.\nGetting another model to create data for a problem is known as synthetic data generation and is a very good way of bootstrapping towards creating a model.\nOne workflow would be to use real data wherever possible and use synthetic data to boost when needed.\nNote that it‚Äôs always advised to evaluate/test models on real-life data as opposed to synthetic data.\n\n\n\n3.1 Where can you get more datasets?\nThe are many different places you can get datasets for text-based problems.\nOne of the best places is on the Hugging Face Hub, specifically huggingface.co/datasets.\nHere you can find many different kinds of problem specific data such as text classification.\nThere are also many more datasets available on Kaggle Datasets.\nAnd thanks to the power of LLMs (Large Language Models), you can also now create your own text classifications by generating samples (this is how I created the dataset for this project).\n\n\n\n\nHugging Face Datasets and Kaggle Datasets are two of the best places on the internet to find all kinds of different datasets. If you can‚Äôt find an existing dataset related to your problem you can either use your own data or potentially generate synthetic data samples with an LLM. For more on synthetic data generation, see the Creating Synthetic Data article by NVIDIA.\n\n\n\n\n3.2 Loading the dataset\nOnce we‚Äôve found/prepared a dataset on the Hugging Face Hub, we can use the Hugging Face datasets library to load it.\nTo load a dataset we can use the datasets.load_dataset(path=NAME_OR_PATH_OF_DATASET) function and pass it the name/path of the dataset we want to load.\nIn our case, our dataset name is mrdbourke/learn_hf_food_not_food_image_captions (you can also change this for your own dataset).\nAnd since our dataset is hosted on Hugging Face, when we run the following code for the first time, it will download it.\nIf your target dataset is quite large, this download may take a while.\nHowever, once the dataset is downloaded, subsequent reloads will be mush faster.\n\n# Load the dataset from Hugging Face Hub\ndataset = datasets.load_dataset(path=\"mrdbourke/learn_hf_food_not_food_image_captions\")\n\n# Inspect the dataset\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 250\n    })\n})\n\n\nDataset loaded!\nLooks like our dataset has two features, text and label.\nAnd 250 total rows (the number of examples in our dataset).\nWe can check the column names with dataset.column_names.\n\n# What features are there?\ndataset.column_names\n\n{'train': ['text', 'label']}\n\n\nLooks like our dataset comes with a train split already (the whole dataset).\nWe can access the train split with dataset[\"train\"] (some datasets also come with built-in \"test\" splits too).\n\n# Access the training split\ndataset[\"train\"]\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 250\n})\n\n\nHow about we check out a single sample?\nWe can do so with indexing.\n\ndataset[\"train\"][0]\n\n{'text': 'Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n 'label': 'food'}\n\n\nNice! We get back a dictionary with the keys text and label.\nThe text key contains the text of the image caption and the label key contains the label (food or not food).\n\n\n3.3 Inspect random examples from the dataset\nAt 250 total samples, our dataset isn‚Äôt too large.\nSo we could sit here and explore the samples one by one.\nBut whenever I interact with a new dataset, I like to view a bunch of random examples and get a feel of the data.\nDoing so is inline with the data explorer‚Äôs motto: visualize, visualize, visualize!\nAs a rule of thumb, I like to view at least 20-100 random examples when interacting with a new dataset.\nLet‚Äôs write some code to view 5 random indexes of our data and their corresponding text and labels at a time.\n\nimport random\n\nrandom_indexs = random.sample(range(len(dataset[\"train\"])), 5)\nrandom_samples = dataset[\"train\"][random_indexs]\n\nprint(f\"[INFO] Random samples from dataset:\\n\")\nfor item in zip(random_samples[\"text\"], random_samples[\"label\"]):\n    print(f\"Text: {item[0]} | Label: {item[1]}\")\n\n[INFO] Random samples from dataset:\n\nText: Set of spatulas kept in a holder | Label: not_food\nText: Mouthwatering paneer tikka masala, featuring juicy paneer in a rich tomato-based sauce, garnished with fresh coriander leaves. | Label: food\nText: Pair of reading glasses left open on a book | Label: not_food\nText: Set of board games stacked on a shelf | Label: not_food\nText: Two handfuls of bananas in a fruit bowl with grapes on the side, the fruit bowl is blue | Label: food\n\n\nBeautiful! Looks like our data contains a mix of shorter and longer sentences (between 5 and 20 words) of texts about food and not food.\nWe can get the unique labels in our dataset with dataset[\"train\"].unique(\"label\").\n\n# Get unique label values\ndataset[\"train\"].unique(\"label\")\n\n['food', 'not_food']\n\n\nIf our dataset is small enough to fit into memory, we can count the number of different labels with Python‚Äôs collections.Counter (a method for counting objects in an iterable or mapping).\n\n# Check number of each label\nfrom collections import Counter\n\nCounter(dataset[\"train\"][\"label\"])\n\nCounter({'food': 125, 'not_food': 125})\n\n\nExcellent, looks like our dataset is well balanced with 125 samples of food and 125 samples of not food.\nIn a binary classification case, this is ideal.\nIf the classes were dramatically unbalanced (e.g.¬†90% food and 10% not food) we might have to consider collecting/creating more data.\nBut best to train a model and see how it goes before making any drastic dataset changes.\nBecause our dataset is small, we could also inspect it via a pandas DataFrame (however, this may not be possible for extremely large datasets).\n\n# Turn our dataset into a DataFrame and get a random sample\nfood_not_food_df = pd.DataFrame(dataset[\"train\"])\nfood_not_food_df.sample(7)\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n142\nA slice of pizza with a generous amount of shr...\nfood\n\n\n6\nPair of reading glasses left open on a book\nnot_food\n\n\n97\nTelescope positioned on a balcony\nnot_food\n\n\n60\nA close-up of a family playing a board game wi...\nnot_food\n\n\n112\nRich and spicy lamb rogan josh with yogurt gar...\nfood\n\n\n181\nA steaming bowl of fiery chicken curry, infuse...\nfood\n\n\n197\nPizza with a stuffed crust, oozing with cheese\nfood\n\n\n\n\n\n\n\n\n# Get the value counts of the label column\nfood_not_food_df[\"label\"].value_counts()\n\nlabel\nfood        125\nnot_food    125\nName: count, dtype: int64",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#preparing-data-for-text-classification",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#preparing-data-for-text-classification",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "4 Preparing data for text classification",
    "text": "4 Preparing data for text classification\nWe‚Äôve got our data ready but there are a few steps we‚Äôll need to take before we can model it.\nThe main two being:\n\nTokenization - turning our text into a numerical representation (machines prefer numbers rather than words), for example, {\"a\": 0, \"b\": 1, \"c\": 2...}.\nCreating a train/test split - right now our data is in a training split only but we‚Äôll create a test set to evaluate our model‚Äôs performance.\n\nThese don‚Äôt necessarily have to be in order either.\nBefore we get to them, let‚Äôs create a small mapping from our labels to numbers.\nIn the same way we need to tokenize our text into numerical representation, we also need to do the same for our labels.\n\n4.1 Creating a mapping from labels to numbers\nOur machine learning model will want to see all numbers (people do well with text, computers do well with numbers).\nThis goes for text as well as label input.\nSo let‚Äôs create a mapping from our labels to numbers.\nSince we‚Äôve only got a couple of labels (\"food\" and \"not_food\"), we can create a dictionary to map them to numbers, however, if you‚Äôve got a fair few labels, you may want to make this mapping programmatically.\nWe can use these dictionaries later on for our model training as well as evaluation.\n\n# Create mapping from id2label and label2id\nid2label = {0: \"not_food\", 1: \"food\"}\nlabel2id = {\"not_food\": 0, \"food\": 1}\n\nprint(f\"Label to ID mapping: {label2id}\")\nprint(f\"ID to Label mapping: {id2label}\")\n\nLabel to ID mapping: {'not_food': 0, 'food': 1}\nID to Label mapping: {0: 'not_food', 1: 'food'}\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn a binary classification task (such as what we‚Äôre working on), the positive class, in our case \"food\", is usually given the label 1 and the negative class (\"not_food\") is given the label 0.\n\n\nRather than hard-coding our label to ID maps, we can also create them programmatically from the dataset (this is helpful if you have many classes).\n\n# Create mappings programmatically from dataset\nid2label = {idx: label for idx, label in enumerate(dataset[\"train\"].unique(\"label\")[::-1])} # reverse sort list to have \"not_food\" first\nlabel2id = {label: idx for idx, label in id2label.items()}\n\nprint(f\"Label to ID mapping: {label2id}\")\nprint(f\"ID to Label mapping: {id2label}\")\n\nLabel to ID mapping: {'not_food': 0, 'food': 1}\nID to Label mapping: {0: 'not_food', 1: 'food'}\n\n\nWith our dictionary mappings created, we can update the labels of our dataset to be numeric.\nWe can do this using the datasets.Dataset.map method and passing it a function to apply to each example.\nLet‚Äôs create a small function which turns an example label into a number.\n\n# Turn labels into 0 or 1 (e.g. 0 for \"not_food\", 1 for \"food\")\ndef map_labels_to_number(example):\n  example[\"label\"] = label2id[example[\"label\"]]\n  return example\n\nexample_sample = {\"text\": \"This is a sentence about my favourite food: honey.\", \"label\": \"food\"}\n\n# Test the function\nmap_labels_to_number(example_sample)\n\n{'text': 'This is a sentence about my favourite food: honey.', 'label': 1}\n\n\nLooks like our function works!\nHow about we map it to the whole dataset?\n\n# Map our dataset labels to numbers\ndataset = dataset[\"train\"].map(map_labels_to_number)\ndataset[:5]\n\n{'text': ['Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n  'Set of books stacked on a desk',\n  'Watching TV together, a family has their dog stretched out on the floor',\n  'Wooden dresser with a mirror reflecting the room',\n  'Lawn mower stored in a shed'],\n 'label': [1, 0, 0, 0, 0]}\n\n\nNice! Looks like our labels are all numerical now.\nWe can check a few random samples using dataset.shuffle() and indexing for the first few.\n\n# Shuffle the dataset and view the first 5 samples (will return different results each time) \ndataset.shuffle()[:5]\n\n{'text': ['Set of oven mitts hanging on a hook',\n  'Set of cookie cutters collected in a jar',\n  'Pizza with a dessert twist, featuring a sweet Nutella base and fresh strawberries on top',\n  'Set of binoculars placed on a table',\n  'Two handfuls of bananas in a fruit bowl with grapes on the side, the fruit bowl is blue'],\n 'label': [0, 0, 1, 0, 1]}\n\n\n\n\n4.2 Split the dataset into training and test sets\nRight now our dataset only has a training split.\nHowever, we‚Äôd like to create a test split so we can evaluate our model.\nIn essence, our model will learn patterns (the relationship between text captions and their labels of food/not_food) on the training data.\nAnd we will evaluate those learned patterns on the test data.\nWe can split our data using the datasets.Dataset.train_test_split method.\nWe can use the test_size parameter to define the percentage of data we‚Äôd like to use in our test set (e.g.¬†test_size=0.2 would mean 20% of the data goes to the test set).\n\n# Create train/test splits\ndataset = dataset.train_test_split(test_size=0.2, seed=42) # note: seed isn't needed, just here for reproducibility, without it you will get different splits each time you run the cell\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50\n    })\n})\n\n\nPerfect!\nOur dataset has been split into 200 training examples and 50 testing examples.\nLet‚Äôs visualize a few random examples to make sure they still look okay.\n\nrandom_idx_train = random.randint(0, len(dataset[\"train\"]))\nrandom_sample_train = dataset[\"train\"][random_idx_train]\n\nrandom_idx_test = random.randint(0, len(dataset[\"test\"]))\nrandom_sample_test = dataset[\"test\"][random_idx_test]\n\nprint(f\"[INFO] Random sample from training dataset:\")\nprint(f\"Text: {random_sample_train['text']}\\nLabel: {random_sample_train['label']} ({id2label[random_sample_train['label']]})\\n\")\nprint(f\"[INFO] Random sample from testing dataset:\")\nprint(f\"Text: {random_sample_test['text']}\\nLabel: {random_sample_test['label']} ({id2label[random_sample_test['label']]})\")\n\n[INFO] Random sample from training dataset:\nText: Set of dumbbells stacked in a gym\nLabel: 0 (not_food)\n\n[INFO] Random sample from testing dataset:\nText: Two handfuls of bananas in a fruit bowl with grapes on the side, the fruit bowl is blue\nLabel: 1 (food)\n\n\n\n\n4.3 Tokenizing text data\nLabels numericalized, dataset split, time to turn our text into numbers.\nHow?\nTokenization.\nWhat‚Äôs tokenization?\nTokenization is the process of converting a non-numerical data source into numbers.\nWhy?\nBecause machines (especially machine learning models) prefer numbers to human-style data.\nIn the case of the text \"I love pizza\" a very simple method of tokenization might be to convert each word to a number.\nFor example, {\"I\": 0, \"love\": 1, \"pizza\": 2}.\nHowever, for most modern machine learning models, the tokenization process is a bit more nuanced.\nFor example, the text \"I love pizza\" might be tokenized into something more like [101, 1045, 2293, 10733, 102].\n\n\n\n\nAlthough it may seem like you can type text directly to machine learning models, behind the scenes they are converting it to numbers first. This happens for all kinds of data being passed to machine learning models. It goes from its raw form (e.g.¬†text, image, audio) and gets turned into a numerical representation (often called tokenization) before it is processed by the model. Exactly how data gets turned into numbers will often be different depending on the model. This example shows the use of OpenAI‚Äôs GPT-3.5 & GPT-4 tokenizer.\n\n\n\n\n\n\n\n\nNote\n\n\n\nDepending on the model you use, the tokenization process could be different.\nFor example, one model might turn \"I love pizza\" into [40, 3021, 23317], where as another model might turn it into [101, 1045, 2293, 10733, 102].\nTo deal with this, Hugging Face models often pair models and tokenizers together by name.\nSuch is the case with distilbert/distilbert-base-uncased (there is a tokenizer.json file as well as a tokenizer_config.json file which contains all of the tokenizer implementation details).\nFor more examples of tokenization, you can see OpenAI‚Äôs tokenization visualizer tool as well as their open-source library tiktoken, Google also have an open-source tokenization library called sentencepiece, finally Hugging Face‚Äôs tokenizers library is also a great resource (this is what we‚Äôll be using behind the scenes).\n\n\nMany of the text-based models on Hugging Face come paired with their own tokenizer.\nFor example, the distilbert/distilbert-base-uncased model is paired with the distilbert/distilbert-base-uncased tokenizer.\nWe can load the tokenizer for a given model using the transformers.AutoTokenizer.from_pretrained method and passing it the name of the model we‚Äôd like to use.\nThe transformers.AutoTokenizer class is part of a series of Auto Classes (such as AutoConfig, AutoModel, AutoProcessor) which automatically loads the correct configuration settings for a given model ID.\nLet‚Äôs load the tokenizer for the distilbert/distilbert-base-uncased model and see how it works.\n\n\n\n\n\n\nNote\n\n\n\nWhy use the distilbert/distilbert-base-uncased model?\nThe short answer is that I‚Äôve used it before and it works well (and fast) on various text classification tasks.\nIt also performed well in the original research paper which introduced it.\nThe longer answer is that Hugging Face has many available open-source models for many different problems available at https://huggingface.co/models.\nNavigating these models can take some practice.\nAnd several models may be suited for the same task (though with various tradeoffs such as size and speed).\nHowever, overtime and with adequate experimentation, you‚Äôll start to build an intuition on which models are good for which problems.\n\n\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n                                          use_fast=True) # uses fast tokenization (backed by tokenziers library and implemented in Rust) by default, if not available will default to Python implementation\n\ntokenizer\n\nDistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n\n\nNice!\nThere‚Äôs our tokenizer!\nIt‚Äôs an instance of the transformers.DistilBertTokenizerFast class.\nYou can read more about it in the documentation.\nFor now, let‚Äôs try it out by passing it a string of text.\n\n# Test out tokenizer\ntokenizer(\"I love pizza\")\n\n{'input_ids': [101, 1045, 2293, 10733, 102], 'attention_mask': [1, 1, 1, 1, 1]}\n\n\n\n# Try adding a \"!\" at the end\ntokenizer(\"I love pizza!\")\n\n{'input_ids': [101, 1045, 2293, 10733, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n\n\nWoohoo!\nOur text gets turned into numbers (or tokens).\nNotice how with even a slight change in the text, the tokenizer produces different results?\nThe input_ids are our tokens.\nAnd the attention_mask (in our case, all [1, 1, 1, 1, 1, 1]) is a mask which tells the model which tokens to use or not.\nTokens with a mask value of 1 get used and tokens with a mask value of 0 get ignored.\nThere are several attributes of the tokenizer we can explore.\n\ntokenizer.vocab will return the vocabulary of the tokenizer or in other words, the unique words/word pieces the tokenizer is capable of converting into numbers.\ntokenizer.model_max_length will return the maximum length of a sequence the tokenizer can process, pass anything longer than this and the sequence will be truncated.\n\n\n# Get the length of the vocabulary \nlength_of_tokenizer_vocab = len(tokenizer.vocab)\nprint(f\"Length of tokenizer vocabulary: {length_of_tokenizer_vocab}\")\n\n# Get the maximum sequence length the tokenizer can handle\nmax_tokenizer_input_sequence_length = tokenizer.model_max_length\nprint(f\"Max tokenizer input sequence length: {max_tokenizer_input_sequence_length}\")\n\nLength of tokenizer vocabulary: 30522\nMax tokenizer input sequence length: 512\n\n\nWoah, looks like our tokenizer has a vocabulary of 30,522 different words and word pieces.\nAnd it can handle a sequence length of up to 512 (any sequence longer than this will be automatically truncated from the end).\nLet‚Äôs check out some of the vocab.\nCan I find my own name?\n\n# Does \"daniel\" occur in the vocab?\ntokenizer.vocab[\"daniel\"]\n\n3817\n\n\nOooh, looks like my name is 3817 in the tokenizer‚Äôs vocab.\nCan you find your own name? (note: there may be an error if the token doesn‚Äôt exist, we‚Äôll get to this)\nHow about ‚Äúpizza‚Äù?\n\ntokenizer.vocab[\"pizza\"]\n\n10733\n\n\nWhat if a word doesn‚Äôt exist in the vocab?\n\ntokenizer.vocab[\"akash\"]\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 tokenizer.vocab[\"akash\"]\n\nKeyError: 'akash'\n\n\n\nDam, we get a KeyError.\nNot to worry, this is okay, since when calling the tokenizer on the word, it will automatically split the word into word pieces or subwords.\n\ntokenizer(\"akash\")\n\n{'input_ids': [101, 9875, 4095, 102], 'attention_mask': [1, 1, 1, 1]}\n\n\nIt works!\nWe can check what word pieces \"akash\" got broken into with tokenizer.convert_ids_to_tokens(input_ids).\n\ntokenizer.convert_ids_to_tokens(tokenizer(\"akash\").input_ids)\n\n['[CLS]', 'aka', '##sh', '[SEP]']\n\n\nAhhh, it seems \"akash\" was split into two tokens, [\"aka\", \"##sh\"].\nThe \"##\" at the start of \"##sh\" means that the sequence is part of a larger sequence.\nAnd the \"[CLS]\" and \"[SEP]\" tokens are special tokens indicating the start and end of a sequence.\nNow, since tokenizers can deal with any text, what if there was an unknown token?\nFor example, rather than \"pizza\" someone used the pizza emoji üçï?\nLet‚Äôs try!\n\n# Try to tokenize an emoji\ntokenizer.convert_ids_to_tokens(tokenizer(\"üçï\").input_ids)\n\n['[CLS]', '[UNK]', '[SEP]']\n\n\nAhh, we get the special \"[UNK]\" token.\nThis stands for ‚Äúunknown‚Äù.\nThe combination of word pieces and \"[UNK]\" special token means that our tokenizer will be able to turn almost any text into numbers for our model.\n\n\n\n\n\n\nNote\n\n\n\nKeep in mind that just because one tokenizer uses an unknown special token for a particular word or emoji (üçï) doesn‚Äôt mean another will.\n\n\nSince the tokenizer.vocab is a Python dictionary, we can get a sample of the vocabulary using tokenizer.vocab.items().\nHow about we get the first 5?\n\n# Get the first 5 items in the tokenizer vocab\nsorted(tokenizer.vocab.items())[:5]\n\n[('!', 999), ('\"', 1000), ('#', 1001), ('##!', 29612), ('##\"', 29613)]\n\n\nThere‚Äôs our '!' from before! Looks like the first five items are all related to punctuation points.\nHow about a random sample of tokens?\n\nimport random\n\nrandom.sample(sorted(tokenizer.vocab.items()), k=5)\n\n[('##vies', 25929),\n ('responsibility', 5368),\n ('##pm', 9737),\n ('persona', 16115),\n ('rhythm', 6348)]\n\n\n\n\n4.4 Making a preprocessing function to tokenize text\nRather than tokenizing our texts one by one, it‚Äôs best practice to define a preprocessing function which does it for us.\nThis process works regardless of whether you‚Äôre working with text data or other kinds of data such as images or audio.\n\n\n\n\n\n\nTurning data into numbers\n\n\n\nFor any kind of machine learning workflow, an important first step is turning your input data into numbers.\nAs machine learning models are algorithms which find patterns in numbers, before they can find patterns in your data (text, images, audio, tables) it must be numerically encoded first (e.g.¬†tokenizing text).\nTo help with this, transformers has an AutoProcessor class which can preprocess data in a specific format required for a paired model.\n\n\nTo prepare our text data, let‚Äôs create a preprocessing function to take in a dictionary which contains the key \"text\" which has the value of a target string (our data samples come in the form of dictionaries) and then returns the tokenized \"text\".\nWe‚Äôll set the following parameters in our tokenizer:\n\npadding=True - This will make all the sequences in a batch the same length by padding shorter sequences with 0‚Äôs until they equal the longest size in the batch. Why? If there are different size sequences in a batch, you can sometimes run into dimensionality issues.\ntruncation=True - This will shorten sequences longer than the model can handle to the model‚Äôs max input size (e.g.¬†if a sequence is 1000 long and the model can handle 512, it will be shortened to 512 via removing all tokens after 512).\n\nYou can see more parameters available for the tokenizer in the transformers.PreTrainedTokenizer documentation.\n\n\n\n\n\n\nNote\n\n\n\nFor more on padding and truncation (two important concepts in sequence processing), I‚Äôd recommend reading the Hugging Face documentation on Padding and Truncation.\n\n\n\ndef tokenize_text(examples):\n    \"\"\"\n    Tokenize given example text and return the tokenized text.\n    \"\"\"\n    return tokenizer(examples[\"text\"],\n                     padding=True, # pad short sequences to longest sequence in the batch\n                     truncation=True) # truncate long sequences to the maximum length the model can handle\n\nWonderful!\nNow let‚Äôs try it out on an example sample.\n\nexample_sample_2 = {\"text\": \"I love pizza\", \"label\": 1}\n\n# Test the function\ntokenize_text(example_sample_2)\n\n{'input_ids': [101, 1045, 2293, 10733, 102], 'attention_mask': [1, 1, 1, 1, 1]}\n\n\nLooking good!\nHow about we map our tokenize_text function to our whole dataset?\nWe can do so with the datasets.Dataset.map method.\nThe map method allows us to apply a given function to all examples in a dataset.\nBy setting batched=True we can apply the given function to batches of examples (many at a time) to speed up computation time.\nLet‚Äôs create a tokenized_dataset object by calling map on our dataset and passing it our tokenize_text function.\n\n# Map our tokenize_text function to the dataset\ntokenized_dataset = dataset.map(function=tokenize_text, \n                                batched=True, # set batched=True to operate across batches of examples rather than only single examples\n                                batch_size=1000) # defaults to 1000, can be increased if you have a large dataset\n\ntokenized_dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 50\n    })\n})\n\n\nDataset tokenized!\nLet‚Äôs inspect a pair of samples.\n\n# Get two samples from the tokenized dataset\ntrain_tokenized_sample = tokenized_dataset[\"train\"][0]\ntest_tokenized_sample = tokenized_dataset[\"test\"][0]\n\nfor key in train_tokenized_sample.keys():\n    print(f\"[INFO] Key: {key}\")\n    print(f\"Train sample: {train_tokenized_sample[key]}\")\n    print(f\"Test sample: {test_tokenized_sample[key]}\")\n    print(\"\")\n\n[INFO] Key: text\nTrain sample: Set of headphones placed on a desk\nTest sample: A slice of pepperoni pizza with a layer of melted cheese\n\n[INFO] Key: label\nTrain sample: 0\nTest sample: 1\n\n[INFO] Key: input_ids\nTrain sample: [101, 2275, 1997, 2132, 19093, 2872, 2006, 1037, 4624, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nTest sample: [101, 1037, 14704, 1997, 11565, 10698, 10733, 2007, 1037, 6741, 1997, 12501, 8808, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n[INFO] Key: attention_mask\nTrain sample: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nTest sample: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\n\nBeautiful! Our samples have been tokenized.\nNotice the zeroes on the end of the inpud_ids and attention_mask values.\nThese are padding tokens to ensure that each sample has the same length as the longest sequence in a given batch.\nWe can now use these tokenized samples later on in our model.\n\n\n4.5 Tokenization takeaways\nWe‚Äôve now seen and used tokenizers in practice.\nA few takeaways before we start to build a model:\n\nTokenizers are used to turn text (or other forms of data such as images and audio) into a numerical representation ready to be used with a machine learning model.\nMany models reuse existing tokenizers and many models have their own specific tokenizer paired with them. Hugging Face‚Äôs transformers.AutoTokenizer, transformers.AutoProcessor and transformers.AutoModel classes make it easy to pair tokenizers and models based on their name (e.g.¬†distilbert/distilbert-base-uncased).",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#setting-up-an-evaluation-metric",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#setting-up-an-evaluation-metric",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "5 Setting up an evaluation metric",
    "text": "5 Setting up an evaluation metric\nAside from training a model, one of the most important steps in machine learning is evaluating a model.\nTo do, we can use evaluation metrics.\nAn evaluation metric attempts to represent a model‚Äôs performance in a single (or series) of numbers (note, I say ‚Äúattempts‚Äù here because evaluation metrics are useful to guage performance but the real test of a machine learning model is in the real world).\nThere are many different kinds of evaluation metrics for various problems.\nBut since we‚Äôre focused on text classification, we‚Äôll use accuracy as our evaluation metric.\nA model which gets 99/100 predictions correct has an accuracy of 99%.\n\\[\n\\text{Accuracy} = \\frac{\\text{correct classifications}}{\\text{all classifications}}\n\\]\nFor some projects, you may have a minimum standard of a metric.\nFor example, when I worked on an insurance claim classification model, the clients required over 98% accuracy on the test dataset for it to be viable to use in production.\nIf needed, we can craft these evaluation metrics ourselves.\nHowever, Hugging Face has a library called evaluate which has various metrics built in ready to use.\nWe can load a metric using evaluate.load(\"METRIC_NAME\").\nLet‚Äôs load in \"accuracy\" and build a function to measure accuracy by comparing arrays of predictions and labels.\n\nimport evaluate\nimport numpy as np\nfrom typing import Tuple\n\naccuracy_metric = evaluate.load(\"accuracy\")\n\ndef compute_accuracy(predictions_and_labels: Tuple[np.array, np.array]):\n  \"\"\"\n  Computes the accuracy of a model by comparing the predictions and labels.\n  \"\"\"\n  predictions, labels = predictions_and_labels\n\n  # Get highest prediction probability of each prediction if predictions are probabilities\n  if len(predictions.shape) &gt;= 2:\n    predictions = np.argmax(predictions, axis=1)\n\n  return accuracy_metric.compute(predictions=predictions, references=labels)\n\nAccuracy function created!\nNow let‚Äôs test it out.\n\n# Create example list of predictions and labels\nexample_predictions_all_correct = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\nexample_predictions_one_wrong = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\nexample_labels = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n# Test the function\nprint(f\"Accuracy when all predictions are correct: {compute_accuracy((example_predictions_all_correct, example_labels))}\")\nprint(f\"Accuracy when one prediction is wrong: {compute_accuracy((example_predictions_one_wrong, example_labels))}\")\n\nAccuracy when all predictions are correct: {'accuracy': 1.0}\nAccuracy when one prediction is wrong: {'accuracy': 0.9}\n\n\nExcellent, our function works just as we‚Äôd like.\nWhen all predictions are correct, it scores 1.0 (or 100% accuracy) and when 9/10 predictions are correct, it returns 0.9 (or 90% accuracy).\nWe can use this function during training and evaluation of our model.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#setting-up-a-model-for-training",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#setting-up-a-model-for-training",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "6 Setting up a model for training",
    "text": "6 Setting up a model for training\nWe‚Äôve gone through the important steps of setting data up for training (and evaluation).\nNow let‚Äôs prepare a model.\nWe‚Äôll keep going through the following steps:\n\n‚úÖ Create and preprocess data.\nDefine the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\nDefine training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nLet‚Äôs start by creating an instance of a model.\nSince we‚Äôre working on text classification, we‚Äôll do so with transformers.AutoModelForSequenceClassification (where sequence classification means a sequence of something, e.g.¬†our sequences of text).\nWe can use the from_pretrained() method to instatiate a pretrained model from the Hugging Face Hub.\n\n\n\n\n\n\nNote\n\n\n\nThe ‚Äúpretrained‚Äù in transformers.AutoModelForSequenceClassification.from_pretrained means acquiring a model which has already been trained on a certain dataset.\nThis is common practice in many machine learning projects and is known as transfer learning.\nThe idea is to take an existing model which works well on a task similar to your target task and then fine-tune it to work even better on your target task.\nIn our case, we‚Äôre going to use the pretrained DistilBERT base model (distilbert/distilbert-base-uncased) which has been trained on many thousands of books as well as a version of the English Wikipedia (millions of words).\nThis training gives it a very good baseline representation of the patterns in language.\nWe‚Äôll take this baseline representation of the patterns in language and adjust it slightly to focus specifically on predicting whether an image caption is about food or not (based on the words it contains).\nThe main two benefits of using transfer learning are:\n\nAbility to get good results with smaller amounts of data (since the main representations are learned on a larger dataset, we only have to show the model a few examples of our specific problem).\nThis process can be repeated acorss various domains and tasks. For example, you can take a computer vision model trained on millions of images and customize it to your own use case. Or an audio model trained on many different nature sounds and customize it specifically for birds.\n\n\n\n\n\nTransfer learning is the process of taking what one model has learned from a (typically large) dataset and applying them to your own custom dataset. This process can be replicated across domains such as computer vision, natural language processing and more.\n\n\nSo when starting a new machine learning project, one of the first questions you should ask is: does an existing pretrained model similar to my task exist and can I fine-tune it for my own task?\nFor an end-to-end example of transfer learning in PyTorch (another popular deep learning framework), see PyTorch Transfer Learning.\n\n\nTime to setup our model instance.\nA few things to note:\n\nWe‚Äôll use transformers.AutoModelForSequenceClassification.from_pretrained, this will create the model architecture we specify with the pretrained_model_name_or_path parameter.\nThe AutoModelForSequenceClassification class comes with a classification head on top of our mdoel (so we can customize this to the number of classes we have with the num_labels parameter).\nUsing from_pretrained will also call the transformers.PretrainedConfig class which will enable us to set id2label and label2id parameters for our fine-tuning task.\n\nLet‚Äôs refresh what our id2label and label2id objects look like.\n\n# Get id and label mappings\nprint(f\"id2label: {id2label}\")\nprint(f\"label2id: {label2id}\")\n\nid2label: {0: 'not_food', 1: 'food'}\nlabel2id: {'not_food': 0, 'food': 1}\n\n\nBeautiful, we can pass these mappings to transformers.AutoModelForSequenceClassification.from_pretrained.\n\nfrom transformers import AutoModelForSequenceClassification\n\n# Setup model for fine-tuning with classification head (top layers of network)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n    num_labels=2, # can customize this to the number of classes in your dataset\n    id2label=id2label, # mappings from class IDs to the class labels (for classification tasks)\n    label2id=label2id\n)\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nModel created!\nYou‚Äôll notice that a warning message gets displayed:\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: [‚Äòclassifier.bias‚Äô, ‚Äòclassifier.weight‚Äô, ‚Äòpre_classifier.bias‚Äô, ‚Äòpre_classifier.weight‚Äô] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nThis is essentially saying ‚Äúhey, some of the layers in this model are newly initialized (with random patterns) and you should probably customize them to your own dataset‚Äù.\nThis happens because we used the AutoModelForSequenceClassification class.\nWhilst the majority of the layers in our model have already learned patterns from a large corpus of text, the top layers (classifier layers) have been randomly setup so we can customize them on our own.\n\n\n\n\nVarious forms of training paradigms. Typically you‚Äôll start with a model that has been pretrained on a large dataset. For example, a base model could be one that has read all of Wikipedia + 1000‚Äôs of books (like our DistilBert model) and has a good general representation of language data. This representation can then be tailored to a specific use case by customizing the outputs and adjusting the representation slightly by feeding it custom data. This process is often referred to as fine-tuning.\n\n\nLet‚Äôs try and make a prediction with our model and see what happens.\n\n# Try and make a prediction with the loaded model (this will error)\nmodel(**tokenized_dataset[\"train\"][0])\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[40], line 2\n      1 # Try and make a prediction with the loaded model (this will error)\n----&gt; 2 model(**tokenized_dataset[\"train\"][0])\n\nFile ~/miniconda3/envs/learn_hf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1552 else:\n-&gt; 1553     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/learn_hf/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)\n   1557 # If we don't have any hooks, we want to skip the rest of the logic in\n   1558 # this function, and just call forward.\n   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1560         or _global_backward_pre_hooks or _global_backward_hooks\n   1561         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1562     return forward_call(*args, **kwargs)\n   1564 try:\n   1565     result = None\n\nTypeError: DistilBertForSequenceClassification.forward() got an unexpected keyword argument 'text'\n\n\n\nOh no! We get an error.\nNot to worry, this is only because our model hasn‚Äôt been trained on our own dataset yet.\nLet‚Äôs take a look at the layers in our model.\n\n# Inspect the model \nmodel\n\nDistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n\n\nYou‚Äôll notice that the model comes in 3 main parts (data flows through these sequentially):\n\nembeddings - This part of the model turns the input tokens into a learned representation. So rather than just a list of integers, the values become a learned representation. This learned representation comes from the base model learning how different words and word pieces relate to eachother thanks to its training data. The size of (30522, 768) means the 30,522 words in the vocabulary are all represented by vectors of size 768 (one word gets represented by 768 numbers, these are often not human interpretable).\ntransformer - This is the main body of the model. There are several TransformerBlock layers stacked on top of each other. These layers attempt to learn a deeper representation of the data going through the model. A thorough breakdown of these layers is beyond the scope of this tutorial, however, for and in-depth guide on Transformer-based models, I‚Äôd recommend reading Transformers from scratch by Peter Bloem, going through Andrej Karpathy‚Äôs lecture on Transformers and their history or reading the original Attention is all you need paper (this is the paper that introduced the Transformer architecture).\nclassifier - This is what is going to take the representation of the data and compress it into our number of target classes (notice out_features=2, this means that we‚Äôll get two output numbers, one for each of our classes).\n\nFor more on the entire DistilBert architecture and its training setup, I‚Äôd recommend reading the DistilBert paper from the Hugging Face team.\nRather than breakdown the model itself, we‚Äôre focused on using it for a particular task (classifying text).\n\n6.1 Counting the parameters of our model\nBefore we move into training, we can get another insight into our model by counting its number of parameters.\nLet‚Äôs create a small function to count the number of trainable (these will update during training) and total parameters in our model.\n\ndef count_params(model):\n    \"\"\"\n    Count the parameters of a PyTorch model.\n    \"\"\"\n    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_parameters = sum(p.numel() for p in model.parameters())\n\n    return {\"trainable_parameters\": trainable_parameters, \"total_parameters\": total_parameters}\n\n# Count the parameters of the model\ncount_params(model)\n\n{'trainable_parameters': 66955010, 'total_parameters': 66955010}\n\n\nNice!\nLooks like our model has a total of 66,955,010 parameters and all of them are trainable.\nA parameter is a numerical value in a model which is capable of being updated to better represent the input data.\nI like to think of them as a small opportunity to learn patterns in the data.\nIf a model has three parameters, it has three small opportunities to learn patterns in the data.\nWhereas, if a model has 60,000,000+ (60M) parameters (like our model), it has 60,000,000+ small opportunities to learn patterns in the data.\nSome models such as Large Language Models (LLMs) like Llama 3 70B have 70,000,000,000+ (70B) parameters (over 1000x our model).\nIn essence, the more parameters a model has, the more opportunities it has to learn (generally).\nMore parameters often results in more capabilities.\nHowever, more parameters also often results in a much larger model size (e.g.¬†many gigabytes versus hundreds of megabytes) as well as a much longer compute time (fewer samples per second).\nFor our use case, a binary text classification task, 60M parameters is more than enough.\n\n\n\n\n\n\nNote\n\n\n\nWhy count the parameters in a model?\nWhile it may be tempting to always go with a model that has the most parameters, there are many considerations to take into account before doing so.\n\nWhat hardware is the model going to run on?\n\nIf you need the model to run on cheap hardware, you‚Äôll likely want a smaller model.\n\nHow fast do you need the model to be?\n\nIf you need 100-1000s of predictions per second, you‚Äôll likely want a smaller model.\n\n‚ÄúI don‚Äôt mind about speed or cost, I just want quality.‚Äù\n\nGo with the biggest model you can.\nHowever, often times you can get really good results by training a small model to do a specific task using quality data than by just always using a large model.\n\n\n\n\n6.2 Create a directory for saving models\nTraining a model can take a while.\nSo we‚Äôll want a place to save our models.\nLet‚Äôs create a directory called \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\" (it‚Äôs a bit verbose and you can change this if you like but I like to be specific).\n\n# Create model output directory\nfrom pathlib import Path\n\n# Create models directory\nmodels_dir = Path(\"models\")\nmodels_dir.mkdir(exist_ok=True)\n\n# Create model save name\nmodel_save_name = \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Create model save path\nmodel_save_dir = Path(models_dir, model_save_name)\n\nmodel_save_dir\n\nPosixPath('models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased')\n\n\n\n\n6.3 Setting up training arguments with TrainingArguments\nTime to get our model ready for training!\nWe‚Äôre up to step 3 of our process:\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\nDefine training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nThe transformers.TrainingArguments class contains a series of helpful items, including hyperparameter settings and model saving strategies to use throughout training.\nIt has many parameters, too many to explain here.\nHowever, the following table breaks down a helpful handful.\nSome of the parameters we‚Äôll set are the same as the defaults (this is on purpose as the defaults are often pretty good), some such as learning_rate are different.\n\n\n\nParameter\nExplanation\n\n\n\n\noutput_dir\nName of output directory to save the model and checkpoints to. For example, learn_hf_food_not_food_text_classifier_model.\n\n\nlearning_rate\nValue of the initial learning rate to use during training. Passed to transformers.AdamW. Initial learning rate because the learning rate can be dynamic during training. The ideal learning is experimental in nature. Defaults to 5e-5 or 0.00001 but we‚Äôll use 0.0001.\n\n\nper_device_train_batch_size\nSize of batches to place on target device during training. For example, a batch size of 32 means the model will look at 32 samples at a time. A batch size too large will result in out of memory issues (e.g.¬†your GPU can‚Äôt handle holding a large number of samples in memory at a time).\n\n\nper_device_eval_batch_size\nSize of batches to place on target device during evaluation. Can often be larger than during training because no gradients are being calculated. For example, training batch size could be 32 where as evaluation batch size may be able to be 128 (4x larger). Though these are only esitmates.\n\n\nnum_train_epochs\nNumber of times to pass over the data to try and learn patterns. For example, if num_train_epochs=10, the model will do 10 full passes of the training data. Because we‚Äôre working with a small dataset, 10 epochs should be fine to begin with. However, if you had a larger dataset, you may want to do a few experiments using less data (e.g.¬†10% of the data) for a smaller number of epochs to make sure things work.\n\n\neval_strategy\nWhen to evaluate the model on the evaluation data. If eval_strategy=\"epoch\", the model will be evaluated every epoch. See the documentation for more options. Note: This was previously called evaluation_strategy but was shortened in transformers==4.46.\n\n\nsave_strategy\nWhen to save a model checkpoint. If save_strategy=\"epoch\", a checkpoint will be saved every epoch. See the documentation for more save options.\n\n\nsave_total_limit\nNumber of total amount of checkpoints to save (so we don‚Äôt save num_train_epochs checkpoints). For example, can limit to 3 saves so the total number of saves are the 3 most recent as well as the best performing checkpoint (as per load_best_model_at_end).\n\n\nuse_cpu\nSet to False by default, will use CUDA GPU (torch.device(\"cuda\")) or MPS device (torch.device(\"mps\"), for Mac) if available. This is because training is generally faster on an accelerator device.\n\n\nseed\nSet to 42 by default for reproducibility. Meaning that subsequent runs with the same setup should achieve the same results.\n\n\nload_best_model_at_end\nWhen set to True, makes sure that the best model found during training is loaded when training finishes. This will mean the best model checkpoint gets saved regardless of what epoch it happened on. This is set to False by default.\n\n\nlogging_strategy\nWhen to log the training results and metrics. For example, if logging_strategy=\"epoch\", results will be logged as outputs every epoch. See the documentation for more logging options.\n\n\nreport_to\nLog experiments to various experiment tracking services. For example, you can log to Weights & Biases using report_to=\"wandb\". We‚Äôll turn this off for now and keep logging to a local directory by setting report_to=\"none\".\n\n\npush_to_hub\nAutomatically upload the model to the Hugging Face Hub every time the model is saved. We‚Äôll set push_to_hub=False as we‚Äôll see how to do this manually later on. See the documentation for more options on saving models to the Hugging Face Hub.\n\n\nhub_token\nAdd your Hugging Face Hub token to push a model to the Hugging Face Hub with push_to_hub (will default to huggingface-cli login details).\n\n\nhub_private_repo\nWhether or not to make the Hugging Face Hub repository private or public, defaults to False (e.g.¬†set to True if you want the repository to be private).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo get more familiar with the transformers.TrainingArguments class, I‚Äôd highly recommend reading the documentation for 15-20 minutes. Perhaps over a couple of sessions. There are quite a large number of parameters which will be helpful to be aware of.\n\n\nPhew!\nThat was a lot to take in.\nBut let‚Äôs now practice setting up our own instance of transformers.TrainingArguments.\n\nfrom transformers import TrainingArguments\n\nprint(f\"[INFO] Saving model checkpoints to: {model_save_dir}\")\n\n# Create training arguments\ntraining_args = TrainingArguments(\n    output_dir=model_save_dir,\n    learning_rate=0.0001,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    eval_strategy=\"epoch\", # was previously \"evaluation_strategy\"\n    save_strategy=\"epoch\",\n    save_total_limit=3, # limit the total amount of save checkpoints (so we don't save num_epochs checkpoints)\n    use_cpu=False, # set to False by default, will use CUDA GPU or MPS device if available\n    seed=42, # set to 42 by default for reproducibility\n    load_best_model_at_end=True, # load the best model when finished training\n    logging_strategy=\"epoch\", # log training results every epoch\n    report_to=\"none\", # optional: log experiments to Weights & Biases/other similar experimenting tracking services (we'll turn this off for now) \n    # push_to_hub=True # optional: automatically upload the model to the Hub (we'll do this manually later on)\n    # hub_token=\"your_token_here\" # optional: add your Hugging Face Hub token to push to the Hub (will default to huggingface-cli login)\n    hub_private_repo=False # optional: make the uploaded model private (defaults to False)\n)\n\n# Optional: Print out training_args to inspect (warning, it is quite a long output)\n# training_args\n\n[INFO] Saving model checkpoints to: models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n\n\nTraining arguments created!\nLet‚Äôs put them to work in an instance of transformers.Trainer.\n\n\n6.4 Setting up an instance of Trainer\nTime for step 4!\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\n‚úÖ Define training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nThe transformers.Trainer class allows you to train models.\nIt‚Äôs built on PyTorch so it gets to leverage all of the powerful PyTorch toolkit.\nBut since it also works closely with the transformers.TrainingArguments class, it offers many helpful features.\n\n\n\n\n\n\nNote\n\n\n\ntransformers.Trainer can work with torch.nn.Module models, however, it is designed to work best with transformers.PreTrainedModel‚Äôs from the transformers library.\nThis is not a problem for us as we‚Äôre using transformers.AutoModelForSequenceClassification.from_pretrained which loads a transformers.PreTrainedModel.\nSee the transformers.Trainer documentation for tips on how to make sure your model is compatible.\n\n\n\n\n\n\n\n\n\nParameter\nExplanation\n\n\n\n\nmodel\nThe model we‚Äôd like to train. Works best with an instance of transformers.PreTrainedModel. Most models loaded using from_pretrained will be of this type.\n\n\nargs\nInstance of transformers.TrainingArguments. We‚Äôll use the training_args object we defined earlier. But if this is not set, it will default to the default settings for transformers.TrainingArguments.\n\n\ntrain_dataset\nDataset to use during training. We can use our tokenized_dataset[\"train\"] as it has already been preprocessed.\n\n\neval_dataset\nDataset to use during evaluation (our model will not see this data during training). We can use our tokenized_dataset[\"test\"] as it has already been preprocessed.\n\n\ntokenizer\nThe tokenizer which was used to preprocess the data. Passing a tokenizer will also pad the inputs to maximum length when batching them. It will also be saved with the model so future re-runs are easier.\n\n\ncompute_metrics\nAn evaluation function to evaluate a model during training and evaluation steps. In our case, we‚Äôll use the compute_accuracy function we defined earlier.\n\n\n\nWith all this being said, let‚Äôs build our Trainer!\n\nfrom transformers import Trainer\n\n# Setup Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    # Note: the 'tokenizer' parameter will be changed to 'processing_class' in Transformers v5.0.0\n    tokenizer=tokenizer, # Pass tokenizer to the Trainer for dynamic padding (padding as the training happens) (see \"data_collator\" in the Trainer docs)\n    compute_metrics=compute_accuracy\n)\n\nWoohoo! We‚Äôve created our own trainer.\nWe‚Äôre one step closer to training!\n\n\n6.5 Training our text classification model\nWe‚Äôve done most of the hard word setting up our transformers.TrainingArguments as well as our transformers.Trainer.\nNow how about we train a model?\nFollowing our steps:\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\n‚úÖ Define training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\n‚úÖ Pass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nLooks like all we have to do is call transformers.Trainer.train().\nWe‚Äôll be sure to save the results of the training to a variable results so we can inspect them later.\nLet‚Äôs try!\n\n# Train a text classification model\nresults = trainer.train()\n\n\n      \n      \n      [70/70 00:07, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.328100\n0.039627\n1.000000\n\n\n2\n0.019200\n0.005586\n1.000000\n\n\n3\n0.003700\n0.002026\n1.000000\n\n\n4\n0.001700\n0.001186\n1.000000\n\n\n5\n0.001100\n0.000858\n1.000000\n\n\n6\n0.000800\n0.000704\n1.000000\n\n\n7\n0.000800\n0.000619\n1.000000\n\n\n8\n0.000700\n0.000571\n1.000000\n\n\n9\n0.000600\n0.000547\n1.000000\n\n\n10\n0.000600\n0.000539\n1.000000\n\n\n\n\n\n\nWoahhhh!!!\nHow cool is that!\nWe just trained a text classification model!\nAnd it looks like the training went pretty quick (thanks to our smaller dataset and relatively small model, for larger datasets, training would likely take longer).\nHow about we check some of the metrics?\nWe can do so using the results.metrics attribute (this returns a Python dictionary with stats from our training run).\n\n# Inspect training metrics\nfor key, value in results.metrics.items():\n    print(f\"{key}: {value}\")\n\ntrain_runtime: 7.5421\ntrain_samples_per_second: 265.177\ntrain_steps_per_second: 9.281\ntotal_flos: 18110777160000.0\ntrain_loss: 0.03574410408868321\nepoch: 10.0\n\n\nNice!\nLooks like our overall training runtime is low because of our small dataset.\nAnd looks like our trainer was able to process a fair few samples per second.\nIf we were to 1000x the size of our dataset (e.g.¬†~250 samples -&gt; ~250,000 samples which is quite a substantial dataset), it seems our training time still wouldn‚Äôt take too long.\nThe total_flos stands for ‚Äúfloating point operations‚Äù (also referred to as FLOPS), this is the total number of calculations our model has performed to find patterns in the data. And as you can see, it‚Äôs quite a large number!\n\n\n\n\n\n\nNote\n\n\n\nDepending on the hardware you‚Äôre using, the results with respect to train_runtime, train_samples_per_second and train_steps_per_second will likely be different.\nThe faster your accelerator hardware (e.g.¬†NVIDIA GPU or Mac GPU), the lower your runtime and higher your samples/steps per second will be.\nFor reference, on my local NVIDIA RTX 4090, I get a train_runtime of 8-9 seconds, train_samples_per_second of 230-250 and train_steps_per_second of 8.565.\n\n\n\n\n6.6 Save the model for later use\nNow our model has been trained, let‚Äôs save it for later use.\nWe‚Äôll save it locally first and push it to the Hugging Face Hub later.\nWe can save our model using the transformers.Trainer.save_model method.\n\n# Save model\nprint(f\"[INFO] Saving model to {model_save_dir}\")\ntrainer.save_model(output_dir=model_save_dir)\n\n[INFO] Saving model to models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n\n\nModel saved locally! Before we save it to the Hugging Face Hub, let‚Äôs check out its metrics.\n\n\n6.7 Inspecting the model training metrics\nWe can get a log of our model‚Äôs training state using trainer.state.log_history.\nThis will give us a collection of metrics per epoch (as long as we set logging_strategy=\"epoch\" in transformers.TrainingArguments), in particular, it will give us a loss value per epoch.\nWe can extract these values and inspect them visually for a better understanding our model training.\nLet‚Äôs get the training history and inspect it.\n\n# Get training history \ntrainer_history_all = trainer.state.log_history \ntrainer_history_metrics = trainer_history_all[:-1] # get everything except the training time metrics (we've seen these already)\ntrainer_history_training_time = trainer_history_all[-1] # this is the same value as results.metrics from above\n\n# View the first 4 metrics from the training history\ntrainer_history_metrics[:4]\n\n[{'loss': 0.3281,\n  'grad_norm': 0.6938912272453308,\n  'learning_rate': 9e-05,\n  'epoch': 1.0,\n  'step': 7},\n {'eval_loss': 0.03962664306163788,\n  'eval_accuracy': 1.0,\n  'eval_runtime': 0.0135,\n  'eval_samples_per_second': 3707.312,\n  'eval_steps_per_second': 148.292,\n  'epoch': 1.0,\n  'step': 7},\n {'loss': 0.0192,\n  'grad_norm': 0.14873287081718445,\n  'learning_rate': 8e-05,\n  'epoch': 2.0,\n  'step': 14},\n {'eval_loss': 0.005585948005318642,\n  'eval_accuracy': 1.0,\n  'eval_runtime': 0.0147,\n  'eval_samples_per_second': 3399.06,\n  'eval_steps_per_second': 135.962,\n  'epoch': 2.0,\n  'step': 14}]\n\n\nOkay, looks like the metrics are logged every epochs in a list Python dictionaries with interleaving loss (this is the training set loss) and eval_loss values.\nHow about we write some code to separate the training set metrics and the evaluation set metrics?\n\nimport pprint # import pretty print for nice printing of lists\n\n# Extract training and evaluation metrics\ntrainer_history_training_set = []\ntrainer_history_eval_set = []\n\n# Loop through metrics and filter for training and eval metrics\nfor item in trainer_history_metrics:\n    item_keys = list(item.keys())\n    # Check to see if \"eval\" is in the keys of the item\n    if any(\"eval\" in item for item in item_keys):\n        trainer_history_eval_set.append(item)\n    else:\n        trainer_history_training_set.append(item)\n\n# Show the first two items in each metric set\nprint(f\"[INFO] First two items in training set:\")\npprint.pprint(trainer_history_training_set[:2])\n\nprint(f\"\\n[INFO] First two items in evaluation set:\")\npprint.pprint(trainer_history_eval_set[:2])\n\n[INFO] First two items in training set:\n[{'epoch': 1.0,\n  'grad_norm': 0.6938912272453308,\n  'learning_rate': 9e-05,\n  'loss': 0.3281,\n  'step': 7},\n {'epoch': 2.0,\n  'grad_norm': 0.14873287081718445,\n  'learning_rate': 8e-05,\n  'loss': 0.0192,\n  'step': 14}]\n\n[INFO] First two items in evaluation set:\n[{'epoch': 1.0,\n  'eval_accuracy': 1.0,\n  'eval_loss': 0.03962664306163788,\n  'eval_runtime': 0.0135,\n  'eval_samples_per_second': 3707.312,\n  'eval_steps_per_second': 148.292,\n  'step': 7},\n {'epoch': 2.0,\n  'eval_accuracy': 1.0,\n  'eval_loss': 0.005585948005318642,\n  'eval_runtime': 0.0147,\n  'eval_samples_per_second': 3399.06,\n  'eval_steps_per_second': 135.962,\n  'step': 14}]\n\n\nBeautiful!\nHow about we take it a step further and turn our metrics into pandas DataFrames so we can view them easier?\n\n# Create pandas DataFrames for the training and evaluation metrics\ntrainer_history_training_df = pd.DataFrame(trainer_history_training_set)\ntrainer_history_eval_df = pd.DataFrame(trainer_history_eval_set)\n\ntrainer_history_training_df.head() \n\n\n\n\n\n\n\n\nloss\ngrad_norm\nlearning_rate\nepoch\nstep\n\n\n\n\n0\n0.3281\n0.693891\n0.00009\n1.0\n7\n\n\n1\n0.0192\n0.148733\n0.00008\n2.0\n14\n\n\n2\n0.0037\n0.037808\n0.00007\n3.0\n21\n\n\n3\n0.0017\n0.022227\n0.00006\n4.0\n28\n\n\n4\n0.0011\n0.018665\n0.00005\n5.0\n35\n\n\n\n\n\n\n\nNice!\nAnd the evaluation DataFrame?\n\ntrainer_history_eval_df.head()\n\n\n\n\n\n\n\n\neval_loss\neval_accuracy\neval_runtime\neval_samples_per_second\neval_steps_per_second\nepoch\nstep\n\n\n\n\n0\n0.039627\n1.0\n0.0135\n3707.312\n148.292\n1.0\n7\n\n\n1\n0.005586\n1.0\n0.0147\n3399.060\n135.962\n2.0\n14\n\n\n2\n0.002026\n1.0\n0.0136\n3680.635\n147.225\n3.0\n21\n\n\n3\n0.001186\n1.0\n0.0151\n3303.902\n132.156\n4.0\n28\n\n\n4\n0.000858\n1.0\n0.0159\n3146.137\n125.845\n5.0\n35\n\n\n\n\n\n\n\nAnd of course, we‚Äôll have follow the data explorer‚Äôs motto of visualize, visualize, visualize! and inspect our loss curves.\n\n# Plot training and evaluation loss\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(trainer_history_training_df[\"epoch\"], trainer_history_training_df[\"loss\"], label=\"Training loss\")\nplt.plot(trainer_history_eval_df[\"epoch\"], trainer_history_eval_df[\"eval_loss\"], label=\"Evaluation loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Text classification with DistilBert training and evaluation loss over time\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nB-e-a-utiful!\nThat is exactly what we wanted.\nTraining and evaluation loss going down over time.\n\n\n6.8 Pushing our model to the Hugging Face Hub\nWe‚Äôve saved our model locally and confirmed that it seems to be performing well on our training metrics but how about we push it to the Hugging Face Hub?\nThe Hugging Face Hub is one of the best sources of machine learning models on the internet.\nAnd we can add our model there so others can use it or we can access it in the future (we could also keep it private on the Hugging Face Hub so only people from our organization can use it).\nSharing models on Hugging Face is also a great way to showcase your skills as a machine learning engineer, it gives you something to show potential employers and say ‚Äúhere‚Äôs what I‚Äôve done‚Äù.\n\n\n\n\n\n\nNote\n\n\n\nBefore sharing a model to the Hugging Face Hub, be sure to go through the following steps:\n\nSetup a Hugging Face token using the huggingface-cli login command.\nRead through the user access tokens guide.\nSet up an access token via https://huggingface.co/settings/tokens (ensure it has ‚Äúwrite‚Äù access).\n\nIf you are using Google Colab, you can add your token under the ‚ÄúSecrets‚Äù tab on the left.\nOn my local computer, my token is saved to /home/daniel/.cache/huggingface/token (thanks to running huggingface-cli login on the command line).\nAnd for more on sharing models to the Hugging Face Hub, be sure to check out the model sharing documentation.\n\n\nWe can push our model, tokenizer and other assosciated files to the Hugging Face Hub using the transformers.Trainer.push_to_hub method.\nWe can also optionally do the following:\n\nAdd a model card (something that describes how the model was created and what it can be used for) using transformers.Trainer.create_model_card.\nAdd a custom README.md file to the model repository to explain more details about the model using huggingface_hub.HfApi.upload_file. This method is similar to model card creation method above but with more customization.\n\nLet‚Äôs save our model to the Hub!\n\n# Save our model to the Hugging Face Hub\n# This will be public, since we set hub_private_repo=False in our TrainingArguments\nmodel_upload_url = trainer.push_to_hub(\n    commit_message=\"Uploading food not food text classifier model\",\n    # token=\"YOUR_HF_TOKEN_HERE\" # This will default to the token you have saved in your Hugging Face config\n)\nprint(f\"[INFO] Model successfully uploaded to Hugging Face Hub with at URL: {model_upload_url}\")\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased/commit/8a8a8aff5bdee5bc518e31558447dc684d448b8f', commit_message='Uploading food not food text classifier model', commit_description='', oid='8a8a8aff5bdee5bc518e31558447dc684d448b8f', pr_url=None, pr_revision=None, pr_num=None)\n\n\nModel pushed to the Hugging Face Hub!\n\n\n\n\n\n\nNote\n\n\n\nYou may see the following error:\n\n403 Forbidden: You don‚Äôt have the rights to create a model under the namespace ‚Äúmrdbourke‚Äù. Cannot access content at: https://huggingface.co/api/repos/create. If you are trying to create or update content, make sure you have a token with the write role.\n\nOr even:\n\nHfHubHTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6699c52XXXXXX)\nInvalid username or password.\n\nIn this case, be sure to go through the setup steps above to make sure you have a Hugging Face access token with ‚Äúwrite‚Äù access.\n\n\nAnd since it‚Äôs public (by default), you can see it at https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased (it gets saved to the same name as our target local directory).\nYou can now share and interact with this model online.\nAs well as download it for use in your own applications.\n\n\n\n\nThe Hugging Face Hub allows us to store and share models, datasets and demos. We can set these to be private or public. Models stored on the Hub can easily be accessed via Hugging Face Transformers.\n\n\nBut before we make an application/demo with our trained model, let‚Äôs keep evaluating it.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#making-and-evaluating-predictions-on-the-test-data",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#making-and-evaluating-predictions-on-the-test-data",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "7 Making and evaluating predictions on the test data",
    "text": "7 Making and evaluating predictions on the test data\nModel trained, let‚Äôs now evaluate it on the test data.\nOr step 7 in our workflow:\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\n‚úÖ Define training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\n‚úÖ Pass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\n‚úÖ Train the model by calling Trainer.train().\n‚úÖ Save the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nA reminder that the test data is data that our model has never seen before.\nSo it will be a good estimate of how our model will do in a production setting.\nWe can make predictions on the test dataset using transformers.Trainer.predict.\nAnd then we can get the prediction values with the predictions attribute and assosciated metrics with the metrics attribute.\n\n# Perform predictions on the test set\npredictions_all = trainer.predict(tokenized_dataset[\"test\"])\nprediction_values = predictions_all.predictions\nprediction_metrics = predictions_all.metrics\n\nprint(f\"[INFO] Prediction metrics on the test data:\")\nprediction_metrics\n\n\n\n\n[INFO] Prediction metrics on the test data:\n\n\n{'test_loss': 0.0005385442636907101,\n 'test_accuracy': 1.0,\n 'test_runtime': 0.0421,\n 'test_samples_per_second': 1186.857,\n 'test_steps_per_second': 47.474}\n\n\nWoah!\nLooks like our model did an outstanding job!\nAnd it was very quick too.\nThis is one of the benefits of using a smaller pretrained model and customizing it to your own dataset.\nYou can achieve outstanding results in a very quick time as well as have a model capable of performing thousands of predictions per second.\nWe can also calculate the accuracy by hand by comparing the prediction labels to the test labels.\nTo do so, we‚Äôll:\n\nCalculate the prediction probabilities (though this is optional as we could skip straight to 2 and get the same results) by passing the prediction_values to torch.softmax.\nFind the index of the prediction value with the highest value (the index will be equivalent to the predicted label) using torch.argmax (we could also use np.argmax here) to find the predicted labels.\nGet the true labels from the test dataset using dataset[\"test\"][\"label\"].\nCompare the predicted labels from 2 to the true labels from 3 using sklearn.metrics.accuracy_score to find the accuracy.\n\n\nimport torch\nfrom sklearn.metrics import accuracy_score\n\n# 1. Get prediction probabilities (this is optional, could get the same results with step 2 onwards)\npred_probs = torch.softmax(torch.tensor(prediction_values), dim=1)\n\n# 2. Get the predicted labels\npred_labels = torch.argmax(pred_probs, dim=1)\n\n# 3. Get the true labels\ntrue_labels = dataset[\"test\"][\"label\"]\n\n# 4. Compare predicted labels to true labels to get the test accuracy\ntest_accuracy = accuracy_score(y_true=true_labels, \n                               y_pred=pred_labels)\n\nprint(f\"[INFO] Test accuracy: {test_accuracy*100}%\")\n\n[INFO] Test accuracy: 100.0%\n\n\nWoah!\nLooks like our model performs really well on our test set.\nIt will be interesting to see how it goes on real world samples.\nWe‚Äôll test this later on.\nHow about we make a pandas DataFrame out of our test samples, predicted labels and predicted probabilities to further inspect our results?\n\n# Make a DataFrame of test predictions\ntest_predictions_df = pd.DataFrame({\n    \"text\": dataset[\"test\"][\"text\"],\n    \"true_label\": true_labels,\n    \"pred_label\": pred_labels,\n    \"pred_prob\": torch.max(pred_probs, dim=1).values\n})\n\ntest_predictions_df.head()\n\n\n\n\n\n\n\n\ntext\ntrue_label\npred_label\npred_prob\n\n\n\n\n0\nA slice of pepperoni pizza with a layer of mel...\n1\n1\n0.999369\n\n\n1\nRed brick fireplace with a mantel serving as a...\n0\n0\n0.999662\n\n\n2\nA bowl of sliced bell peppers with a sprinkle ...\n1\n1\n0.999365\n\n\n3\nSet of mugs hanging on a hook\n0\n0\n0.999682\n\n\n4\nStanding floor lamp providing light next to an...\n0\n0\n0.999678\n\n\n\n\n\n\n\nWe can find the examples with the lowest prediction probability to see where the model is unsure.\n\n# Show 10 examples with low prediction probability\ntest_predictions_df.sort_values(\"pred_prob\", ascending=True).head(10)\n\n\n\n\n\n\n\n\ntext\ntrue_label\npred_label\npred_prob\n\n\n\n\n40\nA bowl of cherries with a sprig of mint for ga...\n1\n1\n0.999331\n\n\n11\nA close-up shot of a cheesy pizza slice being ...\n1\n1\n0.999348\n\n\n26\nA fruit platter with a variety of exotic fruit...\n1\n1\n0.999351\n\n\n42\nBoxes of apples, pears, pineapple, manadrins a...\n1\n1\n0.999353\n\n\n46\nA bowl of sliced kiwi with a sprinkle of sugar...\n1\n1\n0.999360\n\n\n37\nClose-up of a sushi roll with avocado, cucumbe...\n1\n1\n0.999360\n\n\n31\nCrunchy sushi roll with tempura flakes or pank...\n1\n1\n0.999360\n\n\n9\nCherry tomatoes and mozzarella balls in a bowl...\n1\n1\n0.999360\n\n\n14\nTwo handfuls of bananas in a fruit bowl with g...\n1\n1\n0.999360\n\n\n44\nSeasonal sushi roll with ingredients like pers...\n1\n1\n0.999361\n\n\n\n\n\n\n\nHmmm, it looks like our model has quite a high prediction probability for almost all samples.\nWe can further evalaute our model by making predictions on new custom data.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#making-and-inspecting-predictions-on-custom-text-data",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#making-and-inspecting-predictions-on-custom-text-data",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "8 Making and inspecting predictions on custom text data",
    "text": "8 Making and inspecting predictions on custom text data\nWe‚Äôve seen how our model performs on the test dataset (quite well).\nBut how might we check its performance on our own custom data?\nFor example, text-based image captions from the wild.\nWell, we‚Äôve got two ways to load our model now too:\n\nLoad model locally from our computer (e.g.¬†via models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased).\nLoad model from Hugging Face Hub (e.g.¬†via mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased).\n\nEither way of loading the model results in the same outcome: being able to make predictions on given data.\nSo how about we start by setting up our model paths for both local loading and loading from the Hugging Face Hub.\n\n# Setup local model path\nlocal_model_path = \"models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Setup Hugging Face model path (see: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased)\n# Note: Be sure to change \"mrdbourke\" to your own Hugging Face username\nhuggingface_model_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n\n8.1 Discussing ways to make predictions (inference)\nWhen we‚Äôve loaded our trained model, because of the way we‚Äôve set it up, there are two main ways to make predictions on custom data:\n\nPipeline mode using transformers.pipeline and passing it our target model, this allows us to preprocess custom data and make predictions in one step.\nPyTorch mode using a combination of transformers.AutoTokenizer and transformers.AutoModelForSequenceClassification and passing each our target model, this requires us to preprocess our data before passing to a model, however, it offers the most customization.\n\nEach method supports:\n\nPredictions one at a time (batch size of 1), for example, one person using the app at a time.\nBatches of predictions at a time (predictions with a batch size of n where n can be any number, e.g.¬†8, 16, 32), for example, many people using a service simultaneously such as a voice chat and needing to filter comments (predicting on batches of size n is usually much faster than batches of 1).\n\nWhichever method we choose, we‚Äôll have to set the target device we‚Äôd like the operations to happen on.\nIn general, it‚Äôs best to make predictions on the most powerful accelerator you have available.\nAnd in most cases that will be a NVIDIA GPU &gt; Mac GPU &gt; CPU.\nSo let‚Äôs write a small function to pick the target device for us in that order.\n\n\n\n\n\n\nNote\n\n\n\nMaking predictions is also referred to as inference.\nBecause the model is going to infer on some data what the output should be.\nInference is often faster than training on a per sample basis as no model weights are updated (less computation).\nHowever, inference can use more compute than training over the long run because you could train a model once over a few hours (or days or longer) and then use it for inference for several months (or longer), millions of times (or more).\n\n\n\ndef set_device():\n    \"\"\"\n    Set device to CUDA if available, else MPS (Mac), else CPU.\n\n    This defaults to using the best available device (usually).\n    \"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n    return device\n\nDEVICE = set_device()\nprint(f\"[INFO] Using device: {DEVICE}\")\n\n[INFO] Using device: cuda\n\n\nTarget device set!\nLet‚Äôs start predicting.\n\n\n8.2 Making predictions with pipeline\nThe transformers.pipeline method creates a machine learning pipeline.\nData goes in one end and predictions come out the other end.\nYou can create pipelines for many different tasks, such as, text classification, image classification, object detection, text generation and more.\nLet‚Äôs see how we can create a pipeline for our text classification model.\nTo do so we‚Äôll:\n\nInstantiate an instance of transformers.pipeline.\nPass in the task parameter of text-classification (we can do this because our model is already formatted for text classification thanks to using transformers.AutoModelForSequenceClassification).\nSetup the model parameter to be local_model_path (though we could also use huggingface_model_path).\nSet the target device using the device parameter.\nSet top_k=1 to get to the top prediction back (e.g.¬†either \"food\" or \"not_food\", could set this higher to get more labels back).\nSet the BATCH_SIZE=32 so we can pass to the batch_size parameter. This will allow our model to make predictions on up to 32 samples at a time. Predicting on batches of data is usually much faster than single samples at a time, however, this often saturates at a point (e.g.¬†predicting on batches of size 64 may be the same speed as 32 due to memory contraints).\n\n\n\n\n\n\n\nNote\n\n\n\nThere are many more pipelines available in the Hugging Face documentation.\nAs an exericse, I‚Äôd spend 10-15 minutes reading through the pipeline documentation to get familiar with what‚Äôs available.\n\n\nLet‚Äôs setup our pipeline!\n\nimport torch\nfrom transformers import pipeline\n\n# Set the batch size for predictions\nBATCH_SIZE = 32\n\n# Create an instance of transformers.pipeline\nfood_not_food_classifier = pipeline(task=\"text-classification\", # we can use this because our model is an instance of AutoModelForSequenceClassification\n                                    model=local_model_path, # could also pass in huggingface_model_path\n                                    device=DEVICE, # set the target device\n                                    top_k=1, # only return the top predicted value\n                                    batch_size=BATCH_SIZE) # perform predictions on up to BATCH_SIZE number of samples at a time \n\nfood_not_food_classifier\n\n&lt;transformers.pipelines.text_classification.TextClassificationPipeline at 0x7f2695245950&gt;\n\n\nWe‚Äôve created an instance of transformers.pipelines.text_classification.TextClassificationPipeline!\nNow let‚Äôs test it out by passing it a string of text about food.\n\n# Test our trained model on some example text \nsample_text_food = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\nfood_not_food_classifier(sample_text_food)\n\n[[{'label': 'food', 'score': 0.9993335604667664}]]\n\n\nNice! Our model gets it right.\nHow about a string not about food?\n\n# Test the model on some more example text\nsample_text_not_food = \"A yellow tractor driving over the hill\"\nfood_not_food_classifier(sample_text_not_food)\n\n[[{'label': 'not_food', 'score': 0.9996254444122314}]]\n\n\nWoohoo!\nCorrect again!\nWhat if we passed in random text?\nAs in, someone types in something random to the model expecting an output.\n\n# Pass in random text to the model\nfood_not_food_classifier(\"cvnhertiejhwgdjshdfgh394587\")\n\n[[{'label': 'not_food', 'score': 0.9985743761062622}]]\n\n\nThe nature of machine learning models is that they are a predictive/generative function.\nIf you input data, they will output something.\n\n\n\n\n\n\nNote\n\n\n\nWhen deploying machine learning models, there are many things to take into consideration.\nOne of the main ones being: ‚Äúwhat data is going to go into the model?‚Äù\nIf this was a public facing model and people could enter any kind of text, they could enter random text rather than a sentence about food or not food.\nSince our main goal of the model is be able to classify image captions into food/not_food, we‚Äôd also have to consider image cpations that are poorly written or contain little text.\nThis is why it‚Äôs important to continually test your models with as much example test/real-world data as you can.\n\n\nOur pipeline can also work with the model we saved to the Hugging Face Hub.\nLet‚Äôs try out the same pipeline with model=hugggingface_model_path.\n\n# Pipeline also works with remote models (will have to laod the model locally first)\nfood_not_food_classifier_remote = pipeline(task=\"text-classification\", \n                                           model=huggingface_model_path, # load the model from Hugging Face Hub (will download the model if it doesn't already exist)\n                                           batch_size=BATCH_SIZE,\n                                           device=DEVICE)\n\nfood_not_food_classifier_remote(\"This is some new text about bananas and pancakes and ice cream\")\n\n\n\n\n\n\n\n[{'label': 'food', 'score': 0.9993208646774292}]\n\n\nBeautiful!\nOur model loaded from Hugging Face gets it right too!\n\n\n8.3 Making multiple predictions at the same time with batch prediction\nWe can make predictions with our model one at a time but it‚Äôs often much faster to do them in batches.\nTo make predictions in batches, we can set up our transformers.pipeline instance with the batch_size parameter greater than 1.\nThen we‚Äôll be able to pass multiple samples at once in the form of a Python list.\n\n# Create batch size (we don't need to do this again but we're doing it for clarity)\nBATCH_SIZE = 32 # this number is experimental and will require testing on your hardware to find the optimal value (e.g. lower if there are memory issues or higher to try speed up inference)\n\n# Setup pipeline to handle batches (we don't need to do this again either but we're doing it for clarity)\nfood_not_food_classifier = pipeline(task=\"text-classification\", \n                                    model=local_model_path,\n                                    batch_size=BATCH_SIZE,\n                                    device=DEVICE)\n\nWonderful, now we‚Äôve set up a pipeline instance capable of handling batches, we can pass it a list of samples and it will make predictions on each.\nHow about we try with a collection of sentences which are a bit tricky?\n\n# Create a list of sentences to make predictions on\nsentences = [\n    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n    \"We need to marinate these ideas overnight before presenting them to the client.\",\n    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n    \"Her social media post was the perfect recipe for a viral sensation.\",\n    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n    \"The team needs to simmer down a bit before tackling the next challenge.\",\n    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n    \"A beautiful array of fake wax foods (shokuhin sampuru) in the front of a Japanese restaurant.\",\n    \"Daniel Bourke is really cool :D\",\n    \"My favoruite food is biltong!\"\n]\n\nfood_not_food_classifier(sentences)\n\n[{'label': 'not_food', 'score': 0.9986234903335571},\n {'label': 'not_food', 'score': 0.9993952512741089},\n {'label': 'not_food', 'score': 0.9992876648902893},\n {'label': 'not_food', 'score': 0.9994683861732483},\n {'label': 'not_food', 'score': 0.9993450045585632},\n {'label': 'not_food', 'score': 0.9994571805000305},\n {'label': 'not_food', 'score': 0.9991866946220398},\n {'label': 'food', 'score': 0.9993101358413696},\n {'label': 'not_food', 'score': 0.9995250701904297},\n {'label': 'food', 'score': 0.9966572523117065}]\n\n\nWoah! That was quick!\nAnd it looks like our model performed fairly well.\nThough there was one harder sample which may be deemed as food/not_food, the sentence containing ‚Äúshokuhin sampuru‚Äù (meaning ‚Äúfood model‚Äù in Japanese).\nIs a sentence about food models (fake foods) still about food?\n\n\n8.4 Time our model across larger sample sizes\nWe can say that our model is fast or that making predictions in batches is faster than one at a time.\nBut how about we run some tests to confirm this?\nLet‚Äôs start by making predictions one at a time across 100 sentences (10x our sentences list) and then we‚Äôll write some code to make predictions in batches.\nWe‚Äôll time each and see how they go.\n\nimport time\n\n# Create 1000 sentences\nsentences_1000 = sentences * 100\n\n# Time how long it takes to make predictions on all sentences (one at a time)\nprint(f\"[INFO] Number of sentences: {len(sentences_1000)}\")\nstart_time_one_at_a_time = time.time()\nfor sentence in sentences_1000:\n    # Make a prediction on each sentence one at a time\n    food_not_food_classifier(sentence)\nend_time_one_at_a_time = time.time()\n\nprint(f\"[INFO] Time taken for one at a time prediction: {end_time_one_at_a_time - start_time_one_at_a_time} seconds\")\nprint(f\"[INFO] Avg inference time per sentence: {(end_time_one_at_a_time - start_time_one_at_a_time) / len(sentences_1000)} seconds\")\n\n[INFO] Number of sentences: 1000\n[INFO] Time taken for one at a time prediction: 2.5376925468444824 seconds\n[INFO] Avg inference time per sentence: 0.0025376925468444823 seconds\n\n\nOk, on my local NVIDIA RTX 4090 GPU, it took around 5.5 seconds to make 1000 predictions one at a time.\nThat‚Äôs pretty good!\nBut let‚Äôs see if we can make it faster with batching.\nTo do so, we can increase the size of our sentences_big list and pass the list directly to the model to enable batched prediction.\n\nfor i in [10, 100, 1000, 10_000]:\n    sentences_big = sentences * i\n    print(f\"[INFO] Number of sentences: {len(sentences_big)}\")\n\n    start_time = time.time()\n    # Predict on all sentences in batches \n    food_not_food_classifier(sentences_big)\n    end_time = time.time()\n\n    print(f\"[INFO] Inference time for {len(sentences_big)} sentences: {round(end_time - start_time, 5)} seconds.\")\n    print(f\"[INFO] Avg inference time per sentence: {round((end_time - start_time) / len(sentences_big), 8)} seconds.\")\n    print()\n\n[INFO] Number of sentences: 100\n[INFO] Inference time for 100 sentences: 0.04512 seconds.\n[INFO] Avg inference time per sentence: 0.0004512 seconds.\n\n[INFO] Number of sentences: 1000\n[INFO] Inference time for 1000 sentences: 0.31447 seconds.\n[INFO] Avg inference time per sentence: 0.00031447 seconds.\n\n[INFO] Number of sentences: 10000\n[INFO] Inference time for 10000 sentences: 1.82615 seconds.\n[INFO] Avg inference time per sentence: 0.00018261 seconds.\n\n[INFO] Number of sentences: 100000\n[INFO] Inference time for 100000 sentences: 18.63373 seconds.\n[INFO] Avg inference time per sentence: 0.00018634 seconds.\n\n\n\nWoah!\nIt looks like inference/prediction time is ~10-20x faster when using batched prediction versus predicting one at a time (on my local NVIDIA RTX 4090).\nI ran some more tests with the same model on a different GPU on Google Colab (NVIDIA L4 GPU) and got similar results.\n\n\n\nNumber of Sentences\nTotal Prediction Time\nPrediction Type\n\n\n\n\n100\n0.62\none at a time\n\n\n1000\n6.19\none at a time\n\n\n10000\n61.08\none at a time\n\n\n100000\n605.46\none at a time\n\n\n100\n0.06\nbatch\n\n\n1000\n0.51\nbatch\n\n\n10000\n4.97\nbatch\n\n\n100000\n49.7\nbatch\n\n\n\nTesting the speed of a custom text classifier model on different numbers of sentences with one at a time or batched prediction. Tests conducted on Google Colab with a NVIDIA L4 GPU. See the notebook for code to reproduce.\n\n\n8.5 Making predictions with PyTorch\nWe‚Äôve seen how to make predictions/perform inference with transformers.pipeline, now let‚Äôs see how to do the same with PyTorch.\nPerforming predictions with PyTorch requires an extra step compared to pipeline, we have to prepare our inputs first (turn the text into numbers).\nGood news is, we can prepare our inputs with the tokenizer that got automatically saved with our model.\nAnd since we‚Äôve already trained a model and uploaded it to the Hugging Face Hub, we can load our model and tokenizer with transformers.AutoTokenizer and transformers.AutoModelForSequenceClassification passing it the saved path we used (mine is mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased).\nLet‚Äôs start by loading the tokenizer and see what it looks like to tokenize a piece of sample text.\n\nfrom transformers import AutoTokenizer\n\n# Setup model path (can be local or on Hugging Face)\n# Note: Be sure to change \"mrdbourke\" to your own username\nmodel_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Create an example to predict on\nsample_text_food = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\n\n# Prepare the tokenizer and tokenize the inputs\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_path)\ninputs = tokenizer(sample_text_food, \n                   return_tensors=\"pt\") # return the output as PyTorch tensors \ninputs\n\n{'input_ids': tensor([[  101,  1037, 12090,  6302,  1997,  1037,  5127,  1997, 13501,  6763,\n          1010, 11611,  1998, 15174,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n\n\nNice!\nText tokenized!\nWe get a dictionary of input_ids (our text in token form) and attention_mask (tells the model which tokens to pay attention to, 1 = pay attention, 0 = no attention).\nNow we can load the model with the same path.\n\nfrom transformers import AutoModelForSequenceClassification\n\n# Load our text classification model\nmodel = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_path)\n\nModel loaded!\nLet‚Äôs make a prediction.\nWe can do so using the context manager torch.no_grad() (because no gradients/weights get updated during inference) and passing our model out inputs dictionary.\n\n\n\n\n\n\nNote\n\n\n\nA little tidbit about using dictionaries as function inputs in Python is the ability to unpack the keys of the dictionary into function arguments.\nThis is possible using **TARGET_DICTIONARY syntax. Where the ** means ‚Äúuse all the keys in the dictionary as function parameters‚Äù.\nFor example, the following two lines are equivalent:\n# Using ** notation\noutputs = model(**inputs)\n\n# Using explicit notation\noutputs = model(input_ids=inputs[\"input_ids\"],\n                attention_mask=inputs[\"attention_mask\"])\n\n\nLet‚Äôs make a prediction with PyTorch!\n\nimport torch\n\nwith torch.no_grad():\n    outputs = model(**inputs) # '**' means input all of the dictionary keys as arguments to the function\n    # outputs = model(input_ids=inputs[\"input_ids\"],\n    #                 attention_mask=inputs[\"attention_mask\"]) # same as above, but explicitly passing in the keys\n\noutputs\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-3.3686,  3.9443]]), hidden_states=None, attentions=None)\n\n\nBeautiful, we‚Äôve got some outputs, which contain logits with two values (one for each class).\nThe index of the higher value is our model‚Äôs predicted class.\nWe can find it by taking the outputs.logits and calling argmax().item() on it.\nWe can also find the prediction probability by passing outputs.logits to torch.softmax.\n\n# Get predicted class and prediction probability\npredicted_class_id = outputs.logits.argmax().item()\nprediction_probability = torch.softmax(outputs.logits, dim=1).max().item()\n\nprint(f\"Text: {sample_text_food}\")\nprint(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")\nprint(f\"Prediction probability: {prediction_probability}\")\n\nText: A delicious photo of a plate of scrambled eggs, bacon and toast\nPredicted label: food\nPrediction probability: 0.9993335604667664\n\n\nBeautiful! A prediction made with pure PyTorch! It looks very much correct too.\nHow about we put it all together?\n\nimport torch\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_path)\n\n# Make sample text and tokenize it\nsample_text = \"A photo of a broccoli, salmon, rice and radish dish\"\ninputs = tokenizer(sample_text, return_tensors=\"pt\")\n\n# Make a prediction\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Get predicted class and prediction probability\noutput_logits = outputs.logits\npredicted_class_id = torch.argmax(output_logits, dim=1).item()\npredicted_class_label = model.config.id2label[predicted_class_id]\npredicted_probability = torch.softmax(output_logits, dim=1).max().item()\n\n# Print outputs\nprint(f\"Text: {sample_text}\")\nprint(f\"Predicted class: {predicted_class_label} (prob: {predicted_probability * 100:.2f}%)\")\n\nText: A photo of a broccoli, salmon, rice and radish dish\nPredicted class: food (prob: 99.94%)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#putting-it-all-together",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#putting-it-all-together",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "9 Putting it all together",
    "text": "9 Putting it all together\nOk, ok, we‚Äôve covered a lot of ground going from dataset to trained model to making predictions on custom samples.\nHow about we put all of the steps we‚Äôve covered so far together in a single code cell (or two)?\nTo do so, we‚Äôll:\n\nImport necessary packages (e.g.¬†datasets, transformers.pipeline, torch and more).\nSetup variables for model training and saving pipeline such as our model name, save directory and dataset name.\nCreate a directory for saving models.\nLoad and preprocess the dataset from Hugging Face Hub using datasets.load_dataset.\nImport a tokenizer with transformers.AutoTokenizer and map it to our dataset with dataset.map.\nSet up an evaluation metric with evaluate & create a function to evaluate our model‚Äôs predictions.\nImport a model with transformers.AutoModelForSequenceClassification and prepare it for training with transformers.TrainingArguments and transformers.Trainer.\nTrain the model on our text dataset by calling transformers.Trainer.train.\nSave the trained model to a local directory.\nPush the model to the Hugging Face Hub.\nEvaluate the model on the test data.\nTest the trained model on a custom sample using transformers.pipeline to make sure it works.\n\nPhew!\nA fair few steps but nothing we can‚Äôt handle!\nLet‚Äôs do it.\n\n# 1. Import necessary packages\nimport pprint\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\n\nimport datasets\nimport evaluate\n\nfrom transformers import pipeline\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# 2. Setup variables for model training and saving pipeline\nDATASET_NAME = \"mrdbourke/learn_hf_food_not_food_image_captions\"\nMODEL_NAME = \"distilbert/distilbert-base-uncased\"\nMODEL_SAVE_DIR_NAME = \"models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# 3. Create a directory for saving models\n# Note: This will override our existing saved model (if there is one)\nprint(f\"[INFO] Creating directory for saving models: {MODEL_SAVE_DIR_NAME}\")\nmodel_save_dir = Path(MODEL_SAVE_DIR_NAME)\nmodel_save_dir.mkdir(parents=True, exist_ok=True)\n\n# 4. Load and preprocess the dataset from Hugging Face Hub\nprint(f\"[INFO] Downloading dataset from Hugging Face Hub, name: {DATASET_NAME}\")\ndataset = datasets.load_dataset(path=DATASET_NAME)\n\n# Create mappings from id2label and label2id (adjust these for your target dataset, can also create these programmatically)\nid2label = {0: \"not_food\", 1: \"food\"}\nlabel2id = {\"not_food\": 0, \"food\": 1}\n\n# Create function to map IDs to labels in dataset\ndef map_labels_to_number(example):\n    example[\"label\"] = label2id[example[\"label\"]]\n    return example\n\n# Map preprocessing function to dataset\ndataset = dataset[\"train\"].map(map_labels_to_number)\n\n# Split the dataset into train/test sets\ndataset = dataset.train_test_split(test_size=0.2, seed=42)\n\n# 5. Import a tokenizer and map it to our dataset\nprint(f\"[INFO] Tokenizing text for model training with tokenizer: {MODEL_NAME}\")\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=MODEL_NAME,\n                                          use_fast=True)\n\n# Create a preprocessing function to tokenize text\ndef tokenize_text(examples):\n    return tokenizer(examples[\"text\"],\n                     padding=True,\n                     truncation=True)\n\ntokenized_dataset = dataset.map(function=tokenize_text,\n                                batched=True,\n                                batch_size=1000)\n\n# 6. Set up an evaluation metric & function to evaluate our model\naccuracy_metric = evaluate.load(\"accuracy\")\n\ndef compute_accuracy(predictions_and_labels):\n    predictions, labels = predictions_and_labels\n\n    if len(predictions.shape) &gt;= 2:\n        predictions = np.argmax(predictions, axis=1)\n    \n    return accuracy_metric.compute(predictions=predictions, references=labels) # note: use \"references\" parameter rather than \"labels\"\n\n\n# 7. Import a model and prepare it for training \nprint(f\"[INFO] Loading model: {MODEL_NAME}\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=MODEL_NAME,\n    num_labels=2,\n    id2label=id2label,\n    label2id=label2id\n)\nprint(f\"[INFO] Model loading complete!\")\n\n# Setup TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=model_save_dir,\n    learning_rate=0.0001,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=3,\n    use_cpu=False,\n    seed=42,\n    load_best_model_at_end=True,\n    logging_strategy=\"epoch\",\n    report_to=\"none\",\n    push_to_hub=False,\n    hub_private_repo=False # Note: if set to False, your model will be publically available\n)\n\n# Create Trainer instance and train model\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_accuracy\n)\n\n# 8. Train the model on our text dataset\nprint(f\"[INFO] Commencing model training...\")\nresults = trainer.train()\n\n# 9. Save the trained model (note: this will overwrite our previous model, this is ok)\nprint(f\"[INFO] Model training complete, saving model to local path: {model_save_dir}\")\ntrainer.save_model(output_dir=model_save_dir)\n\n# 10. Push the model to the Hugging Face Hub\nprint(f\"[INFO] Uploading model to Hugging Face Hub...\")\nmodel_upload_url = trainer.push_to_hub(\n    commit_message=\"Uploading food not food text classifier model\",\n    # token=\"YOUR_HF_TOKEN_HERE\" # requires a \"write\" HF token \n)\nprint(f\"[INFO] Model upload complete, model available at: {model_upload_url}\")\n\n# 11. Evaluate the model on the test data\nprint(f\"[INFO] Performing evaluation on test dataset...\")\npredictions_all = trainer.predict(tokenized_dataset[\"test\"])\nprediction_values = predictions_all.predictions\nprediction_metrics = predictions_all.metrics\n\nprint(f\"[INFO] Prediction metrics on the test data:\")\npprint.pprint(prediction_metrics)\n\n[INFO] Creating directory for saving models: models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n[INFO] Downloading dataset from Hugging Face Hub, name: mrdbourke/learn_hf_food_not_food_image_captions\n[INFO] Tokenizing text for model training with tokenizer: distilbert/distilbert-base-uncased\n[INFO] Loading model: distilbert/distilbert-base-uncased\n\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n[INFO] Model loading complete!\n[INFO] Commencing model training...\n\n\n\n      \n      \n      [70/70 00:07, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.372500\n0.067892\n1.000000\n\n\n2\n0.028300\n0.009194\n1.000000\n\n\n3\n0.004700\n0.004919\n1.000000\n\n\n4\n0.002000\n0.002121\n1.000000\n\n\n5\n0.001200\n0.001302\n1.000000\n\n\n6\n0.000900\n0.000982\n1.000000\n\n\n7\n0.000800\n0.000839\n1.000000\n\n\n8\n0.000700\n0.000766\n1.000000\n\n\n9\n0.000700\n0.000728\n1.000000\n\n\n10\n0.000700\n0.000715\n1.000000\n\n\n\n\n\n\n[INFO] Model training complete, saving model to local path: models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n[INFO] Uploading model to Hugging Face Hub...\n\n\n\n\n\n\n\n\n\n\n\n[INFO] Model upload complete, model available at: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased/tree/main/\n[INFO] Performing evaluation on test dataset...\n\n\n\n\n\n[INFO] Prediction metrics on the test data:\n{'test_accuracy': 1.0,\n 'test_loss': 0.0007152689504437149,\n 'test_runtime': 0.0507,\n 'test_samples_per_second': 986.278,\n 'test_steps_per_second': 39.451}\n\n\nWoohoo! It all worked!\nNow let‚Äôs make it sure works by turing it into a transformers.pipeline and passing it a custom sample.\n\n# 12. Make sure the model works by testing it on a custom sample\nfood_not_food_classifier = pipeline(task=\"text-classification\",\n                                    model=model_save_dir, # can also use model on Hugging Face Hub path \n                                    device=torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\",\n                                    top_k=1,\n                                    batch_size=32)\n\nfood_not_food_classifier(\"Yo! We just built a food not food sentence classifier model! Good news is, it can be replicated for other kinds of text classification!\")\n\n[[{'label': 'food', 'score': 0.9969706535339355}]]\n\n\nNice!\nLooks like putting all of our code in one cell worked.\nHow about we make our model even more accessible by turning it into a demo?",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#turning-our-model-into-a-demo",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#turning-our-model-into-a-demo",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "10 Turning our model into a demo",
    "text": "10 Turning our model into a demo\nOnce you‚Äôve trained and saved a model, one of the best ways to continue to test it and show/share it with others is to create a demo.\nOr step number 8 in our workflow:\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\n‚úÖ Define training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\n‚úÖ Pass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\n‚úÖ Train the model by calling Trainer.train().\n‚úÖ Save the model (to our local machine or to the Hugging Face Hub).\n‚úÖ Evaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nA demo is a small application with the focus of showing the workflow of your model from data in to data out.\nIt‚Äôs also one way to start testing your model in the wild.\nYou may know where it works and where it doesn‚Äôt but chances are someone out there will find a new bug before you do.\nTo build our demo, we‚Äôre going to use an open-source library called Gradio.\nGradio allows you to make machine learning demo apps with Python code and best of all, it‚Äôs part of the Hugging Face ecosystem so you can share your demo to the public directly through Hugging Face.\n\n\n\n\nGoing on the premise of data, model, demo, Gradio helps to create the demo. Once you‚Äôve got a trained model on the Hugging Face Hub, you can setup a Gradio interface to import that model and interact it with it. Gradio interfaces can be deployed on Hugging Face Spaces and shared with others so they can try your model too.\n\n\nGradio works on the premise of input -&gt; function (this could be a model) -&gt; output.\nIn our case:\n\nInput = A string of text.\nFunction = Our trained text classification model.\nOutput = Predicted output of food/not_food with prediction probability.\n\n\n10.1 Creating a simple function to perform inference\nLet‚Äôs create a function to take an input of text, process it with our model and return a dictionary of the predicted labels.\nOur function will:\n\nTake an input of a string of text.\nSetup a text classification pipeline using transformers.pipeline as well as our trained model (this can be from our local machine or loaded from Hugging Face). We‚Äôll return all the probabilities from the output using top_k=None.\nGet the outputs of the text classification pipeline from 2 as a list of dictionaries (e.g.¬†[{'label': 'food', 'score': 0.999105}, {'label': 'not_food', 'score': 0.00089}]).\nFormat and return the list of dictionaries from 3 to be compatible with Gradio‚Äôs gr.Label output (we‚Äôll see this later) which requires a dictionary in the form [{\"label_1\": probability_1, \"label_2\": probability_2}].\n\nOnward!\n\nfrom typing import Dict\n\n# 1. Create a function which takes text as input \ndef food_not_food_classifier(text: str) -&gt; Dict[str, float]:\n    \"\"\"\n    Takes an input string of text and classifies it into food/not_food in the form of a dictionary.\n    \"\"\"\n\n    # 2. Setup the pipeline to use the local model (or Hugging Face model path)\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        model=local_model_path,\n                                        batch_size=32,\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\", # set the device to work in any environment\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # 3. Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n    \n    # 4. Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\n# Test out the function\nfood_not_food_classifier(\"My lunch today was chicken and salad\")\n\n{'food': 0.9992194175720215, 'not_food': 0.0007805348141118884}\n\n\nBeautiful!\nLooks like our function is working.\n\n\n10.2 Building a small Gradio demo to run locally\nWe‚Äôve got a working function to go from text to predicted labels and probabilities.\nLet‚Äôs now build a Gradio interface to showcase our model.\nWe can do so by:\n\nImporting Gradio (using import gradio as gr).\nCreating an instance of gr.Interface with parameters inputs=\"text\" (for our text-based inputs) called demo and outputs=gr.Label(num_top_classes=2) to display our output dictionary. We can also add some descriptive aspects to our demo with the title, description and examples parameters.\nRunning/launching the demo with gr.Interface.launch().\n\n\n# 1. Import Gradio as the common alias \"gr\"\nimport gradio as gr\n\n# 2. Setup a Gradio interface to accept text and output labels\ndemo = gr.Interface(\n    fn=food_not_food_classifier, \n    inputs=\"text\", \n    outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n    title=\"Food or Not Food Classifier\",\n    description=\"A text classifier to determine if a sentence is about food or not food.\",\n    examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n              [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\n# 3. Launch the interface\ndemo.launch()\n\nRunning on local URL:  http://127.0.0.1:7860\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n\n\n\n\nWoohoo!\nWe‚Äôve made a very clean way of interacting with our model.\nHowever, our model is still only largely accessible to us (except for the model file we‚Äôve uploaded to Hugging Face).\nHow about we make our demo publicly available so it‚Äôs even easier for people to interact with our model?\n\n\n\n\n\n\nNote\n\n\n\nThe gradio.Interface class is full of many different options, I‚Äôd highly recommend reading through the documentation for 10-15 minutes to get an idea of it.\nIf your workflow requires inputs -&gt; function (e.g.¬†a model making predictions on the input) -&gt; output, chances are, you can build it with Gradio.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#making-our-demo-publicly-accessible",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#making-our-demo-publicly-accessible",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "11 Making our demo publicly accessible",
    "text": "11 Making our demo publicly accessible\nOne of the best ways to share your machine learning work is by creating an application.\nAnd one of the best places to share your applications is Hugging Face Spaces.\nHugging Face Spaces allows you to host machine learning (and non-machine learning) applications for free (with optional paid hardware upgrades).\nIf you‚Äôre familiar with GitHub, Hugging Face Spaces works similar to a GitHub repository (each Space is a Git repository itself).\nIf not, that‚Äôs okay, think of Hugging Face Spaces as an online folder where you can upload your files and have them accessed by others.\nCreating a Hugging Face Space can be done in two main ways:\n\nManually - By going to the Hugging Face Spaces website and clicking ‚ÄúCreate new space‚Äù. Or by going directly to https://www.huggingface.co/new-space. Here, you‚Äôll be able to setup a few settings for your Space and choose the framework/runtime (e.g.¬†Streamlit, Gradio, Docker and more).\nProgrammatically - By using the Hugging Face Hub Python API we can write code to directly upload files to the Hugging Face Hub, including Hugging Face Spaces.\n\nBoth are great options but we‚Äôre going to take the second approach.\nThis is so we can create our Hugging Face Space right from this notebook.\nTo do so, we‚Äôll create three files:\n\napp.py - This will be the Python file which will be the main running file on our Hugging Face Space. Inside we‚Äôll include all the code necessary to run our Gradio demo (as above). Hugging Face Spaces will automatically recoginize the app.py file and run it for us.\nrequirements.txt - This text file will include all of the Python packages we need to run our app.py file. Before our Space starts to run, all of the packages in this file will be installed.\nREADME.md - This markdown file will include details about our Space as well as specific Space-related metadata (we‚Äôll see this later on).\n\nWe‚Äôll create these files with the following file structure:\ndemos/\n‚îî‚îÄ‚îÄ food_not_food_text_classifier/\n    ‚îú‚îÄ‚îÄ app.py\n    ‚îú‚îÄ‚îÄ README.md\n    ‚îî‚îÄ‚îÄ requirements.txt\nWhy this way?\nDoing it in the above style means we‚Äôll have a directory which contains all of our demos (demos/) as well as a dedicated directory which contains our food/not_food demo application (food_not_food_text_classifier/).\nThis way, we‚Äôll be able to upload the whole demos/food_not_food_text_classifier/ folder to Hugging Face Spaces.\nLet‚Äôs start by making a directory to store our demo application files.\n\nfrom pathlib import Path\n\n# Make a directory for demos\ndemos_dir = Path(\"../demos\")\ndemos_dir.mkdir(exist_ok=True)\n\n# Create a folder for the food_not_food_text_classifer demo\nfood_not_food_text_classifier_demo_dir = Path(demos_dir, \"food_not_food_text_classifier\")\nfood_not_food_text_classifier_demo_dir.mkdir(exist_ok=True)\n\nDemo directory created, let‚Äôs now create our requried files.\n\n11.1 Making an app file\nOur app.py file will be the main part of our Hugging Face Space.\nThe good news is, we‚Äôve already created most of it when we created our original demo.\nInside the app.py file we‚Äôll:\n\nImport the required libraries/packages for running our demo app.\nSetup a function for going from text to our trained model‚Äôs predicted outputs. And because our model is already hosted on the Hugging Face Hub, we can pass pipeline our model‚Äôs name (e.g.¬†mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased) and when we upload our app.py file to Hugging Face Spaces, it will load the model directly from the Hub.\n\nNote: Be sure to change ‚Äúmrdbourke‚Äù to your own Hugging Face username.\n\nCreate a demo just as before with gr.Interface.\nLaunch our demo with gr.Interface.launch.\n\nWe can write all of the above in a notebook cell.\nAnd we can turn it into a file by using the %%writefile magic command and passing it our target filepath.\nLet‚Äôs do it!\n\n%%writefile ../demos/food_not_food_text_classifier/app.py\n# 1. Import the required packages\nimport torch\nimport gradio as gr\n\nfrom typing import Dict\nfrom transformers import pipeline\n\n# 2. Define function to use our model on given text \ndef food_not_food_classifier(text: str) -&gt; Dict[str, float]:\n    # Set up text classification pipeline\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        # Because our model is on Hugging Face already, we can pass in the model name directly\n                                        model=\"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\", # link to model on HF Hub\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n\n    # Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\n# 3. Create a Gradio interface with details about our app\ndescription = \"\"\"\nA text classifier to determine if a sentence is about food or not food. \n\nFine-tuned from [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) on a [small dataset of food and not food text](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n\nSee [source code](https://github.com/mrdbourke/learn-huggingface/blob/main/notebooks/hugging_face_text_classification_tutorial.ipynb).\n\"\"\"\n\ndemo = gr.Interface(fn=food_not_food_classifier, \n             inputs=\"text\", \n             outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n             title=\"üçóüö´ü•ë Food or Not Food Text Classifier\",\n             description=description,\n             examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n                       [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\n# 4. Launch the interface\nif __name__ == \"__main__\":\n    demo.launch()\n\nOverwriting ../demos/food_not_food_text_classifier/app.py\n\n\napp.py file created!\nNow let‚Äôs setup the requirements file.\n\n\n11.2 Making a requirements file\nWhen you upload an app.py file to Hugging Face Spaces, it will attempt to run it automatically.\nAnd just like running the file locally, we need to make sure all of the required packages are available.\nOtherwise our Space will produce an error like the following:\n===== Application Startup at 2024-06-13 05:37:21 =====\n\nTraceback (most recent call last):\n  File \"/home/user/app/app.py\", line 1, in &lt;module&gt;\n    import torch\nModuleNotFoundError: No module named 'torch'\nGood news is, our demo only has three requirements: gradio, torch, transformers.\nLet‚Äôs create a requirements.txt file with the packages we need and save it to the same directory as our app.py file.\n\n%%writefile ../demos/food_not_food_text_classifier/requirements.txt\ngradio\ntorch\ntransformers\n\nOverwriting ../demos/food_not_food_text_classifier/requirements.txt\n\n\nBeautiful!\nHugging Face Spaces will automatically recognize the requirements.txt file and install the listed packages into our Space.\n\n\n11.3 Making a README file\nOur app.py can contain information about our demo, however, we can also use a README.md file to further communicate our work.\n\n\n\n\n\n\nNote\n\n\n\nIt is common practice in Git repositories (including GitHub and Hugging Face Hub) to add a README.md file to your project so people can read more (hence ‚Äúread me‚Äù) about what your project is about.\n\n\nWe can include anything in markdown-style text in the README.md file.\nHowever, Spaces also have a special YAML block at the top of the README.md file in the root directory with configuration details.\nInside the YAML block you can put special metadata details about your Space including:\n\ntitle - The title of your Space (e.g.¬†title: Food Not Food Text Classifier).\nemoji - The emoji to display on your Space (e.g.¬†emoji: üçóüö´ü•ë).\napp_file - The target app file for Spaces to run (set to app_file: app.py by default).\n\nAnd there are plenty more in the Spaces Configuration References documentation.\n\n\n\n\nExample of Hugging Face Spaces README.md file with YAML front matter (front matter is another term for ‚Äúthings at the front/top of the file‚Äù) for formatting the Space.\n\n\nLet‚Äôs create a README.md file with a YAML block at the top detailing some of the metadata about our project.\n\n\n\n\n\n\nNote\n\n\n\nThe YAML block at the top of the README.md can take some practice.\nIf you want to see a demo of how one gets created, try making a Hugging Face Space with the ‚ÄúCreate new Space‚Äù button on the https://huggingface.co/spaces page and seeing what the README.md file starts with (that‚Äôs how I found out what to do!).\n\n\n\n%%writefile ../demos/food_not_food_text_classifier/README.md\n---\ntitle: Food Not Food Text Classifier\nemoji: üçóüö´ü•ë\ncolorFrom: blue\ncolorTo: yellow\nsdk: gradio\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n\n# üçóüö´ü•ë Food Not Food Text Classifier\n\nSmall demo to showcase a text classifier to determine if a sentence is about food or not food.\n\nDistillBERT model fine-tuned on a small synthetic dataset of 250 generated [Food or Not Food image captions](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n\n[Source code notebook](https://github.com/mrdbourke/learn-huggingface/blob/main/notebooks/hugging_face_text_classification_tutorial.ipynb).\n\nOverwriting ../demos/food_not_food_text_classifier/README.md\n\n\nREADME.md created!\nNow let‚Äôs check out the files we have in our demos/food_not_food_text_classifier/ folder.\n\n!ls ../demos/food_not_food_text_classifier\n\nREADME.md  app.py  requirements.txt\n\n\nPerfect!\nLooks like we‚Äôve got all the files we need to create our Space.\nLet‚Äôs upload them to the Hugging Face Hub.\n\n\n11.4 Uploading our demo to Hugging Face Spaces\nWe‚Äôve created all of the files required for our demo, now for the fun part!\nLet‚Äôs upload them to Hugging Face Spaces.\nTo do so programmatically, we can use the Hugging Face Hub Python API.\n\n\n\n\n\n\nNote\n\n\n\nThe Hugging Face Hub Python API has many different options for interacting with the Hugging Face Hub programmatically.\nYou can create repositories, upload files, upload folders, add comments, change permissions and much much more.\nBe sure to explore the documentation for at least 10-15 minutes to get an idea of what‚Äôs possible.\n\n\nTo get our demo hosted on Hugging Face Spaces we‚Äôll go through the following steps:\n\nImport the required methods from the huggingface_hub package, including create_repo, get_full_repo_name, upload_file (optional, we‚Äôll be using upload_folder) and upload_folder.\nDefine the demo folder we‚Äôd like to upload as well as the different parameters for the Hugging Face Space such as repo type (\"space\"), our target Space name, the target Space SDK (\"gradio\"), our Hugging Face token with write access (optional if it already isn‚Äôt setup).\nCreate a repository on Hugging Face Spaces using the huggingface_hub.create_repo method and filling out the appropriate parameters.\nGet the full name of our created repository using the huggingface_hub.get_full_repo_name method (we could hard code this but I like to get it programmatically incase things change).\nUpload the contents of our target demo folder (../demos/food_not_food_text_classifier/) to Hugging Face Hub with huggingface_hub.upload_folder.\nHope it all works and inspect the results! ü§û\n\nA fair few steps but we‚Äôve got this!\n\n# 1. Import the required methods for uploading to the Hugging Face Hub\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file, # for uploading a single file (if necessary)\n    upload_folder # for uploading multiple files (in a folder)\n)\n\n# 2. Define the parameters we'd like to use for the upload\nLOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"../demos/food_not_food_text_classifier\"\nHF_TARGET_SPACE_NAME = \"learn_hf_food_not_food_text_classifier_demo\"\nHF_REPO_TYPE = \"space\" # we're creating a Hugging Face Space\nHF_SPACE_SDK = \"gradio\"\nHF_TOKEN = \"\" # optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)\n\n# 3. Create a Space repository on Hugging Face Hub \nprint(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\ncreate_repo(\n    repo_id=HF_TARGET_SPACE_NAME,\n    # token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)\n    repo_type=HF_REPO_TYPE,\n    private=False, # set to True if you don't want your Space to be accessible to others\n    space_sdk=HF_SPACE_SDK,\n    exist_ok=True, # set to False if you want an error to raise if the repo_id already exists \n)\n\n# 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})\nfull_hf_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\nprint(f\"[INFO] Full Hugging Face Hub repo name: {full_hf_repo_name}\")\n\n# 5. Upload our demo folder\nprint(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo: {full_hf_repo_name}\")\nfolder_upload_url = upload_folder(\n    repo_id=full_hf_repo_name,\n    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n    path_in_repo=\".\", # upload our folder to the root directory (\".\" means \"base\" or \"root\", this is the default)\n    # token=HF_TOKEN, # optional: set token manually\n    repo_type=HF_REPO_TYPE,\n    commit_message=\"Uploading food not food text classifier demo app.py\"\n)\nprint(f\"[INFO] Demo folder successfully uploaded with commit URL: {folder_upload_url}\")\n\n[INFO] Creating repo on Hugging Face Hub with name: learn_hf_food_not_food_text_classifier_demo\n[INFO] Full Hugging Face Hub repo name: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n[INFO] Uploading ../demos/food_not_food_text_classifier to repo: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n[INFO] Demo folder successfully uploaded with commit URL: https://huggingface.co/spaces/mrdbourke/learn_hf_food_not_food_text_classifier_demo/tree/main/.\n\n\nExcellent!\nLooks like all of the files in our target demo folder were uploaded!\nOnce this happens, Hugging Face Spaces will take a couple of minutes to build our application.\nIf there are any errors, it will let us know.\nOtherwise, our demo application should be running live and be ready to test at a URL similar to: https://huggingface.co/spaces/mrdbourke/learn_hf_food_not_food_text_classifier_demo (though you may have to swap my username ‚Äúmrdbourke‚Äù for your own as well as the name you chose for the Space).\n\n\n11.5 Testing our hosted demo\nOne of the really cool things about Hugging Face Spaces is that we can share our demo application as a link so others can try it out.\nWe can also embed it right into our notebook.\nTo do so, we can go to the three dots in the top right of our hosted Space and select ‚ÄúEmbed this Space‚Äù.\nWe then have the option to embed our Space using a JavaScript web component, HTML iframe or via the direct URL.\nSince Jupyter notebooks have the ability to render HTML via IPython.display.HTML, let‚Äôs embed our Space with HTML.\n\nfrom IPython.display import HTML\n\n\n# You can get embeddable HTML code for your demo by clicking the \"Embed\" button on the demo page\nHTML(data='''\n&lt;iframe\n    src=\"https://mrdbourke-learn-hf-food-not-food-text-classifier-demo.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"450\"\n&gt;&lt;/iframe&gt;     \n''')\n\n\n     \n\n\nNow that‚Äôs cool!\nWe can try out our Food Not Food Text Classifier app from right within our notebook!",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#summary",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#summary",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "12 Summary",
    "text": "12 Summary\nYou should be very proud of yourself!\nWe‚Äôve just gone end-to-end on a machine learning workflow with Hugging Face.\nFrom loading a dataset to training a model to deploying that model in the form of a public demo.\nHere are some of the main takeaways from this project.\nThe Hugging Face ecosystem is a collection of powerful and open-source tools for machine learning workflows.\n\nHugging Face datasets helps you to store and preprocess datasets of almost any shape and size.\nHugging Face transformers has many built-in pretrained models for many different use cases and components such as transformers.Trainer help you to tailor those models to your own custom use cases.\nHugging Face tokenizers works closely with transformers and allows the efficient conversion of raw text data into numerical representation (which is required for machine learning models).\nThe Hugging Face Hub is a great place to share your models and machine learning projects. Over time, you can build up a portfolio of machine learning-based projects to show future employers or clients and to help the community grow.\nThere are many more, but I‚Äôll leave these for you to explore as extra-curriculum.\n\nA common machine learning workflow: dataset -&gt; model -&gt; demo.\nBefore a machine learning model is incorporated into a larger application, a very common workflow is to:\n\nFind an existing or create a new dataset for your specific problem.\nTrain/fine-tune and evaluate an existing model on your dataset.\nCreate a small demo application to test your trained model.\n\nWe‚Äôve just gone through all of these steps for text classification!\nText classification is a very common problem in many business settings. If you have a similar problem but a different dataset, you can replicate this workflow.\nBuilding your own model has several advantages over using APIs.\nAPIs are very helpful to try something out.\nHowever, depending on your use case, you may often want to create your own custom model.\nTraining your own model can often result in faster predictions and far less running costs over time.\nThe Hugging Face ecosystem enables the creation of custom models for almost any kind of machine learning problem.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#exercises-and-extensions",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#exercises-and-extensions",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "13 Exercises and Extensions",
    "text": "13 Exercises and Extensions\nThere‚Äôs no better way to improve other than practicing what you‚Äôve learned.\nThe following exercises and extensions are designed for you to practice the things we‚Äôve covered in this project.\n\nOur text classification model works on food/not_food text samples. How would you create your own binary text classification model on different classes?\n\nCreate ~10 or samples of your own text classes (e.g.¬†10 samples each of spam/not_spam emails) and retrain a text classification model.\nBonus: Share the model you‚Äôve made in a demo just like we did here. Send it to me, I‚Äôd love to see it! My email is on my website.\n\nWe‚Äôve trained our model on two classes (binary classification) but how might we increase that to 3 or more classes (multi-class classification)?\n\nHint: see the num_labels parameter in transformers.AutoModelForSequenceClassification.\n\nOur model seems to work pretty good on our test data and on the few number of examples we tried manually. Can you find any examples where our model fails? For example, what kind of sentences does it struggle with? How could you fix this?\n\nHint: Our model has been trained on examples with at least 5-12 words, does it still work with short sentences? (e.g.¬†‚Äúpie‚Äù).\nBonus: If you find any cases where our model doesn‚Äôt perform well, make an extra 10-20 examples of these and add them to the dataset and then retrain the model (you‚Äôll have to lookup ‚Äúhow to add rows to an existing Hugging Face dataset‚Äù). How does the model perform after adding these additional samples?\n\nDatasets are fundamental to any machine learning project, getting to know how to process and interact with them is a fundamental skill. Spend 1 hour going through the Hugging Face Datasets tutorial.\n\nWrite 5 things you can do with Hugging Face Datasets and where they might come in handy.\n\nThe Hugging Face transformers library has many features. The following readings are to help understand a handful of them.\n\nSpend 10 minutes exploring the transformers.TrainingArguments documentation.\nSpend 10 minutes reading the transformers.Trainer documentation.\n\nSpend 10 minutes reading the Hugging Face model sharing documentation.\n\nSpend 10 minutes reading the Hugging Face transformers.pipeline documentation.\n\nWhat does a pipeline do?\nName 3 different kinds of pipelines and describe what they do in a sentence\n\n\nGradio is a powerful library for making machine learning demos, learning more about it will help you in future creations. Spend 10-15 minutes reading the Gradio quickstart documentation.\n\nWhat are 3 kinds of demos you can create?\nWhat are 3 different inputs and outputs you can make?",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#extra-resources",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#extra-resources",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "14 Extra resources",
    "text": "14 Extra resources\nThere are many things we touched over but didn‚Äôt go into much depth in this notebook.\nThe following resources are for those who‚Äôd like to learn a little bit more.\n\nSee how the food not food image caption dataset was created with synthetic text data (image captions generated by a Large Language Model) in the example Google Colab notebook.\nHugging Face have a great guide on sequence classification (it‚Äôs what this notebook was built on).\nFor more on the concept of padding and truncation in sequence processing, I‚Äôd recommend the Hugging Face padding and truncation guide.\nFor more on Transformers (the architecture) as well as the DistilBert model:\n\nRead Transformers from scratch by Peter Bloem.\nWatch Andrej Karpathy‚Äôs lecture on Transformers and their history.\nRead the original Attention is all you need paper (the paper that introduced the Transformer architecture).\nRead the DistilBert paper from the Hugging Face team (paper that introduced the DistilBert architecture and training setup).",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "extras/todo.html",
    "href": "extras/todo.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "Move todo.md into index.md for easier navigation (one file is enough)\nAdd ‚Äúgetting setup‚Äù file to get started locally with the required dependencies\nAdd text classification dataset creation notebook (so people can see where the data comes from)\nAdd a Hugging Face ecosystem overview (transformers, datasets, tokenizers, torch, Hugging Face Hub, Hugging Face Spaces, etc.)\nAdd a fav icon\nMake code-only versions of notebooks? e.g.¬†text stripped away and only code is left"
  },
  {
    "objectID": "extras/todo.html#quarto-misc",
    "href": "extras/todo.html#quarto-misc",
    "title": "Learn Hugging Face ü§ó",
    "section": "Quarto misc",
    "text": "Quarto misc\n\nCreate share cards - https://quarto.org/docs/websites/website-tools.html#twitter-cards\n\nSee here for share image - https://quarto.org/docs/websites/website-tools.html#preview-images"
  },
  {
    "objectID": "extras/glossary.html",
    "href": "extras/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Glossary\nThis is messy and a work in progress. Will tidy up later.\n\nPredictive vs generative AI = predictive AI -&gt; machine readable outputs, generative AI -&gt; human readable outputs. Predictive style models take in data and map it to a fixed output space (e.g.¬†a text classification model predicting whether an email is spam or not). Generative AI models take in data and generate an unbounded response (though theorectically this response is bounded by the training distribution), such as, a chat system taking in natural language instructions and producing natural language as output. Generative AI models can be turned into predictive-style models, for example a generative LLM could produce JSON outputs if instructed/constrained to do so.\nTransformer = A deep learning model that adopts the attention mechanism to draw global dependencies between input and output\nTokenization = turning a series of data (text or image) into a series of tokens, where a token is a numerical representation of the input data, for example, in the case of text, tokenization could mean turning the words in a sentence into numbers (e.g.¬†‚Äúhello world‚Äù -&gt; [101, 102])\nTokens = a token is a letter, word or word-piece (word) that a model uses to represent input data, for example, in the case of text, a token could be a word (e.g.¬†‚Äúhello‚Äù) or a word-piece (e.g.¬†‚Äúhell‚Äù and ‚Äúo‚Äù), see: https://platform.openai.com/tokenizer for an example\ntransformers = A Python library by Hugging Face that provides a wide range of pre-trained transformer models, fine-tuning tools, and utilities to use them\ndatasets = A Python library by Hugging Face that provides a wide range of datasets for NLP and CV tasks\ntokenizers = A Python library by Hugging Face that provides a wide range of tokenizers for NLP tasks\nevaluate = A Python library by Hugging Face with premade evaluation functions for various tasks\ntorch = PyTorch, an open-source machine learning library\ntransformers.pipeline = an abstraction to get a machine learning pipeline up and running in a few lines of code, handles data preprocessing and device placement behind the scences. For example, transformers.pipeline(\"text-classification\") can be used to tokenize input text and classify it.\ntransfer learning = taking what one model has learned and applying it to another task (e.g.¬†a model which has learned across many millions of words of text from the internet and then adjusting it to work with your smaller dataset)\nfine-tuning = a type of transfer learning where you take the existing patterns of one model (usually trained on a very large dataset) and customize them to work for your smaller dataset\nfull fine-tuning = fine-tune all of a models parameters to your dataset\npartial fine-tuning = only fine-tune a portion of a models parameters to your dataset\nfeature extraction fine-tuning = only fine-tune the final layer(s) of model to your dataset (e.g.¬†the majority of the backbone stays frozen)\nLoRA (Low-Rank Adaptation) = train an adaptor matrix (far fewer parameters than a full model) to apply to your base model weights (base model weights stay frozen)\nhyperparameters = values you can set to adjust training settings, for example, learning rate is a hyperparameter that is adjustable\nHugging Face Hub (or Hub for short) = Place to store datasets, models, and other resources of your own + find existing datasets, models & scripts others have shared. If you are familiar with GitHub, Hugging Face is like the GitHub of machine learning.\nAuto Classes = A series of classes in transformers which enables automatic loading of preprocessor or model classes based on the name or path of the model. For example you can load the processor for microsoft/conditional-detr-resnet-50 with transformers.AutoImageProcessor(microsoft/conditional-detr-resnet-50) or the model architecture with transformers.AutoModelForObjectDetection(microsoft/conditional-detr-resnet-50).\nHugging Face Spaces = A platform to share and run machine learning apps/demos, these can be built with HTML, Gradio or Streamlit\nHF = Hugging Face\nNLP = Natural Language Processing\nCV = Computer Vision\nImage classification = Classify an image in a single or multiple classes (classifying something as multiple items or labels such as [warm, well lit, sunset] is also known as tagging or more specifically, image tagging), for example, is a photo of food or not food.\nObject detection = Detect and locate an item in an image or series of images (e.g.¬†a video). An item can be almost anything in an image, for example, a licence plate, a person, a weed in a garden or a small bug on the body of a bee.\nBounding box = A box, often rectangular in nature, drawn around an item in an image to indicate its location. Can come in several different forms such as XYXY, XYWH and CXCYWH (see more in A Guide to Bounding Box Formats and How to Draw Them).\nTPU = Tensor Processing Unit\nGPU = Graphics Processing Unit\nLearning rate = Often the most important hyperparameter to tune. It is proportional with the amount an optimizer will update a model‚Äôs parameters every update step. A higher amount means larger updates (though sometimes too large) a lower amount means smaller updates (though sometimes not enough). The most ideal learning rate is experimental. Common values include 0.001, 0.0001, 0.0005, 0.00001, 0.00005 (though the learning rate can be any value). Many optimizers have decent default learning rates. For example, the Adam optimizer (a common and generally well performing optimizer) in PyTorch (torch.optim.Adam) has a default learning rate of 0.001. For fine-tuning an already trained model a learning rate of 10x smaller than the default is a good rule of thumb (e.g.¬†if a model was trained with a learning rate of 0.001, fine-tuning with 0.0001 is common). The learning rate does not have to be static and can change dynamcially during training, this practice is referred to as learning rate scheduling.\nInference = using a trained (or untrained) model to make predictions on a given piece of data. The model infers what the output should be based on the inputs. Inference is often much faster than training on a sample per sample basis because no weights get updated during inference. Though, when compared to training, inference can often take more compute in the long run. Because a model can be trained once but then used for inference millions of times (or more) over the next several months (or longer).\nPrediction probability = the probability of a model‚Äôs prediction for a given input, is a score between 0 and 1 with 1 being the highest, for example, a model may have a prediction probability of 0.95, this would mean it‚Äôs quite confident with its prediction but it doesn‚Äôt mean it‚Äôs correct. A good way to inspect potential issues with a dataset is to show examples in the test set which have a high prediction probability but are wrong (e.g.¬†pred prob = 0.98 but the prediction was incorrect).\nHugging Face Pipeline (pipeline) = A high-level API for using model for various tasks (e.g.¬†text-classification, audio-classification, image-classification, object-detection and more), see the docs: https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/pipelines#transformers.pipeline\nloss value = Measure of how wrong your model is by a given metric. A perfect model will have a loss value of 0 (it is able to predict the data perfectly), though this is highly unlikely in practice (there are no perfect models). Ideally, the loss value will go down (towards 0) as training goes on. If the loss value on the training set is lower than the loss value on the test set, the model is likely overfitting (memorizing the training set too well rather than learning generalizable patterns to unseen data). To fix overfitting, introduce more regularization. To fix underfitting (loss not going down), introduce more learning capacity (more data, more parameters in the model, longer training). Machine learning is a constant battle between overfitting and underfitting.\nRandom seed = Value to flavour the randomness of an operation. For example, if you set a random seed to 42 the numbers produced by a random generator will be random but flavoured by the seed. This means if the seed stays at 42, subsequent calls of the same operation will return the same values. Not setting a random seed will result in different random values each time. Setting a random seed is done to ensure reproducibility of an operation. This is helpful when performing experiments and you do not want the outputs to be random each time.\nSynthetic data generation = using a model such as a generative Large Language Model (LLM) to generate synthetic pieces of data for a specific problem. For example, getting an LLM to generate food and not food image captions to create a binary text classification model. Synthetic data is very helpful when bootstrapping a machine learning problem. Though it is advised to only train on synthetic data and to evaluate on real data whenever possible.\nPre-trained models = models which have already been trained on a large dataset, for example, text-based models which have gone through many millions of words of text (e.g.¬†all of Wikipedia and 1000s of books) or image-based models which have seen millions of images (e.g.¬†models trained on ImageNet). In essence, any model which has already spent a large amount of time learning patterns in data. These patterns can then be adjusted for your own sample problems, often with much much smaller amounts of data for excellent results. The process of customizing a pre-trained model for a specific problem is called transfer learning (transferring what an existing model knows to your own problem).\nTraining/test split = One of the most important concepts in machine learning. Train models on the training data and evaluate them on the test data. The test data should never be seen by a model during training. Think of the test data as the final exam in a university course. A model should be able to learn enough patterns in the training set to perform well on the test set. Just like a student should be able to learn enough on course materials to do well on the final exam. If a model performs well on the training set but not well on the test set, this is known as overfitting, as in, the model memorizes the training set rather than learning generalizable patterns to unseen data. If a model performs poorly on both the training set and the test set, this is known as underfitting.\n\nPrediction probabilities = a value assigned to a model‚Äôs prediction on a certain sample after its output logits have passed through an activation function such as Softmax or Sigmoid. For example, in a binary classification problem of whether an image is of food or not of food, a model could assign a prediction probability of the image being 0.98 food and 0.02 not food. Prediction probabilities do not indicate how right a prediction is, more so, how confident a model is in that prediction. The closer a prediction probability to 1, the higher the model‚Äôs confidence in the prediction. A good evaluation step is to inspect samples with low prediction probabilities (the model seems to get confused on them) or inspect test samples where the model has a high prediction probability but the prediction is wrong (these predictions are often referred to as most wrong predictions).\nTK - logits - the raw outputs of a model\nTK - Softmax function - an activation function which can be applied to logits to get prediction probabilities.\n\n\n\n\n\n Back to top"
  }
]