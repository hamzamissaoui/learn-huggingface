{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7a3b4c1e",
      "metadata": {},
      "source": [
        "# Full Fine-Tuning Google Gemma 3-270M-IT\n",
        "\n",
        "This notebook provides a complete workflow for full fine-tuning of the Google Gemma 3-270M model using the Hugging Face `transformers`, `datasets`, and `trl` libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Install necessary libraries\n",
        "!pip install -q transformers datasets trl accelerate torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hamza\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# 2. Login to Hugging Face (Required for Gemma models if they are gated)\n",
        "from huggingface_hub import login\n",
        "# login() # Enter your HF token here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hamza\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\triton\\windows_utils.py:404: UserWarning: Failed to find CUDA.\n",
            "  warnings.warn(\"Failed to find CUDA.\")\n",
            "W0111 05:34:39.966000 2488 torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Configuration\n",
        "model_id = \"google/gemma-3-270m-it\"\n",
        "output_dir = \"./gemma-3-270m-full-finetuned\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'AutoTokenizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 3. Load Tokenizer and Model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m.from_pretrained(model_id)\n\u001b[32m      3\u001b[39m tokenizer.pad_token = tokenizer.eos_token \u001b[38;5;66;03m# Ensure padding token is set\u001b[39;00m\n\u001b[32m      4\u001b[39m tokenizer.padding_side = \u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mNameError\u001b[39m: name 'AutoTokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "# 3. Load Tokenizer and Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token # Ensure padding token is set\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load model for Full Fine-Tuning (no quantization/LoRA)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Prepare Dataset\n",
        "# Using a subset of Dolly 15k for demonstration purposes\n",
        "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train[:1000]\")\n",
        "\n",
        "def formatting_prompts_func(example):\n",
        "    output_texts = []\n",
        "    for i in range(len(example['instruction'])):\n",
        "        # Construct prompt following Gemma's chat template format\n",
        "        text = f\"<start_of_turn>user\\n{example['instruction'][i]}\\n{example['context'][i]}<end_of_turn>\\n<start_of_turn>model\\n{example['response'][i]}<end_of_turn>\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "# For SFTTrainer, we use formatting_func or specify the field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-5, \n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    optim=\"adamw_torch\",\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    report_to=\"none\", # Can be set to \"wandb\" or \"tensorboard\"\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"constant\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Initialize SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "# 7. Start Training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. Save the Fine-Tuned Model\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. Inference Test after Training\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What are the benefits of full fine-tuning an SLM?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(inputs, max_new_tokens=150)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
